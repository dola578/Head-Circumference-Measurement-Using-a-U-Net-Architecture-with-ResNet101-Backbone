{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1801f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801e54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall typing_extensions\n",
    "# !pip install typing_extensions==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c8f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions>=4.3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1b91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30e90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing_extensions==4.12.2\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "Successfully installed typing_extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install typing_extensions==4.12.2 --upgrade\n",
    "# pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8758f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypeIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee1b1c",
   "metadata": {},
   "source": [
    "**Preprocessing - flipping, resizing, rotation, gamma correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe0d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from PIL import Image\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, image_dir, mask_dir, transform=None):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.mask_dir = mask_dir\n",
    "#         self.transform = transform\n",
    "#         self.images = [os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')]\n",
    "#         self.masks = [os.path.join(mask_dir, x) for x in os.listdir(mask_dir) if 'Annotation' in x]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_path = self.images[idx]\n",
    "#         mask_path = self.masks[idx]\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         mask = Image.open(mask_path).convert(\"L\")\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#             mask = self.transform(mask)\n",
    "#         return image, mask\n",
    "\n",
    "# # Define transformations including geometric and intensity-based augmentations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),  # Resize images to match U-Net expected input\n",
    "#     transforms.RandomHorizontalFlip(),  # Random horizontal flipping\n",
    "#     transforms.RandomVerticalFlip(),  # Random vertical flipping\n",
    "#     transforms.RandomRotation(20),  # Random rotations between -20 to 20 degrees\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Random brightness and contrast adjustments\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Lambda(lambda x: x.pow(0.5))  # Gamma correction with gamma=0.5\n",
    "# ])\n",
    "\n",
    "# # Initialize dataset\n",
    "# full_dataset = CustomDataset('denoised_training_set', 'masked_annotations', transform=transform)\n",
    "\n",
    "# # Splitting the dataset into train and validation sets\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# validation_size = len(full_dataset) - train_size\n",
    "# train_dataset, validation_dataset = random_split(full_dataset, [train_size, validation_size])\n",
    "\n",
    "# # Create separate dataloaders for train and validation datasets\n",
    "# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "# validation_loader = DataLoader(validation_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09bb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# Custom joint transformation that applies geometric transforms to both image and mask,\n",
    "# and intensity transforms only to the image.\n",
    "class JointTransform:\n",
    "    def __init__(self, resize=(572, 572), rotation=20, hflip_prob=0.5, vflip_prob=0.5,\n",
    "                 intensity_transforms=None):\n",
    "        self.resize = resize\n",
    "        self.rotation = rotation\n",
    "        self.hflip_prob = hflip_prob\n",
    "        self.vflip_prob = vflip_prob\n",
    "        # Intensity transforms should be a torchvision transform applied only to the image.\n",
    "        # For example, transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        #                                  transforms.Lambda(lambda x: x.pow(0.5))])\n",
    "        self.intensity_transforms = intensity_transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # 1. Resize both image and mask\n",
    "        image = TF.resize(image, self.resize)\n",
    "        mask = TF.resize(mask, self.resize)\n",
    "        \n",
    "        # 2. Random horizontal flip\n",
    "        if np.random.rand() < self.hflip_prob:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "            \n",
    "        # 3. Random vertical flip\n",
    "        if np.random.rand() < self.vflip_prob:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "            \n",
    "        # 4. Random rotation\n",
    "        angle = np.random.uniform(-self.rotation, self.rotation)\n",
    "        image = TF.rotate(image, angle, interpolation=Image.BILINEAR)\n",
    "        # For masks, use nearest neighbor interpolation to preserve label boundaries.\n",
    "        mask = TF.rotate(mask, angle, interpolation=Image.NEAREST)\n",
    "        \n",
    "        # 5. Apply intensity transforms to the image only (if provided)\n",
    "        if self.intensity_transforms:\n",
    "            image = self.intensity_transforms(image)\n",
    "        \n",
    "        # 6. Convert both image and mask to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        # Optionally ensure the mask is binary\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define intensity-only transformations for the image.\n",
    "intensity_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5))  # Gamma correction with gamma=0.5\n",
    "])\n",
    "\n",
    "# Create the joint transformation instance\n",
    "joint_transform = JointTransform(resize=(572, 572), rotation=20,\n",
    "                                 hflip_prob=0.5, vflip_prob=0.5,\n",
    "                                 intensity_transforms=intensity_transforms)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, joint_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.joint_transform = joint_transform\n",
    "        # Sort the file lists to ensure alignment between images and masks\n",
    "        self.images = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')])\n",
    "        self.masks = sorted([os.path.join(mask_dir, x) for x in os.listdir(mask_dir) if 'Annotation' in x])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.joint_transform:\n",
    "            image, mask = self.joint_transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "# Initialize dataset with the joint transform\n",
    "full_dataset = CustomDataset('denoised_training_set', 'masked_annotations', joint_transform=joint_transform)\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# validation_size = len(full_dataset) - train_size\n",
    "# train_dataset, validation_dataset = random_split(full_dataset, [train_size, validation_size])\n",
    "\n",
    "# # Create separate dataloaders for train and validation datasets\n",
    "# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "# validation_loader = DataLoader(validation_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d114c",
   "metadata": {},
   "source": [
    "**Checking alignment and order of train image and the corresponding masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12258a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path, mask_path in zip(sorted(os.listdir('denoised_training_set')), \n",
    "                               sorted(os.listdir('masked_annotations'))):\n",
    "    if img_path.endswith('.png') and 'Annotation' in mask_path:\n",
    "        base_img = os.path.splitext(img_path)[0]\n",
    "        base_mask = os.path.splitext(mask_path)[0].replace('_Annotation', '')\n",
    "        assert base_img == base_mask, f\"Mismatch: {base_img} vs {base_mask}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61e302",
   "metadata": {},
   "source": [
    "**Unet with resnet101 as backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a664895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ResConv(nn.Module):\n",
    "    \"\"\" Convolution block for U-Net with repeated convolutions and ReLU activations. \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ResConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\" Upsampling block for U-Net, using bilinear interpolation and convolution. \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = ResConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        from_up = self.up(from_up)\n",
    "        diffY = from_down.size()[2] - from_up.size()[2]\n",
    "        diffX = from_down.size()[3] - from_up.size()[3]\n",
    "        from_up = F.pad(from_up, [diffX // 2, diffX - diffX // 2,\n",
    "                                  diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([from_down, from_up], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetResNet101(nn.Module):\n",
    "    def __init__(self, n_classes=1):\n",
    "        super(UNetResNet101, self).__init__()\n",
    "        base_model = models.resnet101(pretrained=True)\n",
    "        self.base_layers = list(base_model.children())\n",
    "        \n",
    "        # Extract layers from ResNet101\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3])  # conv1, bn1, relu\n",
    "        self.maxpool = self.base_layers[3]\n",
    "        self.layer1 = self.base_layers[4]  # Output: 256 channels\n",
    "        self.layer2 = self.base_layers[5]  # Output: 512 channels\n",
    "        self.layer3 = self.base_layers[6]  # Output: 1024 channels\n",
    "        self.layer4 = self.base_layers[7]  # Output: 2048 channels\n",
    "\n",
    "        # Decoder (make sure the channel numbers match the skip connection outputs)\n",
    "        self.up4 = UpConv(2048 + 1024, 1024)  # Concatenating x3 (1024) and x4 (2048) -> 3072 channels\n",
    "        self.up3 = UpConv(1024 + 512, 512)    # Concatenating x2 (512) and previous output (1024) -> 1536 channels\n",
    "        self.up2 = UpConv(512 + 256, 256)     # Concatenating x1 (256) and previous output (512) -> 768 channels\n",
    "        self.up1 = UpConv(256 + 64, 64)       # Concatenating x0 (64) and previous output (256) -> 320 channels\n",
    "\n",
    "        # Final upsampling and output convolution to match input size\n",
    "        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder: get intermediate features for skip connections\n",
    "        x0 = self.layer0(x)       # Early features, e.g., 64 channels\n",
    "        x1 = self.maxpool(x0)\n",
    "        x1 = self.layer1(x1)      # 256 channels\n",
    "        x2 = self.layer2(x1)      # 512 channels\n",
    "        x3 = self.layer3(x2)      # 1024 channels\n",
    "        x4 = self.layer4(x3)      # 2048 channels\n",
    "\n",
    "        # Decoder: use skip connections from intermediate features\n",
    "        x = self.up4(x3, x4)      # Upsample: x3 (from_down) + x4 (from_up)\n",
    "        x = self.up3(x2, x)       # Upsample: x2 + output of previous block\n",
    "        x = self.up2(x1, x)       # Upsample: x1 + output of previous block\n",
    "        x = self.up1(x0, x)       # Upsample: x0 + output of previous block\n",
    "\n",
    "        x = self.final_up(x)      # Final upsampling to the original size\n",
    "        x = self.final_conv(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22e554",
   "metadata": {},
   "source": [
    "**Dice loss + Binary cross-entropy loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f608ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # Flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb63fea",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6214ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a14816a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Assuming UNetResNet101 and DiceBCELoss are already imported\n",
    "# model = UNetResNet101().to(device)  # Ensure your model is the one with ResNet-101\n",
    "# loss_function = DiceBCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# num_epochs = 50  # Set the number of epochs you want to train for\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, masks in train_loader:\n",
    "#         images, masks = images.to(device), masks.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = loss_function(outputs, masks)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     # Compute training metrics in evaluation mode\n",
    "#     model.eval()\n",
    "#     train_loss = 0.0\n",
    "#     all_train_preds = []\n",
    "#     all_train_targets = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, masks in train_loader:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = loss_function(outputs, masks)\n",
    "#             train_loss += loss.item()\n",
    "#             # Threshold outputs and targets at 0.5 to obtain binary predictions\n",
    "#             preds = (outputs > 0.5).float()\n",
    "#             binary_masks = (masks > 0.5).float()\n",
    "#             all_train_preds.append(preds.cpu().numpy().flatten())\n",
    "#             all_train_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "#     all_train_preds = np.concatenate(all_train_preds)\n",
    "#     all_train_targets = np.concatenate(all_train_targets)\n",
    "#     train_f1 = f1_score(all_train_targets, all_train_preds)\n",
    "\n",
    "#     # Compute validation metrics\n",
    "#     val_loss = 0.0\n",
    "#     all_val_preds = []\n",
    "#     all_val_targets = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, masks in validation_loader:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = loss_function(outputs, masks)\n",
    "#             val_loss += loss.item()\n",
    "#             preds = (outputs > 0.5).float()\n",
    "#             binary_masks = (masks > 0.5).float()\n",
    "#             all_val_preds.append(preds.cpu().numpy().flatten())\n",
    "#             all_val_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "#     all_val_preds = np.concatenate(all_val_preds)\n",
    "#     all_val_targets = np.concatenate(all_val_targets)\n",
    "#     val_f1 = f1_score(all_val_targets, all_val_preds)\n",
    "\n",
    "#     # Print epoch summary with both training and validation metrics\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "#           f'Train Loss: {running_loss/len(train_loader):.4f}, Train F1: {train_f1:.4f}, '\n",
    "#           f'Validation Loss: {val_loss/len(validation_loader):.4f}, Validation F1: {val_f1:.4f}')\n",
    "    \n",
    "#     # Adjust learning rate based on the validation loss\n",
    "#     scheduler.step(val_loss/len(validation_loader))\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), 'unet_resnet101_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d316b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/30, Train Loss: 0.4800, Train F1: 0.9544, Val Loss: 0.3736, Val F1: 0.9460\n",
      "Fold 1, Epoch 2/30, Train Loss: 0.3216, Train F1: 0.9647, Val Loss: 0.3113, Val F1: 0.9586\n",
      "Fold 1, Epoch 3/30, Train Loss: 0.2732, Train F1: 0.9697, Val Loss: 0.2671, Val F1: 0.9653\n",
      "Fold 1, Epoch 4/30, Train Loss: 0.2332, Train F1: 0.9719, Val Loss: 0.2222, Val F1: 0.9696\n",
      "Fold 1, Epoch 5/30, Train Loss: 0.2105, Train F1: 0.9652, Val Loss: 0.2209, Val F1: 0.9627\n",
      "Fold 1, Epoch 6/30, Train Loss: 0.1897, Train F1: 0.9686, Val Loss: 0.1940, Val F1: 0.9656\n",
      "Fold 1, Epoch 7/30, Train Loss: 0.1707, Train F1: 0.9733, Val Loss: 0.1641, Val F1: 0.9718\n",
      "Fold 1, Epoch 8/30, Train Loss: 0.1579, Train F1: 0.9730, Val Loss: 0.1610, Val F1: 0.9701\n",
      "Fold 1, Epoch 9/30, Train Loss: 0.1454, Train F1: 0.9748, Val Loss: 0.1508, Val F1: 0.9714\n",
      "Fold 1, Epoch 10/30, Train Loss: 0.1305, Train F1: 0.9765, Val Loss: 0.1397, Val F1: 0.9726\n",
      "Saved overlay image at: overlay_images/fold1_epoch10.png\n",
      "Fold 1, Epoch 11/30, Train Loss: 0.1245, Train F1: 0.9725, Val Loss: 0.1430, Val F1: 0.9688\n",
      "Fold 1, Epoch 12/30, Train Loss: 0.1136, Train F1: 0.9768, Val Loss: 0.1196, Val F1: 0.9740\n",
      "Fold 1, Epoch 13/30, Train Loss: 0.1093, Train F1: 0.9758, Val Loss: 0.1134, Val F1: 0.9746\n",
      "Fold 1, Epoch 14/30, Train Loss: 0.1069, Train F1: 0.9751, Val Loss: 0.1108, Val F1: 0.9726\n",
      "Fold 1, Epoch 15/30, Train Loss: 0.1053, Train F1: 0.9741, Val Loss: 0.1344, Val F1: 0.9651\n",
      "Fold 1, Epoch 16/30, Train Loss: 0.1072, Train F1: 0.9711, Val Loss: 0.1215, Val F1: 0.9671\n",
      "Fold 1, Epoch 17/30, Train Loss: 0.1009, Train F1: 0.9769, Val Loss: 0.0999, Val F1: 0.9746\n",
      "Fold 1, Epoch 18/30, Train Loss: 0.0913, Train F1: 0.9765, Val Loss: 0.0993, Val F1: 0.9731\n",
      "Fold 1, Epoch 19/30, Train Loss: 0.0858, Train F1: 0.9783, Val Loss: 0.0919, Val F1: 0.9757\n",
      "Fold 1, Epoch 20/30, Train Loss: 0.0853, Train F1: 0.9786, Val Loss: 0.0889, Val F1: 0.9755\n",
      "Saved overlay image at: overlay_images/fold1_epoch20.png\n",
      "Fold 1, Epoch 21/30, Train Loss: 0.0796, Train F1: 0.9759, Val Loss: 0.1004, Val F1: 0.9716\n",
      "Fold 1, Epoch 22/30, Train Loss: 0.0768, Train F1: 0.9797, Val Loss: 0.0869, Val F1: 0.9757\n",
      "Fold 1, Epoch 23/30, Train Loss: 0.0742, Train F1: 0.9798, Val Loss: 0.0812, Val F1: 0.9769\n",
      "Fold 1, Epoch 24/30, Train Loss: 0.0713, Train F1: 0.9807, Val Loss: 0.0908, Val F1: 0.9737\n",
      "Fold 1, Epoch 25/30, Train Loss: 0.0693, Train F1: 0.9798, Val Loss: 0.0822, Val F1: 0.9756\n",
      "Fold 1, Epoch 26/30, Train Loss: 0.0701, Train F1: 0.9798, Val Loss: 0.0778, Val F1: 0.9767\n",
      "Fold 1, Epoch 27/30, Train Loss: 0.0678, Train F1: 0.9798, Val Loss: 0.0933, Val F1: 0.9724\n",
      "Fold 1, Epoch 28/30, Train Loss: 0.0646, Train F1: 0.9809, Val Loss: 0.0798, Val F1: 0.9758\n",
      "Early stopping triggered in fold 1 at epoch 28\n",
      "Starting fold 2/5\n",
      "Fold 2, Epoch 1/30, Train Loss: 0.5234, Train F1: 0.9564, Val Loss: 0.4489, Val F1: 0.9472\n",
      "Fold 2, Epoch 2/30, Train Loss: 0.3741, Train F1: 0.9660, Val Loss: 0.3907, Val F1: 0.9617\n",
      "Fold 2, Epoch 3/30, Train Loss: 0.3256, Train F1: 0.9593, Val Loss: 0.3413, Val F1: 0.9566\n",
      "Fold 2, Epoch 4/30, Train Loss: 0.2926, Train F1: 0.9727, Val Loss: 0.2948, Val F1: 0.9666\n",
      "Fold 2, Epoch 5/30, Train Loss: 0.2681, Train F1: 0.9717, Val Loss: 0.2716, Val F1: 0.9678\n",
      "Fold 2, Epoch 6/30, Train Loss: 0.2396, Train F1: 0.9580, Val Loss: 0.2806, Val F1: 0.9533\n",
      "Fold 2, Epoch 7/30, Train Loss: 0.2176, Train F1: 0.9731, Val Loss: 0.2240, Val F1: 0.9707\n",
      "Fold 2, Epoch 8/30, Train Loss: 0.2020, Train F1: 0.9718, Val Loss: 0.2143, Val F1: 0.9684\n",
      "Fold 2, Epoch 9/30, Train Loss: 0.1806, Train F1: 0.9737, Val Loss: 0.1923, Val F1: 0.9705\n",
      "Fold 2, Epoch 10/30, Train Loss: 0.1683, Train F1: 0.9743, Val Loss: 0.1596, Val F1: 0.9727\n",
      "Saved overlay image at: overlay_images/fold2_epoch10.png\n",
      "Fold 2, Epoch 11/30, Train Loss: 0.1527, Train F1: 0.9776, Val Loss: 0.1601, Val F1: 0.9732\n",
      "Fold 2, Epoch 12/30, Train Loss: 0.1427, Train F1: 0.9758, Val Loss: 0.1487, Val F1: 0.9727\n",
      "Fold 2, Epoch 13/30, Train Loss: 0.1326, Train F1: 0.9778, Val Loss: 0.1511, Val F1: 0.9740\n",
      "Fold 2, Epoch 14/30, Train Loss: 0.1260, Train F1: 0.9726, Val Loss: 0.1554, Val F1: 0.9674\n",
      "Fold 2, Epoch 15/30, Train Loss: 0.1212, Train F1: 0.9762, Val Loss: 0.1392, Val F1: 0.9716\n",
      "Fold 2, Epoch 16/30, Train Loss: 0.1130, Train F1: 0.9774, Val Loss: 0.1248, Val F1: 0.9736\n",
      "Fold 2, Epoch 17/30, Train Loss: 0.1087, Train F1: 0.9779, Val Loss: 0.1146, Val F1: 0.9749\n",
      "Fold 2, Epoch 18/30, Train Loss: 0.1089, Train F1: 0.9777, Val Loss: 0.1138, Val F1: 0.9730\n",
      "Fold 2, Epoch 19/30, Train Loss: 0.0993, Train F1: 0.9784, Val Loss: 0.1125, Val F1: 0.9738\n",
      "Fold 2, Epoch 20/30, Train Loss: 0.0938, Train F1: 0.9803, Val Loss: 0.1037, Val F1: 0.9751\n",
      "Saved overlay image at: overlay_images/fold2_epoch20.png\n",
      "Fold 2, Epoch 21/30, Train Loss: 0.0886, Train F1: 0.9797, Val Loss: 0.0973, Val F1: 0.9751\n",
      "Fold 2, Epoch 22/30, Train Loss: 0.0852, Train F1: 0.9799, Val Loss: 0.1007, Val F1: 0.9746\n",
      "Fold 2, Epoch 23/30, Train Loss: 0.0806, Train F1: 0.9811, Val Loss: 0.0948, Val F1: 0.9764\n",
      "Fold 2, Epoch 24/30, Train Loss: 0.0776, Train F1: 0.9808, Val Loss: 0.0889, Val F1: 0.9765\n",
      "Fold 2, Epoch 25/30, Train Loss: 0.0750, Train F1: 0.9812, Val Loss: 0.0915, Val F1: 0.9748\n",
      "Fold 2, Epoch 26/30, Train Loss: 0.0755, Train F1: 0.9807, Val Loss: 0.0924, Val F1: 0.9744\n",
      "Fold 2, Epoch 27/30, Train Loss: 0.0712, Train F1: 0.9820, Val Loss: 0.0838, Val F1: 0.9763\n",
      "Fold 2, Epoch 28/30, Train Loss: 0.0918, Train F1: 0.9612, Val Loss: 0.1556, Val F1: 0.9566\n",
      "Fold 2, Epoch 29/30, Train Loss: 0.0957, Train F1: 0.9736, Val Loss: 0.1002, Val F1: 0.9700\n",
      "Early stopping triggered in fold 2 at epoch 29\n",
      "Starting fold 3/5\n",
      "Fold 3, Epoch 1/30, Train Loss: 0.5494, Train F1: 0.9581, Val Loss: 0.4721, Val F1: 0.9545\n",
      "Fold 3, Epoch 2/30, Train Loss: 0.4082, Train F1: 0.9689, Val Loss: 0.3875, Val F1: 0.9666\n",
      "Fold 3, Epoch 3/30, Train Loss: 0.3490, Train F1: 0.9640, Val Loss: 0.3490, Val F1: 0.9618\n",
      "Fold 3, Epoch 4/30, Train Loss: 0.3126, Train F1: 0.9720, Val Loss: 0.3110, Val F1: 0.9695\n",
      "Fold 3, Epoch 5/30, Train Loss: 0.2845, Train F1: 0.9748, Val Loss: 0.2798, Val F1: 0.9715\n",
      "Fold 3, Epoch 6/30, Train Loss: 0.2567, Train F1: 0.9691, Val Loss: 0.2643, Val F1: 0.9672\n",
      "Fold 3, Epoch 7/30, Train Loss: 0.2426, Train F1: 0.9710, Val Loss: 0.2669, Val F1: 0.9627\n",
      "Fold 3, Epoch 8/30, Train Loss: 0.2146, Train F1: 0.9733, Val Loss: 0.2157, Val F1: 0.9722\n",
      "Fold 3, Epoch 9/30, Train Loss: 0.1944, Train F1: 0.9761, Val Loss: 0.1964, Val F1: 0.9741\n",
      "Fold 3, Epoch 10/30, Train Loss: 0.1776, Train F1: 0.9778, Val Loss: 0.1800, Val F1: 0.9750\n",
      "Saved overlay image at: overlay_images/fold3_epoch10.png\n",
      "Fold 3, Epoch 11/30, Train Loss: 0.1689, Train F1: 0.9747, Val Loss: 0.1855, Val F1: 0.9706\n",
      "Fold 3, Epoch 12/30, Train Loss: 0.1681, Train F1: 0.9746, Val Loss: 0.1662, Val F1: 0.9714\n",
      "Fold 3, Epoch 13/30, Train Loss: 0.1633, Train F1: 0.9654, Val Loss: 0.1928, Val F1: 0.9629\n",
      "Fold 3, Epoch 14/30, Train Loss: 0.1423, Train F1: 0.9711, Val Loss: 0.1613, Val F1: 0.9677\n",
      "Fold 3, Epoch 15/30, Train Loss: 0.1328, Train F1: 0.9784, Val Loss: 0.1405, Val F1: 0.9755\n",
      "Fold 3, Epoch 16/30, Train Loss: 0.1213, Train F1: 0.9789, Val Loss: 0.1295, Val F1: 0.9758\n",
      "Fold 3, Epoch 17/30, Train Loss: 0.1180, Train F1: 0.9779, Val Loss: 0.1301, Val F1: 0.9743\n",
      "Fold 3, Epoch 18/30, Train Loss: 0.1112, Train F1: 0.9788, Val Loss: 0.1169, Val F1: 0.9759\n",
      "Fold 3, Epoch 19/30, Train Loss: 0.1090, Train F1: 0.9747, Val Loss: 0.1299, Val F1: 0.9712\n",
      "Fold 3, Epoch 20/30, Train Loss: 0.1185, Train F1: 0.9622, Val Loss: 0.1537, Val F1: 0.9581\n",
      "Saved overlay image at: overlay_images/fold3_epoch20.png\n",
      "Fold 3, Epoch 21/30, Train Loss: 0.1177, Train F1: 0.9749, Val Loss: 0.1111, Val F1: 0.9724\n",
      "Fold 3, Epoch 22/30, Train Loss: 0.1103, Train F1: 0.9746, Val Loss: 0.1179, Val F1: 0.9715\n",
      "Fold 3, Epoch 23/30, Train Loss: 0.0974, Train F1: 0.9784, Val Loss: 0.1030, Val F1: 0.9747\n",
      "Early stopping triggered in fold 3 at epoch 23\n",
      "Starting fold 4/5\n",
      "Fold 4, Epoch 1/30, Train Loss: 0.6319, Train F1: 0.9432, Val Loss: 0.4977, Val F1: 0.9501\n",
      "Fold 4, Epoch 2/30, Train Loss: 0.4744, Train F1: 0.9554, Val Loss: 0.4315, Val F1: 0.9590\n",
      "Fold 4, Epoch 3/30, Train Loss: 0.4149, Train F1: 0.9700, Val Loss: 0.3747, Val F1: 0.9716\n",
      "Fold 4, Epoch 4/30, Train Loss: 0.3766, Train F1: 0.9655, Val Loss: 0.3283, Val F1: 0.9697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 5/30, Train Loss: 0.3518, Train F1: 0.9704, Val Loss: 0.3014, Val F1: 0.9723\n",
      "Fold 4, Epoch 6/30, Train Loss: 0.3160, Train F1: 0.9718, Val Loss: 0.2811, Val F1: 0.9737\n",
      "Fold 4, Epoch 7/30, Train Loss: 0.2832, Train F1: 0.9711, Val Loss: 0.2610, Val F1: 0.9717\n",
      "Fold 4, Epoch 8/30, Train Loss: 0.2532, Train F1: 0.9710, Val Loss: 0.2298, Val F1: 0.9725\n",
      "Fold 4, Epoch 9/30, Train Loss: 0.2314, Train F1: 0.9758, Val Loss: 0.2161, Val F1: 0.9768\n",
      "Fold 4, Epoch 10/30, Train Loss: 0.2162, Train F1: 0.9735, Val Loss: 0.1959, Val F1: 0.9749\n",
      "Saved overlay image at: overlay_images/fold4_epoch10.png\n",
      "Fold 4, Epoch 11/30, Train Loss: 0.2103, Train F1: 0.9721, Val Loss: 0.1974, Val F1: 0.9723\n",
      "Fold 4, Epoch 12/30, Train Loss: 0.1915, Train F1: 0.9726, Val Loss: 0.1687, Val F1: 0.9745\n",
      "Fold 4, Epoch 13/30, Train Loss: 0.1796, Train F1: 0.9748, Val Loss: 0.1599, Val F1: 0.9754\n",
      "Fold 4, Epoch 14/30, Train Loss: 0.1622, Train F1: 0.9769, Val Loss: 0.1503, Val F1: 0.9768\n",
      "Early stopping triggered in fold 4 at epoch 14\n",
      "Starting fold 5/5\n",
      "Fold 5, Epoch 1/30, Train Loss: 0.5809, Train F1: 0.9568, Val Loss: 0.4831, Val F1: 0.9503\n",
      "Fold 5, Epoch 2/30, Train Loss: 0.4109, Train F1: 0.9619, Val Loss: 0.4152, Val F1: 0.9579\n",
      "Fold 5, Epoch 3/30, Train Loss: 0.3619, Train F1: 0.9703, Val Loss: 0.3660, Val F1: 0.9665\n",
      "Fold 5, Epoch 4/30, Train Loss: 0.3269, Train F1: 0.9704, Val Loss: 0.3218, Val F1: 0.9682\n",
      "Fold 5, Epoch 5/30, Train Loss: 0.2942, Train F1: 0.9711, Val Loss: 0.2985, Val F1: 0.9678\n",
      "Fold 5, Epoch 6/30, Train Loss: 0.2692, Train F1: 0.9727, Val Loss: 0.2740, Val F1: 0.9689\n",
      "Fold 5, Epoch 7/30, Train Loss: 0.2503, Train F1: 0.9706, Val Loss: 0.2571, Val F1: 0.9660\n",
      "Fold 5, Epoch 8/30, Train Loss: 0.2371, Train F1: 0.9732, Val Loss: 0.2346, Val F1: 0.9713\n",
      "Fold 5, Epoch 9/30, Train Loss: 0.2098, Train F1: 0.9751, Val Loss: 0.2285, Val F1: 0.9706\n",
      "Fold 5, Epoch 10/30, Train Loss: 0.1918, Train F1: 0.9718, Val Loss: 0.2158, Val F1: 0.9669\n",
      "Saved overlay image at: overlay_images/fold5_epoch10.png\n",
      "Fold 5, Epoch 11/30, Train Loss: 0.1775, Train F1: 0.9774, Val Loss: 0.1858, Val F1: 0.9740\n",
      "Fold 5, Epoch 12/30, Train Loss: 0.1631, Train F1: 0.9787, Val Loss: 0.1658, Val F1: 0.9756\n",
      "Fold 5, Epoch 13/30, Train Loss: 0.1567, Train F1: 0.9710, Val Loss: 0.1838, Val F1: 0.9658\n",
      "Fold 5, Epoch 14/30, Train Loss: 0.1494, Train F1: 0.9778, Val Loss: 0.1523, Val F1: 0.9742\n",
      "Fold 5, Epoch 15/30, Train Loss: 0.1386, Train F1: 0.9739, Val Loss: 0.1662, Val F1: 0.9700\n",
      "Fold 5, Epoch 16/30, Train Loss: 0.1318, Train F1: 0.9782, Val Loss: 0.1347, Val F1: 0.9764\n",
      "Fold 5, Epoch 17/30, Train Loss: 0.1212, Train F1: 0.9777, Val Loss: 0.1319, Val F1: 0.9746\n",
      "Fold 5, Epoch 18/30, Train Loss: 0.1164, Train F1: 0.9787, Val Loss: 0.1248, Val F1: 0.9757\n",
      "Fold 5, Epoch 19/30, Train Loss: 0.1101, Train F1: 0.9795, Val Loss: 0.1229, Val F1: 0.9755\n",
      "Fold 5, Epoch 20/30, Train Loss: 0.1110, Train F1: 0.9728, Val Loss: 0.1388, Val F1: 0.9677\n",
      "Saved overlay image at: overlay_images/fold5_epoch20.png\n",
      "Fold 5, Epoch 21/30, Train Loss: 0.1233, Train F1: 0.9722, Val Loss: 0.1318, Val F1: 0.9677\n",
      "Early stopping triggered in fold 5 at epoch 21\n",
      "Best model saved with validation F1: 0.9769\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Assume full_dataset, device, UNetResNet101, DiceBCELoss, and joint_transform are defined already.\n",
    "# full_dataset = CustomDataset('denoised_training_set', 'masked_annotations', joint_transform=joint_transform)\n",
    "\n",
    "# Number of folds and training epochs\n",
    "num_folds = 5\n",
    "num_epochs = 30\n",
    "patience = 5\n",
    "\n",
    "# Set up KFold splitter\n",
    "indices = np.arange(len(full_dataset))\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Folder to save overlay images (optional)\n",
    "overlay_folder = 'overlay_images'\n",
    "os.makedirs(overlay_folder, exist_ok=True)\n",
    "\n",
    "# Helper function for overlay (as before)\n",
    "def overlay_mask_on_image(image, mask, color=(255, 0, 0), alpha=0.4):\n",
    "    color_mask = np.zeros_like(image)\n",
    "    color_mask[mask == 255] = color\n",
    "    overlay = cv2.addWeighted(image, 1 - alpha, color_mask, alpha, 0)\n",
    "    return overlay\n",
    "\n",
    "# Variables to track the best model overall (based on validation F1 score)\n",
    "best_val_f1_overall = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Begin cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
    "    print(f\"Starting fold {fold+1}/{num_folds}\")\n",
    "    \n",
    "    # Create Subset datasets for current fold\n",
    "    train_subset = Subset(full_dataset, train_idx)\n",
    "    val_subset = Subset(full_dataset, val_idx)\n",
    "    \n",
    "    # Create dataloaders for current fold\n",
    "    train_loader = DataLoader(train_subset, batch_size=10, shuffle=True)\n",
    "    validation_loader = DataLoader(val_subset, batch_size=10, shuffle=False)\n",
    "    \n",
    "    # Reinitialize the model, loss, optimizer, and scheduler for each fold\n",
    "    model = UNetResNet101().to(device)\n",
    "    loss_function = DiceBCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    \n",
    "    best_fold_val_f1 = -1.0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss_epoch = running_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate training set for F1 score\n",
    "        model.eval()\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in train_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                binary_masks = (masks > 0.5).float()\n",
    "                all_train_preds.append(preds.cpu().numpy().flatten())\n",
    "                all_train_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "        all_train_preds = np.concatenate(all_train_preds)\n",
    "        all_train_targets = np.concatenate(all_train_targets)\n",
    "        train_f1 = f1_score(all_train_targets, all_train_preds)\n",
    "        \n",
    "        # Evaluate validation set\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in validation_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_function(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                preds = (outputs > 0.5).float()\n",
    "                binary_masks = (masks > 0.5).float()\n",
    "                all_val_preds.append(preds.cpu().numpy().flatten())\n",
    "                all_val_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "        all_val_preds = np.concatenate(all_val_preds)\n",
    "        all_val_targets = np.concatenate(all_val_targets)\n",
    "        val_f1 = f1_score(all_val_targets, all_val_preds)\n",
    "        val_loss_epoch = val_loss / len(validation_loader)\n",
    "        \n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss_epoch:.4f}, Train F1: {train_f1:.4f}, \"\n",
    "              f\"Val Loss: {val_loss_epoch:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Adjust learning rate based on validation loss\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        \n",
    "        # If current validation F1 is best so far, update best model state across folds/epochs\n",
    "        if val_f1 > best_val_f1_overall:\n",
    "            best_val_f1_overall = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "        # Early stopping for current fold: check if current validation F1 improved over best_fold_val_f1\n",
    "        if val_f1 > best_fold_val_f1:\n",
    "            best_fold_val_f1 = val_f1\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered in fold {fold+1} at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Optionally, every 10 epochs, save an overlay visualization from validation\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                for images, _ in validation_loader:\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    preds = (outputs > 0.5).float()\n",
    "                    img_tensor = images[0].cpu()  # first image from the batch\n",
    "                    pred_tensor = preds[0].cpu()  # corresponding prediction\n",
    "                    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "                    img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "                    mask_np = pred_tensor.squeeze().numpy()\n",
    "                    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "                    overlay_img = overlay_mask_on_image(img_uint8, mask_uint8, color=(255, 0, 0), alpha=0.4)\n",
    "                    # Convert to BGR for cv2.imwrite\n",
    "                    overlay_bgr = cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)\n",
    "                    overlay_save_path = os.path.join(overlay_folder, f'fold{fold+1}_epoch{epoch+1}.png')\n",
    "                    cv2.imwrite(overlay_save_path, overlay_bgr)\n",
    "                    print(f\"Saved overlay image at: {overlay_save_path}\")\n",
    "                    break  # Process only one batch for overlay visualization\n",
    "\n",
    "# After all folds, save the best model\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'best_unet_resnet101_model.pth')\n",
    "    print(f\"Best model saved with validation F1: {best_val_f1_overall:.4f}\")\n",
    "else:\n",
    "    print(\"No model was saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c257ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = UNetResNet101(n_classes=1).to(device)\n",
    "# dummy_input = torch.rand(1, 3, 224, 224).to(device)  # Adjust input size as necessary\n",
    "# output = model(dummy_input)\n",
    "# print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214323f5",
   "metadata": {},
   "source": [
    "**Running Model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3acc027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for 500_HC.png at output_segmentations/seg_500_HC.png\n",
      "Saved segmentation for 501_HC.png at output_segmentations/seg_501_HC.png\n",
      "Saved segmentation for 502_HC.png at output_segmentations/seg_502_HC.png\n",
      "Saved segmentation for 503_HC.png at output_segmentations/seg_503_HC.png\n",
      "Saved segmentation for 504_HC.png at output_segmentations/seg_504_HC.png\n",
      "Saved segmentation for 505_HC.png at output_segmentations/seg_505_HC.png\n",
      "Saved segmentation for 506_HC.png at output_segmentations/seg_506_HC.png\n",
      "Saved segmentation for 507_2HC.png at output_segmentations/seg_507_2HC.png\n",
      "Saved segmentation for 507_HC.png at output_segmentations/seg_507_HC.png\n",
      "Saved segmentation for 508_HC.png at output_segmentations/seg_508_HC.png\n",
      "Saved segmentation for 509_HC.png at output_segmentations/seg_509_HC.png\n",
      "Saved segmentation for 510_HC.png at output_segmentations/seg_510_HC.png\n",
      "Saved segmentation for 511_HC.png at output_segmentations/seg_511_HC.png\n",
      "Saved segmentation for 512_HC.png at output_segmentations/seg_512_HC.png\n",
      "Saved segmentation for 513_HC.png at output_segmentations/seg_513_HC.png\n",
      "Saved segmentation for 514_HC.png at output_segmentations/seg_514_HC.png\n",
      "Saved segmentation for 515_HC.png at output_segmentations/seg_515_HC.png\n",
      "Saved segmentation for 516_HC.png at output_segmentations/seg_516_HC.png\n",
      "Saved segmentation for 517_HC.png at output_segmentations/seg_517_HC.png\n",
      "Saved segmentation for 518_HC.png at output_segmentations/seg_518_HC.png\n",
      "Saved segmentation for 519_HC.png at output_segmentations/seg_519_HC.png\n",
      "Saved segmentation for 520_HC.png at output_segmentations/seg_520_HC.png\n",
      "Saved segmentation for 521_HC.png at output_segmentations/seg_521_HC.png\n",
      "Saved segmentation for 522_HC.png at output_segmentations/seg_522_HC.png\n",
      "Saved segmentation for 523_HC.png at output_segmentations/seg_523_HC.png\n",
      "Saved segmentation for 524_HC.png at output_segmentations/seg_524_HC.png\n",
      "Saved segmentation for 525_HC.png at output_segmentations/seg_525_HC.png\n",
      "Saved segmentation for 526_2HC.png at output_segmentations/seg_526_2HC.png\n",
      "Saved segmentation for 526_HC.png at output_segmentations/seg_526_HC.png\n",
      "Saved segmentation for 527_HC.png at output_segmentations/seg_527_HC.png\n",
      "Saved segmentation for 528_HC.png at output_segmentations/seg_528_HC.png\n",
      "Saved segmentation for 529_HC.png at output_segmentations/seg_529_HC.png\n",
      "Saved segmentation for 530_HC.png at output_segmentations/seg_530_HC.png\n",
      "Saved segmentation for 531_2HC.png at output_segmentations/seg_531_2HC.png\n",
      "Saved segmentation for 531_HC.png at output_segmentations/seg_531_HC.png\n",
      "Saved segmentation for 532_2HC.png at output_segmentations/seg_532_2HC.png\n",
      "Saved segmentation for 532_HC.png at output_segmentations/seg_532_HC.png\n",
      "Saved segmentation for 533_HC.png at output_segmentations/seg_533_HC.png\n",
      "Saved segmentation for 534_HC.png at output_segmentations/seg_534_HC.png\n",
      "Saved segmentation for 535_HC.png at output_segmentations/seg_535_HC.png\n",
      "Saved segmentation for 536_HC.png at output_segmentations/seg_536_HC.png\n",
      "Saved segmentation for 537_HC.png at output_segmentations/seg_537_HC.png\n",
      "Saved segmentation for 538_HC.png at output_segmentations/seg_538_HC.png\n",
      "Saved segmentation for 539_HC.png at output_segmentations/seg_539_HC.png\n",
      "Saved segmentation for 540_HC.png at output_segmentations/seg_540_HC.png\n",
      "Saved segmentation for 541_2HC.png at output_segmentations/seg_541_2HC.png\n",
      "Saved segmentation for 541_HC.png at output_segmentations/seg_541_HC.png\n",
      "Saved segmentation for 542_HC.png at output_segmentations/seg_542_HC.png\n",
      "Saved segmentation for 543_HC.png at output_segmentations/seg_543_HC.png\n",
      "Saved segmentation for 544_2HC.png at output_segmentations/seg_544_2HC.png\n",
      "Saved segmentation for 544_HC.png at output_segmentations/seg_544_HC.png\n",
      "Saved segmentation for 545_2HC.png at output_segmentations/seg_545_2HC.png\n",
      "Saved segmentation for 545_HC.png at output_segmentations/seg_545_HC.png\n",
      "Saved segmentation for 546_HC.png at output_segmentations/seg_546_HC.png\n",
      "Saved segmentation for 547_2HC.png at output_segmentations/seg_547_2HC.png\n",
      "Saved segmentation for 547_HC.png at output_segmentations/seg_547_HC.png\n",
      "Saved segmentation for 548_HC.png at output_segmentations/seg_548_HC.png\n",
      "Saved segmentation for 549_HC.png at output_segmentations/seg_549_HC.png\n",
      "Saved segmentation for 550_HC.png at output_segmentations/seg_550_HC.png\n",
      "Saved segmentation for 551_HC.png at output_segmentations/seg_551_HC.png\n",
      "Saved segmentation for 552_HC.png at output_segmentations/seg_552_HC.png\n",
      "Saved segmentation for 553_HC.png at output_segmentations/seg_553_HC.png\n",
      "Saved segmentation for 554_HC.png at output_segmentations/seg_554_HC.png\n",
      "Saved segmentation for 555_HC.png at output_segmentations/seg_555_HC.png\n",
      "Saved segmentation for 556_2HC.png at output_segmentations/seg_556_2HC.png\n",
      "Saved segmentation for 556_HC.png at output_segmentations/seg_556_HC.png\n",
      "Saved segmentation for 557_HC.png at output_segmentations/seg_557_HC.png\n",
      "Saved segmentation for 558_HC.png at output_segmentations/seg_558_HC.png\n",
      "Saved segmentation for 559_HC.png at output_segmentations/seg_559_HC.png\n",
      "Saved segmentation for 560_HC.png at output_segmentations/seg_560_HC.png\n",
      "Saved segmentation for 561_2HC.png at output_segmentations/seg_561_2HC.png\n",
      "Saved segmentation for 561_3HC.png at output_segmentations/seg_561_3HC.png\n",
      "Saved segmentation for 561_HC.png at output_segmentations/seg_561_HC.png\n",
      "Saved segmentation for 562_HC.png at output_segmentations/seg_562_HC.png\n",
      "Saved segmentation for 563_HC.png at output_segmentations/seg_563_HC.png\n",
      "Saved segmentation for 564_HC.png at output_segmentations/seg_564_HC.png\n",
      "Saved segmentation for 565_HC.png at output_segmentations/seg_565_HC.png\n",
      "Saved segmentation for 566_2HC.png at output_segmentations/seg_566_2HC.png\n",
      "Saved segmentation for 566_HC.png at output_segmentations/seg_566_HC.png\n",
      "Saved segmentation for 567_2HC.png at output_segmentations/seg_567_2HC.png\n",
      "Saved segmentation for 567_HC.png at output_segmentations/seg_567_HC.png\n",
      "Saved segmentation for 568_HC.png at output_segmentations/seg_568_HC.png\n",
      "Saved segmentation for 569_HC.png at output_segmentations/seg_569_HC.png\n",
      "Saved segmentation for 570_2HC.png at output_segmentations/seg_570_2HC.png\n",
      "Saved segmentation for 570_3HC.png at output_segmentations/seg_570_3HC.png\n",
      "Saved segmentation for 570_HC.png at output_segmentations/seg_570_HC.png\n",
      "Saved segmentation for 571_HC.png at output_segmentations/seg_571_HC.png\n",
      "Saved segmentation for 572_HC.png at output_segmentations/seg_572_HC.png\n",
      "Saved segmentation for 573_HC.png at output_segmentations/seg_573_HC.png\n",
      "Saved segmentation for 574_2HC.png at output_segmentations/seg_574_2HC.png\n",
      "Saved segmentation for 574_HC.png at output_segmentations/seg_574_HC.png\n",
      "Saved segmentation for 575_HC.png at output_segmentations/seg_575_HC.png\n",
      "Saved segmentation for 576_HC.png at output_segmentations/seg_576_HC.png\n",
      "Saved segmentation for 577_HC.png at output_segmentations/seg_577_HC.png\n",
      "Saved segmentation for 578_HC.png at output_segmentations/seg_578_HC.png\n",
      "Saved segmentation for 579_HC.png at output_segmentations/seg_579_HC.png\n",
      "Saved segmentation for 580_HC.png at output_segmentations/seg_580_HC.png\n",
      "Saved segmentation for 581_HC.png at output_segmentations/seg_581_HC.png\n",
      "Saved segmentation for 582_HC.png at output_segmentations/seg_582_HC.png\n",
      "Saved segmentation for 583_2HC.png at output_segmentations/seg_583_2HC.png\n",
      "Saved segmentation for 583_HC.png at output_segmentations/seg_583_HC.png\n",
      "Saved segmentation for 584_HC.png at output_segmentations/seg_584_HC.png\n",
      "Saved segmentation for 585_HC.png at output_segmentations/seg_585_HC.png\n",
      "Saved segmentation for 586_HC.png at output_segmentations/seg_586_HC.png\n",
      "Saved segmentation for 587_2HC.png at output_segmentations/seg_587_2HC.png\n",
      "Saved segmentation for 587_HC.png at output_segmentations/seg_587_HC.png\n",
      "Saved segmentation for 588_2HC.png at output_segmentations/seg_588_2HC.png\n",
      "Saved segmentation for 588_HC.png at output_segmentations/seg_588_HC.png\n",
      "Saved segmentation for 589_HC.png at output_segmentations/seg_589_HC.png\n",
      "Saved segmentation for 590_HC.png at output_segmentations/seg_590_HC.png\n",
      "Saved segmentation for 591_HC.png at output_segmentations/seg_591_HC.png\n",
      "Saved segmentation for 592_2HC.png at output_segmentations/seg_592_2HC.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for 592_HC.png at output_segmentations/seg_592_HC.png\n",
      "Saved segmentation for 593_HC.png at output_segmentations/seg_593_HC.png\n",
      "Saved segmentation for 594_HC.png at output_segmentations/seg_594_HC.png\n",
      "Saved segmentation for 595_HC.png at output_segmentations/seg_595_HC.png\n",
      "Saved segmentation for 596_HC.png at output_segmentations/seg_596_HC.png\n",
      "Saved segmentation for 597_HC.png at output_segmentations/seg_597_HC.png\n",
      "Saved segmentation for 598_HC.png at output_segmentations/seg_598_HC.png\n",
      "Saved segmentation for 599_HC.png at output_segmentations/seg_599_HC.png\n",
      "Saved segmentation for 600_HC.png at output_segmentations/seg_600_HC.png\n",
      "Saved segmentation for 601_HC.png at output_segmentations/seg_601_HC.png\n",
      "Saved segmentation for 602_HC.png at output_segmentations/seg_602_HC.png\n",
      "Saved segmentation for 603_HC.png at output_segmentations/seg_603_HC.png\n",
      "Saved segmentation for 604_HC.png at output_segmentations/seg_604_HC.png\n",
      "Saved segmentation for 605_HC.png at output_segmentations/seg_605_HC.png\n",
      "Saved segmentation for 606_HC.png at output_segmentations/seg_606_HC.png\n",
      "Saved segmentation for 607_HC.png at output_segmentations/seg_607_HC.png\n",
      "Saved segmentation for 608_HC.png at output_segmentations/seg_608_HC.png\n",
      "Saved segmentation for 609_2HC.png at output_segmentations/seg_609_2HC.png\n",
      "Saved segmentation for 609_HC.png at output_segmentations/seg_609_HC.png\n",
      "Saved segmentation for 610_HC.png at output_segmentations/seg_610_HC.png\n",
      "Saved segmentation for 611_HC.png at output_segmentations/seg_611_HC.png\n",
      "Saved segmentation for 612_HC.png at output_segmentations/seg_612_HC.png\n",
      "Saved segmentation for 613_2HC.png at output_segmentations/seg_613_2HC.png\n",
      "Saved segmentation for 613_3HC.png at output_segmentations/seg_613_3HC.png\n",
      "Saved segmentation for 613_HC.png at output_segmentations/seg_613_HC.png\n",
      "Saved segmentation for 614_HC.png at output_segmentations/seg_614_HC.png\n",
      "Saved segmentation for 615_HC.png at output_segmentations/seg_615_HC.png\n",
      "Saved segmentation for 616_HC.png at output_segmentations/seg_616_HC.png\n",
      "Saved segmentation for 617_2HC.png at output_segmentations/seg_617_2HC.png\n",
      "Saved segmentation for 617_HC.png at output_segmentations/seg_617_HC.png\n",
      "Saved segmentation for 618_2HC.png at output_segmentations/seg_618_2HC.png\n",
      "Saved segmentation for 618_HC.png at output_segmentations/seg_618_HC.png\n",
      "Saved segmentation for 619_HC.png at output_segmentations/seg_619_HC.png\n",
      "Saved segmentation for 620_HC.png at output_segmentations/seg_620_HC.png\n",
      "Saved segmentation for 621_HC.png at output_segmentations/seg_621_HC.png\n",
      "Saved segmentation for 622_HC.png at output_segmentations/seg_622_HC.png\n",
      "Saved segmentation for 623_HC.png at output_segmentations/seg_623_HC.png\n",
      "Saved segmentation for 624_HC.png at output_segmentations/seg_624_HC.png\n",
      "Saved segmentation for 625_HC.png at output_segmentations/seg_625_HC.png\n",
      "Saved segmentation for 626_HC.png at output_segmentations/seg_626_HC.png\n",
      "Saved segmentation for 627_HC.png at output_segmentations/seg_627_HC.png\n",
      "Saved segmentation for 628_2HC.png at output_segmentations/seg_628_2HC.png\n",
      "Saved segmentation for 628_HC.png at output_segmentations/seg_628_HC.png\n",
      "Saved segmentation for 629_HC.png at output_segmentations/seg_629_HC.png\n",
      "Saved segmentation for 630_2HC.png at output_segmentations/seg_630_2HC.png\n",
      "Saved segmentation for 630_HC.png at output_segmentations/seg_630_HC.png\n",
      "Saved segmentation for 631_2HC.png at output_segmentations/seg_631_2HC.png\n",
      "Saved segmentation for 631_HC.png at output_segmentations/seg_631_HC.png\n",
      "Saved segmentation for 632_HC.png at output_segmentations/seg_632_HC.png\n",
      "Saved segmentation for 633_HC.png at output_segmentations/seg_633_HC.png\n",
      "Saved segmentation for 634_HC.png at output_segmentations/seg_634_HC.png\n",
      "Saved segmentation for 635_HC.png at output_segmentations/seg_635_HC.png\n",
      "Saved segmentation for 636_HC.png at output_segmentations/seg_636_HC.png\n",
      "Saved segmentation for 637_HC.png at output_segmentations/seg_637_HC.png\n",
      "Saved segmentation for 638_HC.png at output_segmentations/seg_638_HC.png\n",
      "Saved segmentation for 639_2HC.png at output_segmentations/seg_639_2HC.png\n",
      "Saved segmentation for 639_HC.png at output_segmentations/seg_639_HC.png\n",
      "Saved segmentation for 640_HC.png at output_segmentations/seg_640_HC.png\n",
      "Saved segmentation for 641_HC.png at output_segmentations/seg_641_HC.png\n",
      "Saved segmentation for 642_HC.png at output_segmentations/seg_642_HC.png\n",
      "Saved segmentation for 643_HC.png at output_segmentations/seg_643_HC.png\n",
      "Saved segmentation for 644_HC.png at output_segmentations/seg_644_HC.png\n",
      "Saved segmentation for 645_HC.png at output_segmentations/seg_645_HC.png\n",
      "Saved segmentation for 646_HC.png at output_segmentations/seg_646_HC.png\n",
      "Saved segmentation for 647_HC.png at output_segmentations/seg_647_HC.png\n",
      "Saved segmentation for 648_2HC.png at output_segmentations/seg_648_2HC.png\n",
      "Saved segmentation for 648_HC.png at output_segmentations/seg_648_HC.png\n",
      "Saved segmentation for 649_HC.png at output_segmentations/seg_649_HC.png\n",
      "Saved segmentation for 650_HC.png at output_segmentations/seg_650_HC.png\n",
      "Saved segmentation for 651_HC.png at output_segmentations/seg_651_HC.png\n",
      "Saved segmentation for 652_2HC.png at output_segmentations/seg_652_2HC.png\n",
      "Saved segmentation for 652_HC.png at output_segmentations/seg_652_HC.png\n",
      "Saved segmentation for 653_HC.png at output_segmentations/seg_653_HC.png\n",
      "Saved segmentation for 654_HC.png at output_segmentations/seg_654_HC.png\n",
      "Saved segmentation for 655_HC.png at output_segmentations/seg_655_HC.png\n",
      "Saved segmentation for 656_HC.png at output_segmentations/seg_656_HC.png\n",
      "Saved segmentation for 657_2HC.png at output_segmentations/seg_657_2HC.png\n",
      "Saved segmentation for 657_HC.png at output_segmentations/seg_657_HC.png\n",
      "Saved segmentation for 658_HC.png at output_segmentations/seg_658_HC.png\n",
      "Saved segmentation for 659_HC.png at output_segmentations/seg_659_HC.png\n",
      "Saved segmentation for 660_HC.png at output_segmentations/seg_660_HC.png\n",
      "Saved segmentation for 661_HC.png at output_segmentations/seg_661_HC.png\n",
      "Saved segmentation for 662_HC.png at output_segmentations/seg_662_HC.png\n",
      "Saved segmentation for 663_HC.png at output_segmentations/seg_663_HC.png\n",
      "Saved segmentation for 664_HC.png at output_segmentations/seg_664_HC.png\n",
      "Saved segmentation for 665_HC.png at output_segmentations/seg_665_HC.png\n",
      "Saved segmentation for 666_HC.png at output_segmentations/seg_666_HC.png\n",
      "Saved segmentation for 667_HC.png at output_segmentations/seg_667_HC.png\n",
      "Saved segmentation for 668_HC.png at output_segmentations/seg_668_HC.png\n",
      "Saved segmentation for 669_HC.png at output_segmentations/seg_669_HC.png\n",
      "Saved segmentation for 670_HC.png at output_segmentations/seg_670_HC.png\n",
      "Saved segmentation for 671_2HC.png at output_segmentations/seg_671_2HC.png\n",
      "Saved segmentation for 671_3HC.png at output_segmentations/seg_671_3HC.png\n",
      "Saved segmentation for 671_HC.png at output_segmentations/seg_671_HC.png\n",
      "Saved segmentation for 672_HC.png at output_segmentations/seg_672_HC.png\n",
      "Saved segmentation for 673_2HC.png at output_segmentations/seg_673_2HC.png\n",
      "Saved segmentation for 673_HC.png at output_segmentations/seg_673_HC.png\n",
      "Saved segmentation for 674_HC.png at output_segmentations/seg_674_HC.png\n",
      "Saved segmentation for 675_2HC.png at output_segmentations/seg_675_2HC.png\n",
      "Saved segmentation for 675_HC.png at output_segmentations/seg_675_HC.png\n",
      "Saved segmentation for 676_HC.png at output_segmentations/seg_676_HC.png\n",
      "Saved segmentation for 677_2HC.png at output_segmentations/seg_677_2HC.png\n",
      "Saved segmentation for 677_HC.png at output_segmentations/seg_677_HC.png\n",
      "Saved segmentation for 678_HC.png at output_segmentations/seg_678_HC.png\n",
      "Saved segmentation for 679_HC.png at output_segmentations/seg_679_HC.png\n",
      "Saved segmentation for 680_HC.png at output_segmentations/seg_680_HC.png\n",
      "Saved segmentation for 681_HC.png at output_segmentations/seg_681_HC.png\n",
      "Saved segmentation for 682_HC.png at output_segmentations/seg_682_HC.png\n",
      "Saved segmentation for 683_HC.png at output_segmentations/seg_683_HC.png\n",
      "Saved segmentation for 684_HC.png at output_segmentations/seg_684_HC.png\n",
      "Saved segmentation for 685_HC.png at output_segmentations/seg_685_HC.png\n",
      "Saved segmentation for 686_2HC.png at output_segmentations/seg_686_2HC.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for 686_HC.png at output_segmentations/seg_686_HC.png\n",
      "Saved segmentation for 687_HC.png at output_segmentations/seg_687_HC.png\n",
      "Saved segmentation for 688_HC.png at output_segmentations/seg_688_HC.png\n",
      "Saved segmentation for 689_HC.png at output_segmentations/seg_689_HC.png\n",
      "Saved segmentation for 690_2HC.png at output_segmentations/seg_690_2HC.png\n",
      "Saved segmentation for 690_HC.png at output_segmentations/seg_690_HC.png\n",
      "Saved segmentation for 691_HC.png at output_segmentations/seg_691_HC.png\n",
      "Saved segmentation for 692_2HC.png at output_segmentations/seg_692_2HC.png\n",
      "Saved segmentation for 692_HC.png at output_segmentations/seg_692_HC.png\n",
      "Saved segmentation for 693_HC.png at output_segmentations/seg_693_HC.png\n",
      "Saved segmentation for 694_HC.png at output_segmentations/seg_694_HC.png\n",
      "Saved segmentation for 695_HC.png at output_segmentations/seg_695_HC.png\n",
      "Saved segmentation for 696_HC.png at output_segmentations/seg_696_HC.png\n",
      "Saved segmentation for 697_HC.png at output_segmentations/seg_697_HC.png\n",
      "Saved segmentation for 698_HC.png at output_segmentations/seg_698_HC.png\n",
      "Saved segmentation for 699_HC.png at output_segmentations/seg_699_HC.png\n",
      "Saved segmentation for 700_HC.png at output_segmentations/seg_700_HC.png\n",
      "Saved segmentation for 701_HC.png at output_segmentations/seg_701_HC.png\n",
      "Saved segmentation for 702_HC.png at output_segmentations/seg_702_HC.png\n",
      "Saved segmentation for 703_HC.png at output_segmentations/seg_703_HC.png\n",
      "Saved segmentation for 704_2HC.png at output_segmentations/seg_704_2HC.png\n",
      "Saved segmentation for 704_HC.png at output_segmentations/seg_704_HC.png\n",
      "Saved segmentation for 705_HC.png at output_segmentations/seg_705_HC.png\n",
      "Saved segmentation for 706_HC.png at output_segmentations/seg_706_HC.png\n",
      "Saved segmentation for 707_HC.png at output_segmentations/seg_707_HC.png\n",
      "Saved segmentation for 708_2HC.png at output_segmentations/seg_708_2HC.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a test dataset class\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_path\n",
    "\n",
    "# Define deterministic transforms for test data\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((572, 572)),\n",
    "    transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Set the directory where your test images are stored\n",
    "test_dir = 'denoised_test_set'  # Adjust this to your actual test directory path\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = TestDataset(test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Instantiate your model (assumed defined in the notebook)\n",
    "model = UNetResNet101(n_classes=1).to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('best_unet_resnet101_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Create an output directory for the segmentation results\n",
    "output_dir = 'output_segmentations'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Inference loop: run the model on each test image and save the segmentation mask\n",
    "for image, image_path in test_loader:\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Get the probability map from the model (sigmoid already applied)\n",
    "        # Threshold the probability map to obtain a binary segmentation mask\n",
    "        seg_mask = (output > 0.5).float()\n",
    "    \n",
    "    # Convert tensor to NumPy array and scale to 0-255 for visualization/saving\n",
    "    seg_mask_np = seg_mask.cpu().numpy().squeeze() * 255\n",
    "\n",
    "    # Generate output filename based on input image name\n",
    "    base_name = os.path.basename(image_path[0])\n",
    "    output_filename = os.path.join(output_dir, f\"seg_{base_name}\")\n",
    "    \n",
    "    # Save the segmentation mask image using OpenCV\n",
    "    cv2.imwrite(output_filename, seg_mask_np.astype('uint8'))\n",
    "    \n",
    "    # Optionally, display the segmentation mask using matplotlib\n",
    "#     plt.imshow(seg_mask_np, cmap='gray')\n",
    "#     plt.title(f\"Segmentation: {base_name}\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    print(f\"Saved segmentation for {base_name} at {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb148a",
   "metadata": {},
   "source": [
    "**Morphological Opening and Closing + Canny edge Detector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3a94443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved edge image for: seg_608_HC.png\n",
      "Processed and saved edge image for: seg_646_HC.png\n",
      "Processed and saved edge image for: seg_517_HC.png\n",
      "Processed and saved edge image for: seg_631_HC.png\n",
      "Processed and saved edge image for: seg_669_HC.png\n",
      "Processed and saved edge image for: seg_676_HC.png\n",
      "Processed and saved edge image for: seg_665_HC.png\n",
      "Processed and saved edge image for: seg_648_2HC.png\n",
      "Processed and saved edge image for: seg_639_HC.png\n",
      "Processed and saved edge image for: seg_592_HC.png\n",
      "Processed and saved edge image for: seg_570_3HC.png\n",
      "Processed and saved edge image for: seg_528_HC.png\n",
      "Processed and saved edge image for: seg_560_HC.png\n",
      "Processed and saved edge image for: seg_593_HC.png\n",
      "Processed and saved edge image for: seg_520_HC.png\n",
      "Processed and saved edge image for: seg_584_HC.png\n",
      "Processed and saved edge image for: seg_610_HC.png\n",
      "Processed and saved edge image for: seg_567_2HC.png\n",
      "Processed and saved edge image for: seg_707_HC.png\n",
      "Processed and saved edge image for: seg_618_2HC.png\n",
      "Processed and saved edge image for: seg_550_HC.png\n",
      "Processed and saved edge image for: seg_607_HC.png\n",
      "Processed and saved edge image for: seg_570_HC.png\n",
      "Processed and saved edge image for: seg_621_HC.png\n",
      "Processed and saved edge image for: seg_619_HC.png\n",
      "Processed and saved edge image for: seg_543_HC.png\n",
      "Processed and saved edge image for: seg_535_HC.png\n",
      "Processed and saved edge image for: seg_501_HC.png\n",
      "Processed and saved edge image for: seg_546_HC.png\n",
      "Processed and saved edge image for: seg_704_2HC.png\n",
      "Processed and saved edge image for: seg_583_2HC.png\n",
      "Processed and saved edge image for: seg_529_HC.png\n",
      "Processed and saved edge image for: seg_613_3HC.png\n",
      "Processed and saved edge image for: seg_611_HC.png\n",
      "Processed and saved edge image for: seg_507_2HC.png\n",
      "Processed and saved edge image for: seg_632_HC.png\n",
      "Processed and saved edge image for: seg_605_HC.png\n",
      "Processed and saved edge image for: seg_628_2HC.png\n",
      "Processed and saved edge image for: seg_557_HC.png\n",
      "Processed and saved edge image for: seg_523_HC.png\n",
      "Processed and saved edge image for: seg_692_HC.png\n",
      "Processed and saved edge image for: seg_647_HC.png\n",
      "Processed and saved edge image for: seg_699_HC.png\n",
      "Processed and saved edge image for: seg_524_HC.png\n",
      "Processed and saved edge image for: seg_532_2HC.png\n",
      "Processed and saved edge image for: seg_627_HC.png\n",
      "Processed and saved edge image for: seg_601_HC.png\n",
      "Processed and saved edge image for: seg_675_2HC.png\n",
      "Processed and saved edge image for: seg_672_HC.png\n",
      "Processed and saved edge image for: seg_652_HC.png\n",
      "Processed and saved edge image for: seg_570_2HC.png\n",
      "Processed and saved edge image for: seg_589_HC.png\n",
      "Processed and saved edge image for: seg_583_HC.png\n",
      "Processed and saved edge image for: seg_515_HC.png\n",
      "Processed and saved edge image for: seg_641_HC.png\n",
      "Processed and saved edge image for: seg_574_2HC.png\n",
      "Processed and saved edge image for: seg_671_HC.png\n",
      "Processed and saved edge image for: seg_582_HC.png\n",
      "Processed and saved edge image for: seg_539_HC.png\n",
      "Processed and saved edge image for: seg_609_HC.png\n",
      "Processed and saved edge image for: seg_564_HC.png\n",
      "Processed and saved edge image for: seg_588_2HC.png\n",
      "Processed and saved edge image for: seg_667_HC.png\n",
      "Processed and saved edge image for: seg_697_HC.png\n",
      "Processed and saved edge image for: seg_670_HC.png\n",
      "Processed and saved edge image for: seg_633_HC.png\n",
      "Processed and saved edge image for: seg_636_HC.png\n",
      "Processed and saved edge image for: seg_506_HC.png\n",
      "Processed and saved edge image for: seg_695_HC.png\n",
      "Processed and saved edge image for: seg_513_HC.png\n",
      "Processed and saved edge image for: seg_663_HC.png\n",
      "Processed and saved edge image for: seg_657_2HC.png\n",
      "Processed and saved edge image for: seg_614_HC.png\n",
      "Processed and saved edge image for: seg_689_HC.png\n",
      "Processed and saved edge image for: seg_622_HC.png\n",
      "Processed and saved edge image for: seg_590_HC.png\n",
      "Processed and saved edge image for: seg_652_2HC.png\n",
      "Processed and saved edge image for: seg_626_HC.png\n",
      "Processed and saved edge image for: seg_700_HC.png\n",
      "Processed and saved edge image for: seg_683_HC.png\n",
      "Processed and saved edge image for: seg_588_HC.png\n",
      "Processed and saved edge image for: seg_685_HC.png\n",
      "Processed and saved edge image for: seg_521_HC.png\n",
      "Processed and saved edge image for: seg_516_HC.png\n",
      "Processed and saved edge image for: seg_591_HC.png\n",
      "Processed and saved edge image for: seg_544_2HC.png\n",
      "Processed and saved edge image for: seg_531_HC.png\n",
      "Processed and saved edge image for: seg_502_HC.png\n",
      "Processed and saved edge image for: seg_612_HC.png\n",
      "Processed and saved edge image for: seg_575_HC.png\n",
      "Processed and saved edge image for: seg_500_HC.png\n",
      "Processed and saved edge image for: seg_703_HC.png\n",
      "Processed and saved edge image for: seg_511_HC.png\n",
      "Processed and saved edge image for: seg_545_2HC.png\n",
      "Processed and saved edge image for: seg_634_HC.png\n",
      "Processed and saved edge image for: seg_541_2HC.png\n",
      "Processed and saved edge image for: seg_514_HC.png\n",
      "Processed and saved edge image for: seg_505_HC.png\n",
      "Processed and saved edge image for: seg_657_HC.png\n",
      "Processed and saved edge image for: seg_553_HC.png\n",
      "Processed and saved edge image for: seg_613_HC.png\n",
      "Processed and saved edge image for: seg_536_HC.png\n",
      "Processed and saved edge image for: seg_702_HC.png\n",
      "Processed and saved edge image for: seg_540_HC.png\n",
      "Processed and saved edge image for: seg_538_HC.png\n",
      "Processed and saved edge image for: seg_508_HC.png\n",
      "Processed and saved edge image for: seg_507_HC.png\n",
      "Processed and saved edge image for: seg_573_HC.png\n",
      "Processed and saved edge image for: seg_518_HC.png\n",
      "Processed and saved edge image for: seg_547_HC.png\n",
      "Processed and saved edge image for: seg_684_HC.png\n",
      "Processed and saved edge image for: seg_525_HC.png\n",
      "Processed and saved edge image for: seg_522_HC.png\n",
      "Processed and saved edge image for: seg_625_HC.png\n",
      "Processed and saved edge image for: seg_561_2HC.png\n",
      "Processed and saved edge image for: seg_551_HC.png\n",
      "Processed and saved edge image for: seg_547_2HC.png\n",
      "Processed and saved edge image for: seg_677_HC.png\n",
      "Processed and saved edge image for: seg_569_HC.png\n",
      "Processed and saved edge image for: seg_701_HC.png\n",
      "Processed and saved edge image for: seg_534_HC.png\n",
      "Processed and saved edge image for: seg_671_3HC.png\n",
      "Processed and saved edge image for: seg_561_HC.png\n",
      "Processed and saved edge image for: seg_552_HC.png\n",
      "Processed and saved edge image for: seg_563_HC.png\n",
      "Processed and saved edge image for: seg_668_HC.png\n",
      "Processed and saved edge image for: seg_690_HC.png\n",
      "Processed and saved edge image for: seg_659_HC.png\n",
      "Processed and saved edge image for: seg_642_HC.png\n",
      "Processed and saved edge image for: seg_592_2HC.png\n",
      "Processed and saved edge image for: seg_597_HC.png\n",
      "Processed and saved edge image for: seg_504_HC.png\n",
      "Processed and saved edge image for: seg_696_HC.png\n",
      "Processed and saved edge image for: seg_690_2HC.png\n",
      "Processed and saved edge image for: seg_519_HC.png\n",
      "Processed and saved edge image for: seg_526_HC.png\n",
      "Processed and saved edge image for: seg_576_HC.png\n",
      "Processed and saved edge image for: seg_666_HC.png\n",
      "Processed and saved edge image for: seg_509_HC.png\n",
      "Processed and saved edge image for: seg_628_HC.png\n",
      "Processed and saved edge image for: seg_558_HC.png\n",
      "Processed and saved edge image for: seg_698_HC.png\n",
      "Processed and saved edge image for: seg_600_HC.png\n",
      "Processed and saved edge image for: seg_602_HC.png\n",
      "Processed and saved edge image for: seg_649_HC.png\n",
      "Processed and saved edge image for: seg_568_HC.png\n",
      "Processed and saved edge image for: seg_662_HC.png\n",
      "Processed and saved edge image for: seg_656_HC.png\n",
      "Processed and saved edge image for: seg_512_HC.png\n",
      "Processed and saved edge image for: seg_655_HC.png\n",
      "Processed and saved edge image for: seg_541_HC.png\n",
      "Processed and saved edge image for: seg_542_HC.png\n",
      "Processed and saved edge image for: seg_679_HC.png\n",
      "Processed and saved edge image for: seg_620_HC.png\n",
      "Processed and saved edge image for: seg_673_HC.png\n",
      "Processed and saved edge image for: seg_549_HC.png\n",
      "Processed and saved edge image for: seg_545_HC.png\n",
      "Processed and saved edge image for: seg_567_HC.png\n",
      "Processed and saved edge image for: seg_660_HC.png\n",
      "Processed and saved edge image for: seg_648_HC.png\n",
      "Processed and saved edge image for: seg_675_HC.png\n",
      "Processed and saved edge image for: seg_618_HC.png\n",
      "Processed and saved edge image for: seg_677_2HC.png\n",
      "Processed and saved edge image for: seg_671_2HC.png\n",
      "Processed and saved edge image for: seg_548_HC.png\n",
      "Processed and saved edge image for: seg_686_HC.png\n",
      "Processed and saved edge image for: seg_595_HC.png\n",
      "Processed and saved edge image for: seg_623_HC.png\n",
      "Processed and saved edge image for: seg_693_HC.png\n",
      "Processed and saved edge image for: seg_606_HC.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved edge image for: seg_596_HC.png\n",
      "Processed and saved edge image for: seg_682_HC.png\n",
      "Processed and saved edge image for: seg_638_HC.png\n",
      "Processed and saved edge image for: seg_617_HC.png\n",
      "Processed and saved edge image for: seg_664_HC.png\n",
      "Processed and saved edge image for: seg_615_HC.png\n",
      "Processed and saved edge image for: seg_527_HC.png\n",
      "Processed and saved edge image for: seg_688_HC.png\n",
      "Processed and saved edge image for: seg_617_2HC.png\n",
      "Processed and saved edge image for: seg_705_HC.png\n",
      "Processed and saved edge image for: seg_565_HC.png\n",
      "Processed and saved edge image for: seg_706_HC.png\n",
      "Processed and saved edge image for: seg_640_HC.png\n",
      "Processed and saved edge image for: seg_637_HC.png\n",
      "Processed and saved edge image for: seg_630_2HC.png\n",
      "Processed and saved edge image for: seg_674_HC.png\n",
      "Processed and saved edge image for: seg_643_HC.png\n",
      "Processed and saved edge image for: seg_554_HC.png\n",
      "Processed and saved edge image for: seg_609_2HC.png\n",
      "Processed and saved edge image for: seg_650_HC.png\n",
      "Processed and saved edge image for: seg_691_HC.png\n",
      "Processed and saved edge image for: seg_585_HC.png\n",
      "Processed and saved edge image for: seg_651_HC.png\n",
      "Processed and saved edge image for: seg_653_HC.png\n",
      "Processed and saved edge image for: seg_556_HC.png\n",
      "Processed and saved edge image for: seg_599_HC.png\n",
      "Processed and saved edge image for: seg_624_HC.png\n",
      "Processed and saved edge image for: seg_681_HC.png\n",
      "Processed and saved edge image for: seg_678_HC.png\n",
      "Processed and saved edge image for: seg_594_HC.png\n",
      "Processed and saved edge image for: seg_686_2HC.png\n",
      "Processed and saved edge image for: seg_645_HC.png\n",
      "Processed and saved edge image for: seg_687_HC.png\n",
      "Processed and saved edge image for: seg_692_2HC.png\n",
      "Processed and saved edge image for: seg_578_HC.png\n",
      "Processed and saved edge image for: seg_580_HC.png\n",
      "Processed and saved edge image for: seg_708_2HC.png\n",
      "Processed and saved edge image for: seg_532_HC.png\n",
      "Processed and saved edge image for: seg_680_HC.png\n",
      "Processed and saved edge image for: seg_559_HC.png\n",
      "Processed and saved edge image for: seg_562_HC.png\n",
      "Processed and saved edge image for: seg_587_2HC.png\n",
      "Processed and saved edge image for: seg_639_2HC.png\n",
      "Processed and saved edge image for: seg_537_HC.png\n",
      "Processed and saved edge image for: seg_613_2HC.png\n",
      "Processed and saved edge image for: seg_704_HC.png\n",
      "Processed and saved edge image for: seg_635_HC.png\n",
      "Processed and saved edge image for: seg_630_HC.png\n",
      "Processed and saved edge image for: seg_533_HC.png\n",
      "Processed and saved edge image for: seg_694_HC.png\n",
      "Processed and saved edge image for: seg_577_HC.png\n",
      "Processed and saved edge image for: seg_604_HC.png\n",
      "Processed and saved edge image for: seg_566_2HC.png\n",
      "Processed and saved edge image for: seg_581_HC.png\n",
      "Processed and saved edge image for: seg_644_HC.png\n",
      "Processed and saved edge image for: seg_586_HC.png\n",
      "Processed and saved edge image for: seg_510_HC.png\n",
      "Processed and saved edge image for: seg_654_HC.png\n",
      "Processed and saved edge image for: seg_673_2HC.png\n",
      "Processed and saved edge image for: seg_531_2HC.png\n",
      "Processed and saved edge image for: seg_571_HC.png\n",
      "Processed and saved edge image for: seg_598_HC.png\n",
      "Processed and saved edge image for: seg_629_HC.png\n",
      "Processed and saved edge image for: seg_603_HC.png\n",
      "Processed and saved edge image for: seg_579_HC.png\n",
      "Processed and saved edge image for: seg_555_HC.png\n",
      "Processed and saved edge image for: seg_561_3HC.png\n",
      "Processed and saved edge image for: seg_616_HC.png\n",
      "Processed and saved edge image for: seg_587_HC.png\n",
      "Processed and saved edge image for: seg_658_HC.png\n",
      "Processed and saved edge image for: seg_503_HC.png\n",
      "Processed and saved edge image for: seg_566_HC.png\n",
      "Processed and saved edge image for: seg_526_2HC.png\n",
      "Processed and saved edge image for: seg_544_HC.png\n",
      "Processed and saved edge image for: seg_574_HC.png\n",
      "Processed and saved edge image for: seg_661_HC.png\n",
      "Processed and saved edge image for: seg_556_2HC.png\n",
      "Processed and saved edge image for: seg_631_2HC.png\n",
      "Processed and saved edge image for: seg_572_HC.png\n",
      "Processed and saved edge image for: seg_530_HC.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = 'output_segmentations'\n",
    "output_folder = 'output_edges'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define a structuring element for the morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Loop over all segmented images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        # Read the segmentation image in grayscale\n",
    "        seg_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Apply morphological opening (erosion followed by dilation) to remove small artifacts\n",
    "        opened = cv2.morphologyEx(seg_img, cv2.MORPH_OPEN, kernel)\n",
    "        # Then apply morphological closing (dilation followed by erosion) to fill small holes\n",
    "        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Apply Canny edge detector to extract the contour\n",
    "        # Adjust thresholds as necessary (here, 50 and 150 are example values)\n",
    "        edges = cv2.Canny(closed, 50, 150)\n",
    "        \n",
    "        # Save the edge-detected image\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, edges)\n",
    "        \n",
    "        # Optionally, display the original segmentation, post-morphology, and edge image side by side\n",
    "#         plt.figure(figsize=(12, 4))\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(seg_img, cmap='gray')\n",
    "#         plt.title('Original Segmentation')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(closed, cmap='gray')\n",
    "#         plt.title('After Morphological Ops')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(edges, cmap='gray')\n",
    "#         plt.title('Canny Edges')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.show()\n",
    "        print(f\"Processed and saved edge image for: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a792bc6",
   "metadata": {},
   "source": [
    "**Ellipse fiting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa48dc8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'ellipse_results.csv' saved with 250 rows.\n"
     ]
    }
   ],
   "source": [
    "    import csv\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import math\n",
    "\n",
    "    # --- Step 1: Load the pixel size information from the CSV file ---\n",
    "    # Assume your CSV file 'test_set_pixel_size.csv' has at least these columns:\n",
    "    # filename,pixel_size_mm\n",
    "    pixel_size_dict = {}\n",
    "    with open('test_set_pixel_size.csv', mode='r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # Adjust the column names if they are different in your CSV file.\n",
    "            filename_csv = row['filename']\n",
    "            pixel_size_mm = float(row['pixel size(mm)'])\n",
    "            pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "    # --- Step 2: Process the edge images and fit ellipses ---\n",
    "    edges_folder = 'output_edges'\n",
    "    csv_output = 'ellipse_results.csv'\n",
    "    header = [\"filename\", \"center_x_mm\", \"center_y_mm\", \"semi_axes_a_mm\", \"semi_axes_b_mm\", \"angle_rad\"]\n",
    "    rows = []\n",
    "\n",
    "    for filename in sorted(os.listdir(edges_folder)):\n",
    "        if filename.endswith('.png'):\n",
    "            filepath = os.path.join(edges_folder, filename)\n",
    "            # Read the edge image in grayscale\n",
    "            edge_img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Find contours in the edge image\n",
    "            contours, _ = cv2.findContours(edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if len(contours) == 0:\n",
    "                print(f\"No contours found in {filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Choose the largest contour (assumed to be the head contour)\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "            # cv2.fitEllipse requires at least 5 points\n",
    "            if len(largest_contour) < 5:\n",
    "                print(f\"Not enough points for ellipse fitting in {filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Fit ellipse to the largest contour\n",
    "            ellipse = cv2.fitEllipse(largest_contour)\n",
    "            # ellipse returns ((center_x, center_y), (full_axis_length_a, full_axis_length_b), angle_in_degrees)\n",
    "            center, axes, angle = ellipse\n",
    "            # Compute semi-axes (the axes given are the full lengths)\n",
    "            semi_a = axes[0] / 2.0  # semi-major axis in pixels\n",
    "            semi_b = axes[1] / 2.0  # semi-minor axis in pixels\n",
    "\n",
    "            # --- Step 3: Look up the pixel conversion factor for this image ---\n",
    "            # The filenames in the CSV are expected to be like \"001_HC.png\"\n",
    "            # and our edge images are named \"seg_001_HC.png\". Remove the \"seg_\" prefix.\n",
    "            base_filename = filename.replace(\"seg_\", \"\", 1)\n",
    "            if base_filename in pixel_size_dict:\n",
    "                pixel_to_mm = pixel_size_dict[base_filename]\n",
    "            else:\n",
    "                print(f\"Pixel size for {base_filename} not found in CSV. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Convert measurements from pixels to millimeters\n",
    "            center_x_mm = center[0] * pixel_to_mm\n",
    "            center_y_mm = center[1] * pixel_to_mm\n",
    "            semi_a_mm = semi_a * pixel_to_mm\n",
    "            semi_b_mm = semi_b * pixel_to_mm\n",
    "\n",
    "            # Convert angle from degrees to radians\n",
    "            angle_rad = math.radians(angle)\n",
    "\n",
    "            # Append the result: filename, center_x_mm, center_y_mm, semi_axes_a_mm, semi_axes_b_mm, angle_rad\n",
    "            rows.append([base_filename, center_x_mm, center_y_mm, semi_a_mm, semi_b_mm, angle_rad])\n",
    "\n",
    "    # --- Step 4: Write results to a CSV file ---\n",
    "    with open(csv_output, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"CSV file '{csv_output}' saved with {len(rows)} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
