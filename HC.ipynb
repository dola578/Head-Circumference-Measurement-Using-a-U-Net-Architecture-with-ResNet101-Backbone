{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1801f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b722b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70eb7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.5\n",
      "Uninstalling numpy-2.2.5:\n",
      "  Successfully uninstalled numpy-2.2.5\n",
      "Found existing installation: opencv-python 4.11.0.86\n",
      "Uninstalling opencv-python-4.11.0.86:\n",
      "  Successfully uninstalled opencv-python-4.11.0.86\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.23.5\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-image<0.25\n",
      "  Downloading scikit_image-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.9 in /home/dola-s/.local/lib/python3.10/site-packages (from scikit-image<0.25) (1.15.2)\n",
      "Requirement already satisfied: pillow>=9.1 in /home/dola-s/.local/lib/python3.10/site-packages (from scikit-image<0.25) (11.2.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/dola-s/.local/lib/python3.10/site-packages (from scikit-image<0.25) (0.4)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/dola-s/.local/lib/python3.10/site-packages (from scikit-image<0.25) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/dola-s/.local/lib/python3.10/site-packages (from scikit-image<0.25) (2025.3.30)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/dola-s/.local/lib/python3.10/site-packages (from scikit-image<0.25) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/dola-s/.local/lib/python3.10/site-packages (from scikit-image<0.25) (1.23.5)\n",
      "Requirement already satisfied: packaging>=21 in /usr/lib/python3/dist-packages (from scikit-image<0.25) (21.3)\n",
      "Installing collected packages: scikit-image\n",
      "Successfully installed scikit-image-0.24.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy opencv-python -y\n",
    "!pip install --upgrade \"numpy==1.23.5\"\n",
    "!pip install --upgrade \"scikit-image<0.25\"\n",
    "!pip install --no-deps opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801e54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall typing_extensions\n",
    "# !pip install typing_extensions==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c8f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions>=4.3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1b91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30e90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing_extensions==4.12.2\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "Successfully installed typing_extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install typing_extensions==4.12.2 --upgrade\n",
    "# pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8758f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypeIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee1b1c",
   "metadata": {},
   "source": [
    "**Preprocessing - flipping, resizing, rotation, intensity based transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09bb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# Custom joint transformation that applies geometric transforms to both image and mask,\n",
    "# and intensity transforms only to the image.\n",
    "class JointTransform:\n",
    "    def __init__(self, resize=(256, 256), rotation=20, hflip_prob=0.5, vflip_prob=0.5,\n",
    "                 intensity_transforms=None):\n",
    "        self.resize = resize\n",
    "        self.rotation = rotation\n",
    "        self.hflip_prob = hflip_prob\n",
    "        self.vflip_prob = vflip_prob\n",
    "        # Intensity transforms should be a torchvision transform applied only to the image.\n",
    "        # For example, transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        #                                  transforms.Lambda(lambda x: x.pow(0.5))])\n",
    "        self.intensity_transforms = intensity_transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # 1. Resize both image and mask\n",
    "        image = TF.resize(image, self.resize)\n",
    "        mask = TF.resize(mask, self.resize)\n",
    "        \n",
    "        # 2. Random horizontal flip\n",
    "        if np.random.rand() < self.hflip_prob:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "            \n",
    "        # 3. Random vertical flip\n",
    "        if np.random.rand() < self.vflip_prob:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "            \n",
    "        # 4. Random rotation\n",
    "        angle = np.random.uniform(-self.rotation, self.rotation)\n",
    "        image = TF.rotate(image, angle, interpolation=Image.BILINEAR)\n",
    "        # For masks, use nearest neighbor interpolation to preserve label boundaries.\n",
    "        mask = TF.rotate(mask, angle, interpolation=Image.NEAREST)\n",
    "        \n",
    "        # 5. Apply intensity transforms to the image only (if provided)\n",
    "        if self.intensity_transforms:\n",
    "            image = self.intensity_transforms(image)\n",
    "        \n",
    "        # 6. Convert both image and mask to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        # Optionally ensure the mask is binary\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define intensity-only transformations for the image.\n",
    "intensity_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "#     transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5))  # Gamma correction with gamma=0.5\n",
    "])\n",
    "\n",
    "# Create the joint transformation instance\n",
    "joint_transform = JointTransform(resize=(256, 256), rotation=20,\n",
    "                                 hflip_prob=0.5, vflip_prob=0.5,\n",
    "                                 intensity_transforms=intensity_transforms)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, joint_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.joint_transform = joint_transform\n",
    "        # Sort the file lists to ensure alignment between images and masks\n",
    "        self.images = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')])\n",
    "        self.masks = sorted([os.path.join(mask_dir, x) for x in os.listdir(mask_dir) if 'Annotation' in x])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.joint_transform:\n",
    "            image, mask = self.joint_transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "# Initialize dataset with the joint transform\n",
    "full_dataset = CustomDataset('denoised_training_set', 'masked_annotations', joint_transform=joint_transform)\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# validation_size = len(full_dataset) - train_size\n",
    "# train_dataset, validation_dataset = random_split(full_dataset, [train_size, validation_size])\n",
    "\n",
    "# # Create separate dataloaders for train and validation datasets\n",
    "# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "# validation_loader = DataLoader(validation_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cc602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     img, mask = full_dataset[i]\n",
    "#     print(f\"Sample {i}:\")\n",
    "#     print(\"  Image type:\", type(img), \"shape:\", img.shape, \"dtype:\", img.dtype)\n",
    "#     print(\"  Mask type:\", type(mask), \"shape:\", mask.shape, \"dtype:\", mask.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f45942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data samples: 749\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data samples:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d114c",
   "metadata": {},
   "source": [
    "**Checking alignment and order of train image and the corresponding masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12258a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path, mask_path in zip(sorted(os.listdir('denoised_training_set')), \n",
    "                               sorted(os.listdir('masked_annotations'))):\n",
    "    if img_path.endswith('.png') and 'Annotation' in mask_path:\n",
    "        base_img = os.path.splitext(img_path)[0]\n",
    "        base_mask = os.path.splitext(mask_path)[0].replace('_Annotation', '')\n",
    "        assert base_img == base_mask, f\"Mismatch: {base_img} vs {base_mask}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61e302",
   "metadata": {},
   "source": [
    "**Unet with resnet101 as backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a664895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ResConv(nn.Module):\n",
    "    \"\"\" Convolution block for U-Net with repeated convolutions and ReLU activations. \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ResConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\" Upsampling block for U-Net, using bilinear interpolation and convolution. \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = ResConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        from_up = self.up(from_up)\n",
    "        diffY = from_down.size()[2] - from_up.size()[2]\n",
    "        diffX = from_down.size()[3] - from_up.size()[3]\n",
    "        from_up = F.pad(from_up, [diffX // 2, diffX - diffX // 2,\n",
    "                                  diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([from_down, from_up], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetResNet101(nn.Module):\n",
    "    def __init__(self, n_classes=1):\n",
    "        super(UNetResNet101, self).__init__()\n",
    "        base_model = models.resnet101(pretrained=True)\n",
    "        self.base_layers = list(base_model.children())\n",
    "        \n",
    "        # Extract layers from ResNet101\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3])  # conv1, bn1, relu\n",
    "        self.maxpool = self.base_layers[3]\n",
    "        self.layer1 = self.base_layers[4]  # Output: 256 channels\n",
    "        self.layer2 = self.base_layers[5]  # Output: 512 channels\n",
    "        self.layer3 = self.base_layers[6]  # Output: 1024 channels\n",
    "        self.layer4 = self.base_layers[7]  # Output: 2048 channels\n",
    "\n",
    "        # Decoder (make sure the channel numbers match the skip connection outputs)\n",
    "        self.up4 = UpConv(2048 + 1024, 1024)  # Concatenating x3 (1024) and x4 (2048) -> 3072 channels\n",
    "        self.up3 = UpConv(1024 + 512, 512)    # Concatenating x2 (512) and previous output (1024) -> 1536 channels\n",
    "        self.up2 = UpConv(512 + 256, 256)     # Concatenating x1 (256) and previous output (512) -> 768 channels\n",
    "        self.up1 = UpConv(256 + 64, 64)       # Concatenating x0 (64) and previous output (256) -> 320 channels\n",
    "\n",
    "        # Final upsampling and output convolution to match input size\n",
    "        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder: get intermediate features for skip connections\n",
    "        x0 = self.layer0(x)       # Early features, e.g., 64 channels\n",
    "        x1 = self.maxpool(x0)\n",
    "        x1 = self.layer1(x1)      # 256 channels\n",
    "        x2 = self.layer2(x1)      # 512 channels\n",
    "        x3 = self.layer3(x2)      # 1024 channels\n",
    "        x4 = self.layer4(x3)      # 2048 channels\n",
    "\n",
    "        # Decoder: use skip connections from intermediate features\n",
    "        x = self.up4(x3, x4)      # Upsample: x3 (from_down) + x4 (from_up)\n",
    "        x = self.up3(x2, x)       # Upsample: x2 + output of previous block\n",
    "        x = self.up2(x1, x)       # Upsample: x1 + output of previous block\n",
    "        x = self.up1(x0, x)       # Upsample: x0 + output of previous block\n",
    "\n",
    "        x = self.final_up(x)      # Final upsampling to the original size\n",
    "        x = self.final_conv(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22e554",
   "metadata": {},
   "source": [
    "**Dice loss + Binary cross-entropy loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f608ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # Flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb63fea",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6214ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d316b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/50, Train Loss: 0.5483, Train F1: 0.9509, Val Loss: 0.4571, Val F1: 0.9484\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.3948, Train F1: 0.9690, Val Loss: 0.3732, Val F1: 0.9687\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.3462, Train F1: 0.9684, Val Loss: 0.3382, Val F1: 0.9683\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.3041, Train F1: 0.9735, Val Loss: 0.2988, Val F1: 0.9716\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.2781, Train F1: 0.9746, Val Loss: 0.2635, Val F1: 0.9730\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.2437, Train F1: 0.9761, Val Loss: 0.2340, Val F1: 0.9740\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.2246, Train F1: 0.9765, Val Loss: 0.2214, Val F1: 0.9746\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.2054, Train F1: 0.9770, Val Loss: 0.1986, Val F1: 0.9748\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.1848, Train F1: 0.9756, Val Loss: 0.1916, Val F1: 0.9714\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.1707, Train F1: 0.9767, Val Loss: 0.1725, Val F1: 0.9740\n",
      "Saved overlay image at: overlay_images/fold1_epoch10.png\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.1579, Train F1: 0.9760, Val Loss: 0.1664, Val F1: 0.9738\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.1458, Train F1: 0.9787, Val Loss: 0.1576, Val F1: 0.9746\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.1378, Train F1: 0.9773, Val Loss: 0.1520, Val F1: 0.9728\n",
      "Early stopping triggered in fold 1 at epoch 13\n",
      "Starting fold 2/5\n",
      "Fold 2, Epoch 1/50, Train Loss: 0.5237, Train F1: 0.9587, Val Loss: 0.4323, Val F1: 0.9568\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.3583, Train F1: 0.9683, Val Loss: 0.3497, Val F1: 0.9658\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.3124, Train F1: 0.9710, Val Loss: 0.3250, Val F1: 0.9687\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.2706, Train F1: 0.9725, Val Loss: 0.2705, Val F1: 0.9706\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.2400, Train F1: 0.9728, Val Loss: 0.2401, Val F1: 0.9698\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.2151, Train F1: 0.9745, Val Loss: 0.2103, Val F1: 0.9728\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.1939, Train F1: 0.9748, Val Loss: 0.1942, Val F1: 0.9733\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.1713, Train F1: 0.9727, Val Loss: 0.1898, Val F1: 0.9696\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.1575, Train F1: 0.9776, Val Loss: 0.1627, Val F1: 0.9737\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.1454, Train F1: 0.9768, Val Loss: 0.1496, Val F1: 0.9727\n",
      "Saved overlay image at: overlay_images/fold2_epoch10.png\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.1362, Train F1: 0.9771, Val Loss: 0.1462, Val F1: 0.9732\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.1282, Train F1: 0.9776, Val Loss: 0.1468, Val F1: 0.9726\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.1194, Train F1: 0.9785, Val Loss: 0.1252, Val F1: 0.9748\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.1116, Train F1: 0.9791, Val Loss: 0.1210, Val F1: 0.9740\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.1044, Train F1: 0.9795, Val Loss: 0.1168, Val F1: 0.9759\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.0980, Train F1: 0.9792, Val Loss: 0.1122, Val F1: 0.9753\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.0944, Train F1: 0.9785, Val Loss: 0.1088, Val F1: 0.9737\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.0913, Train F1: 0.9795, Val Loss: 0.0991, Val F1: 0.9757\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.0875, Train F1: 0.9797, Val Loss: 0.0976, Val F1: 0.9752\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.0817, Train F1: 0.9803, Val Loss: 0.0958, Val F1: 0.9754\n",
      "Early stopping triggered in fold 2 at epoch 20\n",
      "Starting fold 3/5\n",
      "Fold 3, Epoch 1/50, Train Loss: 0.6055, Train F1: 0.9530, Val Loss: 0.5313, Val F1: 0.9477\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.4508, Train F1: 0.9695, Val Loss: 0.4435, Val F1: 0.9683\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.3982, Train F1: 0.9673, Val Loss: 0.3929, Val F1: 0.9665\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.3571, Train F1: 0.9686, Val Loss: 0.3657, Val F1: 0.9667\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.3199, Train F1: 0.9728, Val Loss: 0.3195, Val F1: 0.9709\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.2862, Train F1: 0.9731, Val Loss: 0.2780, Val F1: 0.9705\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.2601, Train F1: 0.9721, Val Loss: 0.2693, Val F1: 0.9701\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.2400, Train F1: 0.9631, Val Loss: 0.2559, Val F1: 0.9619\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.2217, Train F1: 0.9655, Val Loss: 0.2532, Val F1: 0.9604\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.2031, Train F1: 0.9756, Val Loss: 0.2086, Val F1: 0.9730\n",
      "Saved overlay image at: overlay_images/fold3_epoch10.png\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.1964, Train F1: 0.9743, Val Loss: 0.2136, Val F1: 0.9714\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.1781, Train F1: 0.9777, Val Loss: 0.1826, Val F1: 0.9752\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.1631, Train F1: 0.9767, Val Loss: 0.1662, Val F1: 0.9749\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.1502, Train F1: 0.9779, Val Loss: 0.1607, Val F1: 0.9759\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.1408, Train F1: 0.9785, Val Loss: 0.1505, Val F1: 0.9758\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.1333, Train F1: 0.9802, Val Loss: 0.1394, Val F1: 0.9766\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.1252, Train F1: 0.9790, Val Loss: 0.1413, Val F1: 0.9738\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.1212, Train F1: 0.9784, Val Loss: 0.1312, Val F1: 0.9750\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.1162, Train F1: 0.9798, Val Loss: 0.1200, Val F1: 0.9765\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.1085, Train F1: 0.9811, Val Loss: 0.1124, Val F1: 0.9766\n",
      "Saved overlay image at: overlay_images/fold3_epoch20.png\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.1048, Train F1: 0.9802, Val Loss: 0.1126, Val F1: 0.9762\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.0989, Train F1: 0.9792, Val Loss: 0.1167, Val F1: 0.9747\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.0945, Train F1: 0.9806, Val Loss: 0.1091, Val F1: 0.9763\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.0926, Train F1: 0.9781, Val Loss: 0.1070, Val F1: 0.9735\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.0874, Train F1: 0.9813, Val Loss: 0.1004, Val F1: 0.9768\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.0835, Train F1: 0.9813, Val Loss: 0.0950, Val F1: 0.9763\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.0863, Train F1: 0.9304, Val Loss: 0.2840, Val F1: 0.9251\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.1361, Train F1: 0.9639, Val Loss: 0.1361, Val F1: 0.9629\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.1153, Train F1: 0.9722, Val Loss: 0.1095, Val F1: 0.9696\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.0946, Train F1: 0.9766, Val Loss: 0.0989, Val F1: 0.9738\n",
      "Early stopping triggered in fold 3 at epoch 30\n",
      "Starting fold 4/5\n",
      "Fold 4, Epoch 1/50, Train Loss: 0.5180, Train F1: 0.9558, Val Loss: 0.3583, Val F1: 0.9626\n",
      "Fold 4, Epoch 2/50, Train Loss: 0.3653, Train F1: 0.9680, Val Loss: 0.3136, Val F1: 0.9708\n",
      "Fold 4, Epoch 3/50, Train Loss: 0.3112, Train F1: 0.9715, Val Loss: 0.2866, Val F1: 0.9718\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Assume full_dataset, device, UNetResNet101, DiceBCELoss, and joint_transform are defined already.\n",
    "# full_dataset = CustomDataset('denoised_training_set', 'masked_annotations', joint_transform=joint_transform)\n",
    "\n",
    "# Number of folds and training epochs\n",
    "num_folds = 5\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "\n",
    "# Set up KFold splitter\n",
    "indices = np.arange(len(full_dataset))\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Folder to save overlay images (optional)\n",
    "overlay_folder = 'overlay_images'\n",
    "os.makedirs(overlay_folder, exist_ok=True)\n",
    "\n",
    "# Helper function for overlay (as before)\n",
    "def overlay_mask_on_image(image, mask, color=(255, 0, 0), alpha=0.4):\n",
    "    color_mask = np.zeros_like(image)\n",
    "    color_mask[mask == 255] = color\n",
    "    overlay = cv2.addWeighted(image, 1 - alpha, color_mask, alpha, 0)\n",
    "    return overlay\n",
    "\n",
    "# Variables to track the best model overall (based on validation F1 score)\n",
    "best_val_f1_overall = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Begin cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
    "    print(f\"Starting fold {fold+1}/{num_folds}\")\n",
    "    \n",
    "    # Create Subset datasets for current fold\n",
    "    train_subset = Subset(full_dataset, train_idx)\n",
    "    val_subset = Subset(full_dataset, val_idx)\n",
    "    \n",
    "    # Create dataloaders for current fold\n",
    "    train_loader = DataLoader(train_subset, batch_size=10, shuffle=True)\n",
    "    validation_loader = DataLoader(val_subset, batch_size=10, shuffle=False)\n",
    "    \n",
    "    # Reinitialize the model, loss, optimizer, and scheduler for each fold\n",
    "    model = UNetResNet101().to(device)\n",
    "    loss_function = DiceBCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    \n",
    "    best_fold_val_f1 = -1.0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss_epoch = running_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate training set for F1 score\n",
    "        model.eval()\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in train_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                binary_masks = (masks > 0.5).float()\n",
    "                \n",
    "                all_train_preds.append(preds.cpu().numpy().flatten())\n",
    "                all_train_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "        all_train_preds = np.concatenate(all_train_preds)\n",
    "        all_train_targets = np.concatenate(all_train_targets)\n",
    "        train_f1 = f1_score(all_train_targets, all_train_preds)\n",
    "        \n",
    "        # Evaluate validation set\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in validation_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_function(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                preds = (outputs > 0.5).float()\n",
    "                binary_masks = (masks > 0.5).float()\n",
    "                all_val_preds.append(preds.cpu().numpy().flatten())\n",
    "                all_val_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "        all_val_preds = np.concatenate(all_val_preds)\n",
    "        all_val_targets = np.concatenate(all_val_targets)\n",
    "        val_f1 = f1_score(all_val_targets, all_val_preds)\n",
    "        val_loss_epoch = val_loss / len(validation_loader)\n",
    "        \n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss_epoch:.4f}, Train F1: {train_f1:.4f}, \"\n",
    "              f\"Val Loss: {val_loss_epoch:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Adjust learning rate based on validation loss\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        \n",
    "        # If current validation F1 is best so far, update best model state across folds/epochs\n",
    "        if val_f1 > best_val_f1_overall:\n",
    "            best_val_f1_overall = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "        # Early stopping for current fold: check if current validation F1 improved over best_fold_val_f1\n",
    "        if val_f1 > best_fold_val_f1:\n",
    "            best_fold_val_f1 = val_f1\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered in fold {fold+1} at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Optionally, every 10 epochs, save an overlay visualization from validation\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                for images, _ in validation_loader:\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    preds = (outputs > 0.5).float()\n",
    "                    img_tensor = images[0].cpu()  # first image from the batch\n",
    "                    pred_tensor = preds[0].cpu()  # corresponding prediction\n",
    "                    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "                    img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "                    mask_np = pred_tensor.squeeze().numpy()\n",
    "                    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "                    overlay_img = overlay_mask_on_image(img_uint8, mask_uint8, color=(255, 0, 0), alpha=0.4)\n",
    "                    # Convert to BGR for cv2.imwrite\n",
    "                    overlay_bgr = cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)\n",
    "                    overlay_save_path = os.path.join(overlay_folder, f'fold{fold+1}_epoch{epoch+1}.png')\n",
    "                    cv2.imwrite(overlay_save_path, overlay_bgr)\n",
    "                    print(f\"Saved overlay image at: {overlay_save_path}\")\n",
    "                    break  # Process only one batch for overlay visualization\n",
    "\n",
    "# After all folds, save the best model\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'best_unet_resnet101_model.pth')\n",
    "    print(f\"Best model saved with validation F1: {best_val_f1_overall:.4f}\")\n",
    "else:\n",
    "    print(\"No model was saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = UNetResNet101(n_classes=1).to(device)\n",
    "# dummy_input = torch.rand(1, 3, 224, 224).to(device)  # Adjust input size as necessary\n",
    "# output = model(dummy_input)\n",
    "# print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214323f5",
   "metadata": {},
   "source": [
    "**Running Model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a test dataset class\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_path\n",
    "\n",
    "# Define deterministic transforms for test data\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),  # Match mask interpolation\n",
    "#     transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Set the directory where your test images are stored\n",
    "test_dir = 'denoised_test_set' \n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = TestDataset(test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Instantiate your model (assumed defined in the notebook)\n",
    "model = UNetResNet101(n_classes=1).to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('best_unet_resnet101_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Create an output directory for the segmentation results\n",
    "output_dir = 'output_segmentations'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Inference loop: run the model on each test image and save the segmentation mask\n",
    "for image, image_path in test_loader:\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Get the probability map from the model (sigmoid already applied)\n",
    "        # Threshold the probability map to obtain a binary segmentation mask\n",
    "        seg_mask = (output > 0.5).float()\n",
    "    \n",
    "    # Convert tensor to NumPy array and scale to 0-255 for visualization/saving\n",
    "    seg_mask_np = seg_mask.cpu().numpy().squeeze() * 255\n",
    "\n",
    "    # Generate output filename based on input image name\n",
    "    base_name = os.path.basename(image_path[0])\n",
    "    output_filename = os.path.join(output_dir, f\"seg_{base_name}\")\n",
    "    \n",
    "    # Save the segmentation mask image using OpenCV\n",
    "    cv2.imwrite(output_filename, seg_mask_np.astype('uint8'))\n",
    "    \n",
    "    # Optionally, display the segmentation mask using matplotlib\n",
    "#     plt.imshow(seg_mask_np, cmap='gray')\n",
    "#     plt.title(f\"Segmentation: {base_name}\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    print(f\"Saved segmentation for {base_name} at {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b490cd5",
   "metadata": {},
   "source": [
    "**DSC (Dice Similarity Coefficient) with filled annotations as ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4328eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Dice Similarity Coefficient (DSC): 0.9766\n",
      "Mean Hausdorff Distance (MHD): 4.8704 mm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "def dice_coefficient(pred, true, smooth=1e-8):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient between two binary masks.\n",
    "    \n",
    "    Args:\n",
    "        pred (np.array): Predicted binary mask (0 or 1).\n",
    "        true (np.array): Ground truth binary mask (0 or 1).\n",
    "        smooth (float): A small value to avoid division by zero.\n",
    "        \n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred = pred.flatten().astype(np.float32)\n",
    "    true = true.flatten().astype(np.float32)\n",
    "    intersection = np.sum(pred * true)\n",
    "    dice = (2.0 * intersection + smooth) / (np.sum(pred) + np.sum(true) + smooth)\n",
    "    return dice\n",
    "\n",
    "def hausdorff_distance(pred, true):\n",
    "    \"\"\"\n",
    "    Computes the Hausdorff distance between the boundaries of two binary masks.\n",
    "    Extracts the boundaries using Canny edge detection, then computes the symmetric directed Hausdorff distance.\n",
    "    \n",
    "    Args:\n",
    "        pred (np.array): Predicted binary mask (with values 0 or 255, uint8).\n",
    "        true (np.array): Ground truth binary mask (with values 0 or 255, uint8).\n",
    "    \n",
    "    Returns:\n",
    "        float: Hausdorff distance.\n",
    "    \"\"\"\n",
    "    pred_edges = cv2.Canny(pred, 30, 100)\n",
    "    true_edges = cv2.Canny(true, 30, 100)\n",
    "    \n",
    "    pred_points = np.column_stack(np.where(pred_edges != 0))\n",
    "    true_points = np.column_stack(np.where(true_edges != 0))\n",
    "    \n",
    "    if pred_points.size == 0 or true_points.size == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    d1 = directed_hausdorff(pred_points, true_points)[0]\n",
    "    d2 = directed_hausdorff(true_points, pred_points)[0]\n",
    "    return max(d1, d2)\n",
    "\n",
    "# Directories for predicted segmentation masks and ground-truth masked annotations\n",
    "pred_dir = 'output_segmentations'\n",
    "gt_dir = 'masked_annotations_test'  # File name style: \"500_HC_Annotation.png\"\n",
    "\n",
    "# Get a sorted list of predicted files (assuming .png format)\n",
    "pred_files = sorted([f for f in os.listdir(pred_dir) if f.endswith('.png')])\n",
    "\n",
    "dice_scores = []\n",
    "hd_scores = []\n",
    "\n",
    "for pred_file in pred_files:\n",
    "    pred_path = os.path.join(pred_dir, pred_file)\n",
    "    \n",
    "    # Extract the base name by removing the \"seg_\" prefix and \".png\" extension\n",
    "    base_name = pred_file.replace(\"seg_\", \"\").replace(\".png\", \"\")  # e.g., \"500_HC\"\n",
    "    # Form ground truth filename by appending \"_Annotation.png\"\n",
    "    gt_file = base_name + \"_Annotation.png\"  # e.g., \"500_HC_Annotation.png\"\n",
    "    gt_path = os.path.join(gt_dir, gt_file)\n",
    "    \n",
    "    if not os.path.exists(gt_path):\n",
    "        print(f\"Ground truth file {gt_file} not found for {pred_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Load predicted mask and ground truth mask in grayscale\n",
    "    pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "    gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if pred_mask is None or gt_mask is None:\n",
    "        print(f\"Error loading {pred_file} or {gt_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Resize ground truth mask to match predicted mask size using nearest neighbor interpolation\n",
    "    gt_mask_resized = cv2.resize(gt_mask, (pred_mask.shape[1], pred_mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Convert masks to binary: threshold at 127 (if values are 0 and 255)\n",
    "    _, pred_mask_bin = cv2.threshold(pred_mask, 127, 1, cv2.THRESH_BINARY)\n",
    "    _, gt_mask_bin   = cv2.threshold(gt_mask_resized, 127, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Calculate Dice (DSC)\n",
    "    dice = dice_coefficient(pred_mask_bin, gt_mask_bin)\n",
    "    dice_scores.append(dice)\n",
    "    \n",
    "    # For Hausdorff distance, convert binary masks back to uint8 with values 0 or 255.\n",
    "    pred_mask_uint8 = (pred_mask_bin * 255).astype(np.uint8)\n",
    "    gt_mask_uint8   = (gt_mask_bin * 255).astype(np.uint8)\n",
    "    hd = hausdorff_distance(pred_mask_uint8, gt_mask_uint8)\n",
    "    hd_scores.append(hd)\n",
    "\n",
    "# Compute overall average metrics, ignoring NaN values in Hausdorff distance if any.\n",
    "mean_dice = np.nanmean(dice_scores)\n",
    "mean_hd = np.nanmean(hd_scores)\n",
    "\n",
    "print(\"Mean Dice Similarity Coefficient (DSC): {:.4f}\".format(mean_dice))\n",
    "print(\"Mean Hausdorff Distance (MHD): {:.4f} mm\".format(mean_hd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_idx = np.argmax(hd_scores)  # Index of image with max HD\n",
    "worst_file = pred_files[worst_idx]\n",
    "print(f\"Worst HD case: {worst_file} (HD = {hd_scores[worst_idx]:.2f} mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ceaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"HD stats: Median = {np.median(hd_scores):.2f} mm, 95th %ile = {np.percentile(hd_scores, 95):.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_outlier(pred_dir, gt_dir, worst_file):\n",
    "    base_name = worst_file.replace(\"seg_\", \"\").replace(\".png\", \"\")\n",
    "    gt_file = base_name + \"_Annotation.png\"\n",
    "\n",
    "    pred_mask = cv2.imread(os.path.join(pred_dir, worst_file), cv2.IMREAD_GRAYSCALE)\n",
    "    gt_mask = cv2.imread(os.path.join(gt_dir, gt_file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(121), plt.imshow(pred_mask, cmap='gray'), plt.title(\"Predicted\")\n",
    "    plt.subplot(122), plt.imshow(gt_mask, cmap='gray'), plt.title(\"Ground Truth\")\n",
    "    plt.suptitle(f\"Worst HD Case: {worst_file} (HD = 10.30 mm)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_outlier(pred_dir, gt_dir, \"seg_704_2HC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb148a",
   "metadata": {},
   "source": [
    "**Morphological Opening and Closing + Canny edge Detector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a94443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/lib/python3/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/lib/python3/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1781435/732511934.py\", line 1, in <module>\n",
      "    import cv2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1781435/732511934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = 'output_segmentations'\n",
    "output_folder = 'output_edges'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define a structuring element for the morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Loop over all segmented images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        # Read the segmentation image in grayscale\n",
    "        seg_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Apply morphological opening (erosion followed by dilation) to remove small artifacts\n",
    "        opened = cv2.morphologyEx(seg_img, cv2.MORPH_OPEN, kernel)\n",
    "        # Then apply morphological closing (dilation followed by erosion) to fill small holes\n",
    "        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Apply Canny edge detector to extract the contour\n",
    "        # Adjust thresholds as necessary (here, 50 and 150 are example values)\n",
    "        edges = cv2.Canny(closed, 50, 150)\n",
    "        \n",
    "        # Save the edge-detected image\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, edges)\n",
    "        \n",
    "        # Optionally, display the original segmentation, post-morphology, and edge image side by side\n",
    "#         plt.figure(figsize=(12, 4))\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(seg_img, cmap='gray')\n",
    "#         plt.title('Original Segmentation')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(closed, cmap='gray')\n",
    "#         plt.title('After Morphological Ops')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(edges, cmap='gray')\n",
    "#         plt.title('Canny Edges')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.show()\n",
    "        print(f\"Processed and saved edge image for: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587634d",
   "metadata": {},
   "source": [
    "**Morphological Opening and Closing + Canny edge Detector on filled annotations for MHD (Mean Hausdorff Distance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4a267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved edge image for: 531_2HC_Annotation.png\n",
      "Processed and saved edge image for: 586_HC_Annotation.png\n",
      "Processed and saved edge image for: 669_HC_Annotation.png\n",
      "Processed and saved edge image for: 587_HC_Annotation.png\n",
      "Processed and saved edge image for: 695_HC_Annotation.png\n",
      "Processed and saved edge image for: 516_HC_Annotation.png\n",
      "Processed and saved edge image for: 569_HC_Annotation.png\n",
      "Processed and saved edge image for: 520_HC_Annotation.png\n",
      "Processed and saved edge image for: 621_HC_Annotation.png\n",
      "Processed and saved edge image for: 617_2HC_Annotation.png\n",
      "Processed and saved edge image for: 591_HC_Annotation.png\n",
      "Processed and saved edge image for: 653_HC_Annotation.png\n",
      "Processed and saved edge image for: 566_2HC_Annotation.png\n",
      "Processed and saved edge image for: 506_HC_Annotation.png\n",
      "Processed and saved edge image for: 577_HC_Annotation.png\n",
      "Processed and saved edge image for: 613_3HC_Annotation.png\n",
      "Processed and saved edge image for: 570_HC_Annotation.png\n",
      "Processed and saved edge image for: 640_HC_Annotation.png\n",
      "Processed and saved edge image for: 618_HC_Annotation.png\n",
      "Processed and saved edge image for: 592_2HC_Annotation.png\n",
      "Processed and saved edge image for: 609_2HC_Annotation.png\n",
      "Processed and saved edge image for: 561_3HC_Annotation.png\n",
      "Processed and saved edge image for: 676_HC_Annotation.png\n",
      "Processed and saved edge image for: 583_2HC_Annotation.png\n",
      "Processed and saved edge image for: 518_HC_Annotation.png\n",
      "Processed and saved edge image for: 530_HC_Annotation.png\n",
      "Processed and saved edge image for: 689_HC_Annotation.png\n",
      "Processed and saved edge image for: 633_HC_Annotation.png\n",
      "Processed and saved edge image for: 628_2HC_Annotation.png\n",
      "Processed and saved edge image for: 521_HC_Annotation.png\n",
      "Processed and saved edge image for: 515_HC_Annotation.png\n",
      "Processed and saved edge image for: 544_2HC_Annotation.png\n",
      "Processed and saved edge image for: 500_HC_Annotation.png\n",
      "Processed and saved edge image for: 580_HC_Annotation.png\n",
      "Processed and saved edge image for: 574_2HC_Annotation.png\n",
      "Processed and saved edge image for: 584_HC_Annotation.png\n",
      "Processed and saved edge image for: 692_2HC_Annotation.png\n",
      "Processed and saved edge image for: 563_HC_Annotation.png\n",
      "Processed and saved edge image for: 704_2HC_Annotation.png\n",
      "Processed and saved edge image for: 652_HC_Annotation.png\n",
      "Processed and saved edge image for: 551_HC_Annotation.png\n",
      "Processed and saved edge image for: 610_HC_Annotation.png\n",
      "Processed and saved edge image for: 503_HC_Annotation.png\n",
      "Processed and saved edge image for: 642_HC_Annotation.png\n",
      "Processed and saved edge image for: 572_HC_Annotation.png\n",
      "Processed and saved edge image for: 512_HC_Annotation.png\n",
      "Processed and saved edge image for: 573_HC_Annotation.png\n",
      "Processed and saved edge image for: 687_HC_Annotation.png\n",
      "Processed and saved edge image for: 605_HC_Annotation.png\n",
      "Processed and saved edge image for: 632_HC_Annotation.png\n",
      "Processed and saved edge image for: 549_HC_Annotation.png\n",
      "Processed and saved edge image for: 680_HC_Annotation.png\n",
      "Processed and saved edge image for: 672_HC_Annotation.png\n",
      "Processed and saved edge image for: 649_HC_Annotation.png\n",
      "Processed and saved edge image for: 647_HC_Annotation.png\n",
      "Processed and saved edge image for: 659_HC_Annotation.png\n",
      "Processed and saved edge image for: 671_HC_Annotation.png\n",
      "Processed and saved edge image for: 703_HC_Annotation.png\n",
      "Processed and saved edge image for: 668_HC_Annotation.png\n",
      "Processed and saved edge image for: 507_HC_Annotation.png\n",
      "Processed and saved edge image for: 554_HC_Annotation.png\n",
      "Processed and saved edge image for: 660_HC_Annotation.png\n",
      "Processed and saved edge image for: 557_HC_Annotation.png\n",
      "Processed and saved edge image for: 627_HC_Annotation.png\n",
      "Processed and saved edge image for: 663_HC_Annotation.png\n",
      "Processed and saved edge image for: 501_HC_Annotation.png\n",
      "Processed and saved edge image for: 593_HC_Annotation.png\n",
      "Processed and saved edge image for: 575_HC_Annotation.png\n",
      "Processed and saved edge image for: 522_HC_Annotation.png\n",
      "Processed and saved edge image for: 681_HC_Annotation.png\n",
      "Processed and saved edge image for: 567_2HC_Annotation.png\n",
      "Processed and saved edge image for: 527_HC_Annotation.png\n",
      "Processed and saved edge image for: 566_HC_Annotation.png\n",
      "Processed and saved edge image for: 691_HC_Annotation.png\n",
      "Processed and saved edge image for: 684_HC_Annotation.png\n",
      "Processed and saved edge image for: 535_HC_Annotation.png\n",
      "Processed and saved edge image for: 505_HC_Annotation.png\n",
      "Processed and saved edge image for: 686_HC_Annotation.png\n",
      "Processed and saved edge image for: 504_HC_Annotation.png\n",
      "Processed and saved edge image for: 675_HC_Annotation.png\n",
      "Processed and saved edge image for: 502_HC_Annotation.png\n",
      "Processed and saved edge image for: 526_HC_Annotation.png\n",
      "Processed and saved edge image for: 656_HC_Annotation.png\n",
      "Processed and saved edge image for: 578_HC_Annotation.png\n",
      "Processed and saved edge image for: 648_2HC_Annotation.png\n",
      "Processed and saved edge image for: 603_HC_Annotation.png\n",
      "Processed and saved edge image for: 553_HC_Annotation.png\n",
      "Processed and saved edge image for: 606_HC_Annotation.png\n",
      "Processed and saved edge image for: 622_HC_Annotation.png\n",
      "Processed and saved edge image for: 590_HC_Annotation.png\n",
      "Processed and saved edge image for: 690_HC_Annotation.png\n",
      "Processed and saved edge image for: 525_HC_Annotation.png\n",
      "Processed and saved edge image for: 702_HC_Annotation.png\n",
      "Processed and saved edge image for: 615_HC_Annotation.png\n",
      "Processed and saved edge image for: 617_HC_Annotation.png\n",
      "Processed and saved edge image for: 526_2HC_Annotation.png\n",
      "Processed and saved edge image for: 637_HC_Annotation.png\n",
      "Processed and saved edge image for: 523_HC_Annotation.png\n",
      "Processed and saved edge image for: 567_HC_Annotation.png\n",
      "Processed and saved edge image for: 509_HC_Annotation.png\n",
      "Processed and saved edge image for: 636_HC_Annotation.png\n",
      "Processed and saved edge image for: 561_2HC_Annotation.png\n",
      "Processed and saved edge image for: 629_HC_Annotation.png\n",
      "Processed and saved edge image for: 562_HC_Annotation.png\n",
      "Processed and saved edge image for: 677_HC_Annotation.png\n",
      "Processed and saved edge image for: 613_HC_Annotation.png\n",
      "Processed and saved edge image for: 630_2HC_Annotation.png\n",
      "Processed and saved edge image for: 673_HC_Annotation.png\n",
      "Processed and saved edge image for: 588_2HC_Annotation.png\n",
      "Processed and saved edge image for: 594_HC_Annotation.png\n",
      "Processed and saved edge image for: 544_HC_Annotation.png\n",
      "Processed and saved edge image for: 595_HC_Annotation.png\n",
      "Processed and saved edge image for: 661_HC_Annotation.png\n",
      "Processed and saved edge image for: 536_HC_Annotation.png\n",
      "Processed and saved edge image for: 564_HC_Annotation.png\n",
      "Processed and saved edge image for: 677_2HC_Annotation.png\n",
      "Processed and saved edge image for: 574_HC_Annotation.png\n",
      "Processed and saved edge image for: 662_HC_Annotation.png\n",
      "Processed and saved edge image for: 582_HC_Annotation.png\n",
      "Processed and saved edge image for: 529_HC_Annotation.png\n",
      "Processed and saved edge image for: 541_HC_Annotation.png\n",
      "Processed and saved edge image for: 697_HC_Annotation.png\n",
      "Processed and saved edge image for: 599_HC_Annotation.png\n",
      "Processed and saved edge image for: 643_HC_Annotation.png\n",
      "Processed and saved edge image for: 540_HC_Annotation.png\n",
      "Processed and saved edge image for: 538_HC_Annotation.png\n",
      "Processed and saved edge image for: 565_HC_Annotation.png\n",
      "Processed and saved edge image for: 624_HC_Annotation.png\n",
      "Processed and saved edge image for: 583_HC_Annotation.png\n",
      "Processed and saved edge image for: 700_HC_Annotation.png\n",
      "Processed and saved edge image for: 507_2HC_Annotation.png\n",
      "Processed and saved edge image for: 688_HC_Annotation.png\n",
      "Processed and saved edge image for: 576_HC_Annotation.png\n",
      "Processed and saved edge image for: 596_HC_Annotation.png\n",
      "Processed and saved edge image for: 625_HC_Annotation.png\n",
      "Processed and saved edge image for: 639_2HC_Annotation.png\n",
      "Processed and saved edge image for: 508_HC_Annotation.png\n",
      "Processed and saved edge image for: 589_HC_Annotation.png\n",
      "Processed and saved edge image for: 611_HC_Annotation.png\n",
      "Processed and saved edge image for: 609_HC_Annotation.png\n",
      "Processed and saved edge image for: 655_HC_Annotation.png\n",
      "Processed and saved edge image for: 631_2HC_Annotation.png\n",
      "Processed and saved edge image for: 511_HC_Annotation.png\n",
      "Processed and saved edge image for: 552_HC_Annotation.png\n",
      "Processed and saved edge image for: 698_HC_Annotation.png\n",
      "Processed and saved edge image for: 630_HC_Annotation.png\n",
      "Processed and saved edge image for: 616_HC_Annotation.png\n",
      "Processed and saved edge image for: 612_HC_Annotation.png\n",
      "Processed and saved edge image for: 665_HC_Annotation.png\n",
      "Processed and saved edge image for: 555_HC_Annotation.png\n",
      "Processed and saved edge image for: 510_HC_Annotation.png\n",
      "Processed and saved edge image for: 683_HC_Annotation.png\n",
      "Processed and saved edge image for: 706_HC_Annotation.png\n",
      "Processed and saved edge image for: 561_HC_Annotation.png\n",
      "Processed and saved edge image for: 550_HC_Annotation.png\n",
      "Processed and saved edge image for: 664_HC_Annotation.png\n",
      "Processed and saved edge image for: 657_HC_Annotation.png\n",
      "Processed and saved edge image for: 579_HC_Annotation.png\n",
      "Processed and saved edge image for: 704_HC_Annotation.png\n",
      "Processed and saved edge image for: 602_HC_Annotation.png\n",
      "Processed and saved edge image for: 638_HC_Annotation.png\n",
      "Processed and saved edge image for: 604_HC_Annotation.png\n",
      "Processed and saved edge image for: 524_HC_Annotation.png\n",
      "Processed and saved edge image for: 657_2HC_Annotation.png\n",
      "Processed and saved edge image for: 631_HC_Annotation.png\n",
      "Processed and saved edge image for: 547_2HC_Annotation.png\n",
      "Processed and saved edge image for: 607_HC_Annotation.png\n",
      "Processed and saved edge image for: 667_HC_Annotation.png\n",
      "Processed and saved edge image for: 674_HC_Annotation.png\n",
      "Processed and saved edge image for: 546_HC_Annotation.png\n",
      "Processed and saved edge image for: 623_HC_Annotation.png\n",
      "Processed and saved edge image for: 701_HC_Annotation.png\n",
      "Processed and saved edge image for: 692_HC_Annotation.png\n",
      "Processed and saved edge image for: 652_2HC_Annotation.png\n",
      "Processed and saved edge image for: 690_2HC_Annotation.png\n",
      "Processed and saved edge image for: 598_HC_Annotation.png\n",
      "Processed and saved edge image for: 707_HC_Annotation.png\n",
      "Processed and saved edge image for: 519_HC_Annotation.png\n",
      "Processed and saved edge image for: 588_HC_Annotation.png\n",
      "Processed and saved edge image for: 651_HC_Annotation.png\n",
      "Processed and saved edge image for: 658_HC_Annotation.png\n",
      "Processed and saved edge image for: 646_HC_Annotation.png\n",
      "Processed and saved edge image for: 693_HC_Annotation.png\n",
      "Processed and saved edge image for: 641_HC_Annotation.png\n",
      "Processed and saved edge image for: 517_HC_Annotation.png\n",
      "Processed and saved edge image for: 635_HC_Annotation.png\n",
      "Processed and saved edge image for: 654_HC_Annotation.png\n",
      "Processed and saved edge image for: 513_HC_Annotation.png\n",
      "Processed and saved edge image for: 613_2HC_Annotation.png\n",
      "Processed and saved edge image for: 618_2HC_Annotation.png\n",
      "Processed and saved edge image for: 548_HC_Annotation.png\n",
      "Processed and saved edge image for: 532_2HC_Annotation.png\n",
      "Processed and saved edge image for: 650_HC_Annotation.png\n",
      "Processed and saved edge image for: 685_HC_Annotation.png\n",
      "Processed and saved edge image for: 626_HC_Annotation.png\n",
      "Processed and saved edge image for: 543_HC_Annotation.png\n",
      "Processed and saved edge image for: 639_HC_Annotation.png\n",
      "Processed and saved edge image for: 614_HC_Annotation.png\n",
      "Processed and saved edge image for: 556_2HC_Annotation.png\n",
      "Processed and saved edge image for: 532_HC_Annotation.png\n",
      "Processed and saved edge image for: 699_HC_Annotation.png\n",
      "Processed and saved edge image for: 679_HC_Annotation.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved edge image for: 570_3HC_Annotation.png\n",
      "Processed and saved edge image for: 587_2HC_Annotation.png\n",
      "Processed and saved edge image for: 705_HC_Annotation.png\n",
      "Processed and saved edge image for: 528_HC_Annotation.png\n",
      "Processed and saved edge image for: 556_HC_Annotation.png\n",
      "Processed and saved edge image for: 542_HC_Annotation.png\n",
      "Processed and saved edge image for: 675_2HC_Annotation.png\n",
      "Processed and saved edge image for: 644_HC_Annotation.png\n",
      "Processed and saved edge image for: 541_2HC_Annotation.png\n",
      "Processed and saved edge image for: 708_2HC_Annotation.png\n",
      "Processed and saved edge image for: 531_HC_Annotation.png\n",
      "Processed and saved edge image for: 547_HC_Annotation.png\n",
      "Processed and saved edge image for: 537_HC_Annotation.png\n",
      "Processed and saved edge image for: 673_2HC_Annotation.png\n",
      "Processed and saved edge image for: 581_HC_Annotation.png\n",
      "Processed and saved edge image for: 571_HC_Annotation.png\n",
      "Processed and saved edge image for: 671_3HC_Annotation.png\n",
      "Processed and saved edge image for: 694_HC_Annotation.png\n",
      "Processed and saved edge image for: 608_HC_Annotation.png\n",
      "Processed and saved edge image for: 670_HC_Annotation.png\n",
      "Processed and saved edge image for: 666_HC_Annotation.png\n",
      "Processed and saved edge image for: 560_HC_Annotation.png\n",
      "Processed and saved edge image for: 696_HC_Annotation.png\n",
      "Processed and saved edge image for: 545_2HC_Annotation.png\n",
      "Processed and saved edge image for: 514_HC_Annotation.png\n",
      "Processed and saved edge image for: 634_HC_Annotation.png\n",
      "Processed and saved edge image for: 619_HC_Annotation.png\n",
      "Processed and saved edge image for: 600_HC_Annotation.png\n",
      "Processed and saved edge image for: 559_HC_Annotation.png\n",
      "Processed and saved edge image for: 628_HC_Annotation.png\n",
      "Processed and saved edge image for: 545_HC_Annotation.png\n",
      "Processed and saved edge image for: 601_HC_Annotation.png\n",
      "Processed and saved edge image for: 682_HC_Annotation.png\n",
      "Processed and saved edge image for: 620_HC_Annotation.png\n",
      "Processed and saved edge image for: 585_HC_Annotation.png\n",
      "Processed and saved edge image for: 558_HC_Annotation.png\n",
      "Processed and saved edge image for: 539_HC_Annotation.png\n",
      "Processed and saved edge image for: 597_HC_Annotation.png\n",
      "Processed and saved edge image for: 570_2HC_Annotation.png\n",
      "Processed and saved edge image for: 533_HC_Annotation.png\n",
      "Processed and saved edge image for: 678_HC_Annotation.png\n",
      "Processed and saved edge image for: 671_2HC_Annotation.png\n",
      "Processed and saved edge image for: 648_HC_Annotation.png\n",
      "Processed and saved edge image for: 534_HC_Annotation.png\n",
      "Processed and saved edge image for: 645_HC_Annotation.png\n",
      "Processed and saved edge image for: 592_HC_Annotation.png\n",
      "Processed and saved edge image for: 686_2HC_Annotation.png\n",
      "Processed and saved edge image for: 568_HC_Annotation.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define input folder for the filled annotations and the output folder for the processed annotations\n",
    "input_folder = 'masked_annotations_test'  # Folder with filled ground truth annotations, e.g., \"500_HC_Annotation.png\"\n",
    "output_folder = 'postprocessing_annotations'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define a structuring element for the morphological operations (same as used on the predictions)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "resized_dim = 256\n",
    "\n",
    "# Loop over all annotation images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        # Read the filled annotation image in grayscale\n",
    "        annotation_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        annotation_img = cv2.resize(\n",
    "            annotation_img,\n",
    "            (resized_dim, resized_dim),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "        \n",
    "        # Apply morphological opening (erosion followed by dilation) to remove small artifacts\n",
    "        opened = cv2.morphologyEx(annotation_img, cv2.MORPH_OPEN, kernel)\n",
    "        # Then apply morphological closing (dilation followed by erosion) to fill small holes\n",
    "        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Apply Canny edge detector to extract the boundary\n",
    "        # Adjust thresholds if necessary; here using 50 and 150 as example values\n",
    "        edges = cv2.Canny(closed, 50, 150)\n",
    "        \n",
    "        # Save the processed (edge-detected) annotation image\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, edges)\n",
    "        \n",
    "        # Optionally display the images side by side\n",
    "        # plt.figure(figsize=(12, 4))\n",
    "        # plt.subplot(1, 3, 1)\n",
    "        # plt.imshow(annotation_img, cmap='gray')\n",
    "        # plt.title('Original Annotation')\n",
    "        # plt.axis('off')\n",
    "        #\n",
    "        # plt.subplot(1, 3, 2)\n",
    "        # plt.imshow(closed, cmap='gray')\n",
    "        # plt.title('After Morphological Ops')\n",
    "        # plt.axis('off')\n",
    "        #\n",
    "        # plt.subplot(1, 3, 3)\n",
    "        # plt.imshow(edges, cmap='gray')\n",
    "        # plt.title('Canny Edges')\n",
    "        # plt.axis('off')\n",
    "        #\n",
    "        # plt.show()\n",
    "        \n",
    "        print(f\"Processed and saved edge image for: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc5da6",
   "metadata": {},
   "source": [
    "**MHD (Mean Hausdorff Distance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13055df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Hausdorff Distance (MHD): 0.7077 mm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Load the CSV file with pixel size info.\n",
    "# ------------------------------------------------------------\n",
    "# The CSV (test_set_pixel_size.csv) is assumed to have columns \"filename\" and \"pixel size(mm)\"\n",
    "# where the filename corresponds to the original test image (e.g., \"500_HC.png\").\n",
    "pixel_size_dict = {}\n",
    "csv_file = 'test_set_pixel_size.csv'\n",
    "with open(csv_file, mode='r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        filename_csv = row['filename']  # e.g., \"500_HC.png\"\n",
    "        pixel_size_mm = float(row['pixel size(mm)'])\n",
    "        pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Set the resized dimension used in your inference pipeline.\n",
    "# ------------------------------------------------------------\n",
    "resized_dim = 256  # All images are resized to 256x256 during preprocessing/inference\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Define function to compute Hausdorff distance with coordinate transformation.\n",
    "# ------------------------------------------------------------\n",
    "def hausdorff_distance_transformed(pred_edges, true_edges, scale_x, scale_y):\n",
    "    \"\"\"\n",
    "    Computes the Hausdorff distance between two edge images after transforming\n",
    "    the coordinates from the resized (256x256) space to the original image space.\n",
    "    \n",
    "    Args:\n",
    "        pred_edges (np.array): Predicted edge image in 256x256 space (uint8),\n",
    "                               with edges as nonzero (e.g., 255).\n",
    "        true_edges (np.array): Ground truth edge image in 256x256 space (uint8).\n",
    "        scale_x (float): Factor to convert x coordinates.\n",
    "        scale_y (float): Factor to convert y coordinates.\n",
    "        \n",
    "    Returns:\n",
    "        float: Hausdorff distance in pixel units (in the original image space).\n",
    "    \"\"\"\n",
    "    # np.where returns (rows, cols) which are (y, x)\n",
    "    pred_points = np.column_stack(np.where(pred_edges != 0))\n",
    "    true_points = np.column_stack(np.where(true_edges != 0))\n",
    "    \n",
    "    if pred_points.size == 0 or true_points.size == 0:\n",
    "        return np.nan  # Return NaN if no edges are found in one of the images.\n",
    "    \n",
    "    # Transform predicted points from resized space to original space.\n",
    "    pred_points_transformed = np.zeros_like(pred_points, dtype=np.float32)\n",
    "    pred_points_transformed[:, 0] = pred_points[:, 0] * scale_y  # y coordinate\n",
    "    pred_points_transformed[:, 1] = pred_points[:, 1] * scale_x  # x coordinate\n",
    "    \n",
    "    # Similarly, transform ground truth points.\n",
    "    true_points_transformed = np.zeros_like(true_points, dtype=np.float32)\n",
    "    true_points_transformed[:, 0] = true_points[:, 0] * scale_y\n",
    "    true_points_transformed[:, 1] = true_points[:, 1] * scale_x\n",
    "    \n",
    "    d1 = directed_hausdorff(pred_points_transformed, true_points_transformed)[0]\n",
    "    d2 = directed_hausdorff(true_points_transformed, pred_points_transformed)[0]\n",
    "    return max(d1, d2)\n",
    "\n",
    "\n",
    "def mean_hausdorff_distance_transformed(pred_edges, true_edges, scale_x, scale_y):\n",
    "    \"\"\"\n",
    "    Computes the Mean Hausdorff Distance (MHD) between two edge images after coordinate transformation.\n",
    "    \"\"\"\n",
    "    pred_points = np.column_stack(np.where(pred_edges != 0))\n",
    "    true_points = np.column_stack(np.where(true_edges != 0))\n",
    "    \n",
    "    if pred_points.size == 0 or true_points.size == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Transform points to original image space\n",
    "    pred_points_transformed = pred_points * np.array([scale_y, scale_x])\n",
    "    true_points_transformed = true_points * np.array([scale_y, scale_x])\n",
    "    \n",
    "    # Compute MHD\n",
    "    dist_matrix = cdist(pred_points_transformed, true_points_transformed, 'euclidean')\n",
    "    min_dists_pred_to_true = np.min(dist_matrix, axis=1)\n",
    "    min_dists_true_to_pred = np.min(dist_matrix, axis=0)\n",
    "    mhd = np.mean(np.concatenate([min_dists_pred_to_true, min_dists_true_to_pred]))\n",
    "    \n",
    "    return mhd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3: Set directories for predicted edge images, ground truth boundary annotations,\n",
    "#         and the original test images.\n",
    "# ------------------------------------------------------------\n",
    "pred_edge_dir = 'output_edges'                  # Predicted edge images (resized: 256x256)\n",
    "gt_edge_dir = 'postprocessing_annotations'        # Ground truth boundaries (resized: 256x256)\n",
    "orig_image_dir = 'denoised_test_set'              # Original test images (actual dimensions), e.g., \"500_HC.png\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 4: Loop over predicted edge files, compute dynamic scaling, then compute MHD.\n",
    "# ------------------------------------------------------------\n",
    "pred_files = sorted([f for f in os.listdir(pred_edge_dir) if f.endswith('.png')])\n",
    "hd_scores_mm = []  # To store Hausdorff distances in millimeters\n",
    "\n",
    "for pred_file in pred_files:\n",
    "    pred_path = os.path.join(pred_edge_dir, pred_file)\n",
    "    \n",
    "    # Derive the base filename. For example, if the predicted file is \"seg_500_HC.png\",\n",
    "    # remove the \"seg_\" prefix to get \"500_HC.png\".\n",
    "    base_filename = pred_file.replace(\"seg_\", \"\", 1)  # e.g., \"500_HC.png\"\n",
    "    \n",
    "    # Build the key for CSV lookup (should match the original image filename)\n",
    "    csv_key = base_filename  # e.g., \"500_HC.png\"\n",
    "    if csv_key not in pixel_size_dict:\n",
    "        print(f\"No pixel size found for {csv_key}; skipping {pred_file}.\")\n",
    "        continue\n",
    "    pixel_to_mm = pixel_size_dict[csv_key]\n",
    "    \n",
    "    # Get the corresponding original test image from orig_image_dir.\n",
    "    orig_img_path = os.path.join(orig_image_dir, base_filename)\n",
    "    if not os.path.exists(orig_img_path):\n",
    "        print(f\"Original image {base_filename} not found; skipping {pred_file}.\")\n",
    "        continue\n",
    "    with Image.open(orig_img_path) as img:\n",
    "        orig_width, orig_height = img.size  # Dynamically retrieve original dimensions\n",
    "    \n",
    "    # Compute scale factors for this image:\n",
    "    scale_x = orig_width / float(resized_dim)\n",
    "    scale_y = orig_height / float(resized_dim)\n",
    "    \n",
    "    # Construct the corresponding ground truth (boundary) filename.\n",
    "    gt_file = base_filename.replace(\".png\", \"_Annotation.png\")  # e.g., \"500_HC_Annotation.png\"\n",
    "    gt_path = os.path.join(gt_edge_dir, gt_file)\n",
    "    if not os.path.exists(gt_path):\n",
    "        print(f\"Ground truth file {gt_file} not found for {pred_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Load predicted and ground truth edge images in grayscale (both should be 256x256)\n",
    "    pred_edges = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "    true_edges = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if pred_edges is None or true_edges is None:\n",
    "        print(f\"Error loading {pred_file} or {gt_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Compute Hausdorff distance in pixel units (transformed to original image coordinate space)\n",
    "    hd_pixels = mean_hausdorff_distance_transformed(pred_edges, true_edges, scale_x, scale_y)\n",
    "    if np.isnan(hd_pixels):\n",
    "        print(f\"No edges found in one of the images for {pred_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert the Hausdorff distance from original pixel units to millimeters.\n",
    "    hd_mm = hd_pixels * pixel_to_mm\n",
    "    hd_scores_mm.append(hd_mm)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 5: Compute and print overall Mean Hausdorff Distance in mm.\n",
    "# ------------------------------------------------------------\n",
    "mean_hd_mm = np.nanmean(hd_scores_mm)\n",
    "print(\"Mean Hausdorff Distance (MHD): {:.4f} mm\".format(mean_hd_mm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a792bc6",
   "metadata": {},
   "source": [
    "**Ellipse fiting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48dc8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#     import csv\n",
    "#     import cv2\n",
    "#     import numpy as np\n",
    "#     import os\n",
    "#     import math\n",
    "\n",
    "#     # --- Step 1: Load the pixel size information from the CSV file ---\n",
    "#     # Assume your CSV file 'test_set_pixel_size.csv' has at least these columns:\n",
    "#     # filename,pixel_size_mm\n",
    "#     pixel_size_dict = {}\n",
    "#     with open('test_set_pixel_size.csv', mode='r') as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         for row in reader:\n",
    "#             # Adjust the column names if they are different in your CSV file.\n",
    "#             filename_csv = row['filename']\n",
    "#             pixel_size_mm = float(row['pixel size(mm)'])\n",
    "#             pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "#     # --- Step 2: Process the edge images and fit ellipses ---\n",
    "#     edges_folder = 'output_edges'\n",
    "#     csv_output = 'ellipse_results.csv'\n",
    "#     header = [\"filename\", \"center_x_mm\", \"center_y_mm\", \"semi_axes_a_mm\", \"semi_axes_b_mm\", \"angle_rad\"]\n",
    "#     rows = []\n",
    "\n",
    "#     for filename in sorted(os.listdir(edges_folder)):\n",
    "#         if filename.endswith('.png'):\n",
    "#             filepath = os.path.join(edges_folder, filename)\n",
    "#             # Read the edge image in grayscale\n",
    "#             edge_img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#             # Find contours in the edge image\n",
    "#             contours, _ = cv2.findContours(edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             if len(contours) == 0:\n",
    "#                 print(f\"No contours found in {filename}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Choose the largest contour (assumed to be the head contour)\n",
    "#             largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "#             # cv2.fitEllipse requires at least 5 points\n",
    "#             if len(largest_contour) < 5:\n",
    "#                 print(f\"Not enough points for ellipse fitting in {filename}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Fit ellipse to the largest contour\n",
    "#             ellipse = cv2.fitEllipse(largest_contour)\n",
    "#             # ellipse returns ((center_x, center_y), (full_axis_length_a, full_axis_length_b), angle_in_degrees)\n",
    "#             center, axes, angle = ellipse\n",
    "#             # Compute semi-axes (the axes given are the full lengths)\n",
    "#             semi_a = axes[0] / 2.0  # semi-major axis in pixels\n",
    "#             semi_b = axes[1] / 2.0  # semi-minor axis in pixels\n",
    "\n",
    "#             # --- Step 3: Look up the pixel conversion factor for this image ---\n",
    "#             # The filenames in the CSV are expected to be like \"001_HC.png\"\n",
    "#             # and our edge images are named \"seg_001_HC.png\". Remove the \"seg_\" prefix.\n",
    "#             base_filename = filename.replace(\"seg_\", \"\", 1)\n",
    "#             if base_filename in pixel_size_dict:\n",
    "#                 pixel_to_mm = pixel_size_dict[base_filename]\n",
    "#             else:\n",
    "#                 print(f\"Pixel size for {base_filename} not found in CSV. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Convert measurements from pixels to millimeters\n",
    "#             center_x_mm = center[0] * pixel_to_mm\n",
    "#             center_y_mm = center[1] * pixel_to_mm\n",
    "#             semi_a_mm = semi_a * pixel_to_mm\n",
    "#             semi_b_mm = semi_b * pixel_to_mm\n",
    "\n",
    "#             # Convert angle from degrees to radians\n",
    "#             angle_rad = math.radians(angle)\n",
    "\n",
    "#             # Append the result: filename, center_x_mm, center_y_mm, semi_axes_a_mm, semi_axes_b_mm, angle_rad\n",
    "#             rows.append([base_filename, center_x_mm, center_y_mm, semi_a_mm, semi_b_mm, angle_rad])\n",
    "\n",
    "#     # --- Step 4: Write results to a CSV file ---\n",
    "#     with open(csv_output, mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(header)\n",
    "#         writer.writerows(rows)\n",
    "\n",
    "#     print(f\"CSV file '{csv_output}' saved with {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import math\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Define original image dimensions and resized dimensions\n",
    "# # ------------------------------------------------------------\n",
    "# original_width = 800    # original image width in pixels\n",
    "# original_height = 540   # original image height in pixels\n",
    "# resized_dim = 256       # both width and height after resizing\n",
    "\n",
    "# # Compute scale factors for converting from the resized space to the original space:\n",
    "# scale_x = original_width / float(resized_dim)   # e.g., 800/256\n",
    "# scale_y = original_height / float(resized_dim)  # e.g., 540/256\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Step 1: Load the pixel size information from the CSV file\n",
    "# # ------------------------------------------------------------\n",
    "# # The CSV (test_set_pixel_size.csv) is assumed to have columns \"filename\" and \"pixel size(mm)\" \n",
    "# # where the filename is based on the original image (e.g., \"001_HC.png\").\n",
    "# pixel_size_dict = {}\n",
    "# csv_file = 'test_set_pixel_size.csv'\n",
    "# with open(csv_file, mode='r') as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     for row in reader:\n",
    "#         filename_csv = row['filename']  # e.g., \"001_HC.png\"\n",
    "#         pixel_size_mm = float(row['pixel size(mm)'])\n",
    "#         pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Step 2: Process the edge images (from output_edges folder) and fit ellipses.\n",
    "# #         Note: These edge images are produced from images resized to 256x256.\n",
    "# # ------------------------------------------------------------\n",
    "# edges_folder = 'output_edges'\n",
    "# csv_output = 'ellipse_results.csv'\n",
    "# header = [\"filename\", \"center_x_mm\", \"center_y_mm\", \"semi_axes_a_mm\", \"semi_axes_b_mm\", \"angle_rad\"]\n",
    "# rows = []\n",
    "\n",
    "# for filename in sorted(os.listdir(edges_folder)):\n",
    "#     if filename.endswith('.png'):\n",
    "#         filepath = os.path.join(edges_folder, filename)\n",
    "#         # Read the edge image in grayscale (should be 256x256)\n",
    "#         edge_img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "#         if edge_img is None:\n",
    "#             print(f\"Error loading {filename}; skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Find contours in the edge image\n",
    "#         contours, _ = cv2.findContours(edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#         if len(contours) == 0:\n",
    "#             print(f\"No contours found in {filename}. Skipping.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Choose the largest contour (assumed to be the head contour)\n",
    "#         largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "#         # cv2.fitEllipse requires at least 5 points\n",
    "#         if len(largest_contour) < 5:\n",
    "#             print(f\"Not enough points for ellipse fitting in {filename}. Skipping.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Fit ellipse to the largest contour\n",
    "#         ellipse = cv2.fitEllipse(largest_contour)\n",
    "#         # ellipse returns ((center_x, center_y), (full_axis_length_a, full_axis_length_b), angle_in_degrees)\n",
    "#         center, axes, angle = ellipse\n",
    "#         # Compute semi-axes in the resized coordinate space\n",
    "#         semi_a_resized = axes[0] / 2.0\n",
    "#         semi_b_resized = axes[1] / 2.0\n",
    "\n",
    "#         # Scale the center and semi-axes from resized (256x256) to original image coordinate space\n",
    "#         center_x_original = center[0] * scale_x\n",
    "#         center_y_original = center[1] * scale_y\n",
    "#         semi_a_original = semi_a_resized * scale_x\n",
    "#         semi_b_original = semi_b_resized * scale_y\n",
    "\n",
    "#         # --- Step 3: Look up the pixel-to-mm conversion factor for this image ---\n",
    "#         # The CSV expects file names like \"001_HC.png\"; our predictions are named \"seg_001_HC.png\"\n",
    "#         base_filename = filename.replace(\"seg_\", \"\", 1)  # e.g., \"001_HC.png\"\n",
    "#         if base_filename in pixel_size_dict:\n",
    "#             pixel_to_mm = pixel_size_dict[base_filename]\n",
    "#         else:\n",
    "#             print(f\"Pixel size for {base_filename} not found in CSV. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Convert measurements from original pixel space to millimeters\n",
    "#         center_x_mm = center_x_original * pixel_to_mm\n",
    "#         center_y_mm = center_y_original * pixel_to_mm\n",
    "#         semi_a_mm = semi_a_original * pixel_to_mm\n",
    "#         semi_b_mm = semi_b_original * pixel_to_mm\n",
    "\n",
    "#         # Convert angle from degrees to radians\n",
    "#         angle_rad = math.radians(angle)\n",
    "\n",
    "#         # Append the result\n",
    "#         rows.append([base_filename, center_x_mm, center_y_mm, semi_a_mm, semi_b_mm, angle_rad])\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Step 4: Write results to a CSV file\n",
    "# # ------------------------------------------------------------\n",
    "# with open(csv_output, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerows(rows)\n",
    "\n",
    "# print(f\"CSV file '{csv_output}' saved with {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c083a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define resized dimension (from your on-the-fly augmentation)\n",
    "# ------------------------------------------------------------\n",
    "resized_dim = 256  # images are resized to 256x256 during preprocessing/inference\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Load the pixel size information from the CSV file\n",
    "# ------------------------------------------------------------\n",
    "# The CSV (test_set_pixel_size.csv) is assumed to have columns \"filename\" and \"pixel size(mm)\"\n",
    "# where the filename is based on the original test image (e.g., \"500_HC.png\").\n",
    "pixel_size_dict = {}\n",
    "csv_file = 'test_set_pixel_size.csv'\n",
    "with open(csv_file, mode='r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        filename_csv = row['filename']  # e.g., \"500_HC.png\"\n",
    "        pixel_size_mm = float(row['pixel size(mm)'])\n",
    "        pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Folder for original test images (to retrieve dynamic dimensions)\n",
    "# ------------------------------------------------------------\n",
    "orig_image_dir = 'denoised_test_set'  # Test images are stored here (e.g., \"500_HC.png\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Process the edge images (from output_edges folder) and fit ellipses.\n",
    "#         Note: These edge images are produced from images resized to 256x256.\n",
    "# ------------------------------------------------------------\n",
    "edges_folder = 'output_edges'\n",
    "csv_output = 'ellipse_results.csv'\n",
    "header = [\"filename\", \"center_x_mm\", \"center_y_mm\", \"semi_axes_a_mm\", \"semi_axes_b_mm\", \"angle_rad\"]\n",
    "rows = []\n",
    "\n",
    "for filename in sorted(os.listdir(edges_folder)):\n",
    "    if filename.endswith('.png'):\n",
    "        filepath = os.path.join(edges_folder, filename)\n",
    "        # Read the edge image in grayscale (expected size: 256x256)\n",
    "        edge_img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        if edge_img is None:\n",
    "            print(f\"Error loading {filename}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Find contours in the edge image\n",
    "        contours, _ = cv2.findContours(edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 0:\n",
    "            print(f\"No contours found in {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Choose the largest contour (assumed to be the head contour)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # cv2.fitEllipse requires at least 5 points\n",
    "        if len(largest_contour) < 5:\n",
    "            print(f\"Not enough points for ellipse fitting in {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Fit ellipse to the largest contour\n",
    "        ellipse = cv2.fitEllipse(largest_contour)\n",
    "        # ellipse returns ((center_x, center_y), (full_axis_length_a, full_axis_length_b), angle_in_degrees)\n",
    "        center, axes, angle = ellipse\n",
    "        # Compute semi-axes in the resized coordinate space\n",
    "        semi_a_resized = axes[0] / 2.0\n",
    "        semi_b_resized = axes[1] / 2.0\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Step 3: Dynamically determine the original image dimensions.\n",
    "        # ------------------------------------------------------------\n",
    "        # The predictions are based on images that were resized from the original.\n",
    "        # For a given predicted file, its base name (e.g., \"500_HC.png\") is used to find the corresponding original image.\n",
    "        # For test data, the predicted file is likely named \"seg_500_HC.png\" (if using the same prefix convention).\n",
    "        base_filename = filename.replace(\"seg_\", \"\", 1)  # e.g., \"500_HC.png\"\n",
    "        orig_img_path = os.path.join(orig_image_dir, base_filename)\n",
    "        if not os.path.exists(orig_img_path):\n",
    "            print(f\"Original test image {base_filename} not found; skipping {filename}.\")\n",
    "            continue\n",
    "        with Image.open(orig_img_path) as img:\n",
    "            orig_width, orig_height = img.size  # (width, height)\n",
    "\n",
    "        # Compute scale factors for this image from the resized dimension to its original dimensions.\n",
    "        scale_x = orig_width / float(resized_dim)\n",
    "        scale_y = orig_height / float(resized_dim)\n",
    "        \n",
    "        # ------------------------------------------------------------\n",
    "        # Step 4: Scale the ellipse parameters from the resized (256x256) space back to the original space.\n",
    "        # ------------------------------------------------------------\n",
    "        center_x_original = center[0] * scale_x\n",
    "        center_y_original = center[1] * scale_y\n",
    "        semi_a_original = semi_a_resized * scale_x\n",
    "        semi_b_original = semi_b_resized * scale_y\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Step 5: Look up the pixel-to-mm conversion factor for this image.\n",
    "        # The CSV expects file names like \"500_HC.png\"; our base_filename should match.\n",
    "        # ------------------------------------------------------------\n",
    "        if base_filename in pixel_size_dict:\n",
    "            pixel_to_mm = pixel_size_dict[base_filename]\n",
    "        else:\n",
    "            print(f\"Pixel size for {base_filename} not found in CSV. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Convert the measurements from original pixel space to millimeters.\n",
    "        center_x_mm = center_x_original * pixel_to_mm\n",
    "        center_y_mm = center_y_original * pixel_to_mm\n",
    "        semi_a_mm = semi_a_original * pixel_to_mm\n",
    "        semi_b_mm = semi_b_original * pixel_to_mm\n",
    "\n",
    "        # Convert angle from degrees to radians\n",
    "        angle_rad = math.radians(angle)\n",
    "\n",
    "        # Append the results: filename, center_x_mm, center_y_mm, semi_axes_a_mm, semi_axes_b_mm, angle_rad\n",
    "        rows.append([base_filename, center_x_mm, center_y_mm, semi_a_mm, semi_b_mm, angle_rad])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 6: Write results to a CSV file\n",
    "# ------------------------------------------------------------\n",
    "with open(csv_output, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"CSV file '{csv_output}' saved with {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
