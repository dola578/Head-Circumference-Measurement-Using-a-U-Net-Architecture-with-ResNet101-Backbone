{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1801f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aecddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683713fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy opencv-python -y\n",
    "# !pip install --upgrade \"numpy==1.23.5\"\n",
    "# !pip install --upgrade \"scikit-image<0.25\"\n",
    "# !pip install --no-deps opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801e54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall typing_extensions\n",
    "# !pip install typing_extensions==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c8f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions>=4.3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1b91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30e90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing_extensions==4.12.2\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "Successfully installed typing_extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install typing_extensions==4.12.2 --upgrade\n",
    "# pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8758f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypeIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee1b1c",
   "metadata": {},
   "source": [
    "**Preprocessing - flipping, resizing, rotation, intensity based transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09bb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# Custom joint transformation that applies geometric transforms to both image and mask,\n",
    "# and intensity transforms only to the image.\n",
    "class JointTransform:\n",
    "    def __init__(self, resize=(256, 256), rotation=20, hflip_prob=0.5, vflip_prob=0.5,\n",
    "                 intensity_transforms=None):\n",
    "        self.resize = resize\n",
    "        self.rotation = rotation\n",
    "        self.hflip_prob = hflip_prob\n",
    "        self.vflip_prob = vflip_prob\n",
    "        # Intensity transforms should be a torchvision transform applied only to the image.\n",
    "        # For example, transforms.Compose([transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        #                                  transforms.Lambda(lambda x: x.pow(0.5))])\n",
    "        self.intensity_transforms = intensity_transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # 1. Resize both image and mask\n",
    "        image = TF.resize(image, self.resize)\n",
    "        mask = TF.resize(mask, self.resize)\n",
    "        \n",
    "        # 2. Random horizontal flip\n",
    "        if np.random.rand() < self.hflip_prob:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "            \n",
    "        # 3. Random vertical flip\n",
    "        if np.random.rand() < self.vflip_prob:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "            \n",
    "        # 4. Random rotation\n",
    "        angle = np.random.uniform(-self.rotation, self.rotation)\n",
    "        image = TF.rotate(image, angle, interpolation=Image.BILINEAR)\n",
    "        # For masks, use nearest neighbor interpolation to preserve label boundaries.\n",
    "        mask = TF.rotate(mask, angle, interpolation=Image.NEAREST)\n",
    "        \n",
    "        # 5. Apply intensity transforms to the image only (if provided)\n",
    "        if self.intensity_transforms:\n",
    "            image = self.intensity_transforms(image)\n",
    "        \n",
    "        # 6. Convert both image and mask to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        # Optionally ensure the mask is binary\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define intensity-only transformations for the image.\n",
    "intensity_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "#     transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5))  # Gamma correction with gamma=0.5\n",
    "])\n",
    "\n",
    "# Create the joint transformation instance\n",
    "joint_transform = JointTransform(resize=(256, 256), rotation=20,\n",
    "                                 hflip_prob=0.5, vflip_prob=0.5,\n",
    "                                 intensity_transforms=intensity_transforms)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, joint_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.joint_transform = joint_transform\n",
    "        # Sort the file lists to ensure alignment between images and masks\n",
    "        self.images = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')])\n",
    "        self.masks = sorted([os.path.join(mask_dir, x) for x in os.listdir(mask_dir) if 'Annotation' in x])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.joint_transform:\n",
    "            image, mask = self.joint_transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "# Initialize dataset with the joint transform\n",
    "full_dataset = CustomDataset('denoised_training_set', 'masked_annotations', joint_transform=joint_transform)\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# validation_size = len(full_dataset) - train_size\n",
    "# train_dataset, validation_dataset = random_split(full_dataset, [train_size, validation_size])\n",
    "\n",
    "# # Create separate dataloaders for train and validation datasets\n",
    "# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "# validation_loader = DataLoader(validation_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92cc602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     img, mask = full_dataset[i]\n",
    "#     print(f\"Sample {i}:\")\n",
    "#     print(\"  Image type:\", type(img), \"shape:\", img.shape, \"dtype:\", img.dtype)\n",
    "#     print(\"  Mask type:\", type(mask), \"shape:\", mask.shape, \"dtype:\", mask.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f45942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data samples: 749\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data samples:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d114c",
   "metadata": {},
   "source": [
    "**Checking alignment and order of train image and the corresponding masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12258a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path, mask_path in zip(sorted(os.listdir('denoised_training_set')), \n",
    "                               sorted(os.listdir('masked_annotations'))):\n",
    "    if img_path.endswith('.png') and 'Annotation' in mask_path:\n",
    "        base_img = os.path.splitext(img_path)[0]\n",
    "        base_mask = os.path.splitext(mask_path)[0].replace('_Annotation', '')\n",
    "        assert base_img == base_mask, f\"Mismatch: {base_img} vs {base_mask}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61e302",
   "metadata": {},
   "source": [
    "**Unet with resnet101 as backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a664895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ResConv(nn.Module):\n",
    "    \"\"\" Convolution block for U-Net with repeated convolutions and ReLU activations. \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ResConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\" Upsampling block for U-Net, using bilinear interpolation and convolution. \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = ResConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        from_up = self.up(from_up)\n",
    "        diffY = from_down.size()[2] - from_up.size()[2]\n",
    "        diffX = from_down.size()[3] - from_up.size()[3]\n",
    "        from_up = F.pad(from_up, [diffX // 2, diffX - diffX // 2,\n",
    "                                  diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([from_down, from_up], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetResNet101(nn.Module):\n",
    "    def __init__(self, n_classes=1):\n",
    "        super(UNetResNet101, self).__init__()\n",
    "        base_model = models.resnet101(pretrained=True)\n",
    "        self.base_layers = list(base_model.children())\n",
    "        \n",
    "        # Extract layers from ResNet101\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3])  # conv1, bn1, relu\n",
    "        self.maxpool = self.base_layers[3]\n",
    "        self.layer1 = self.base_layers[4]  # Output: 256 channels\n",
    "        self.layer2 = self.base_layers[5]  # Output: 512 channels\n",
    "        self.layer3 = self.base_layers[6]  # Output: 1024 channels\n",
    "        self.layer4 = self.base_layers[7]  # Output: 2048 channels\n",
    "\n",
    "        # Decoder (make sure the channel numbers match the skip connection outputs)\n",
    "        self.up4 = UpConv(2048 + 1024, 1024)  # Concatenating x3 (1024) and x4 (2048) -> 3072 channels\n",
    "        self.up3 = UpConv(1024 + 512, 512)    # Concatenating x2 (512) and previous output (1024) -> 1536 channels\n",
    "        self.up2 = UpConv(512 + 256, 256)     # Concatenating x1 (256) and previous output (512) -> 768 channels\n",
    "        self.up1 = UpConv(256 + 64, 64)       # Concatenating x0 (64) and previous output (256) -> 320 channels\n",
    "\n",
    "        # Final upsampling and output convolution to match input size\n",
    "        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder: get intermediate features for skip connections\n",
    "        x0 = self.layer0(x)       # Early features, e.g., 64 channels\n",
    "        x1 = self.maxpool(x0)\n",
    "        x1 = self.layer1(x1)      # 256 channels\n",
    "        x2 = self.layer2(x1)      # 512 channels\n",
    "        x3 = self.layer3(x2)      # 1024 channels\n",
    "        x4 = self.layer4(x3)      # 2048 channels\n",
    "\n",
    "        # Decoder: use skip connections from intermediate features\n",
    "        x = self.up4(x3, x4)      # Upsample: x3 (from_down) + x4 (from_up)\n",
    "        x = self.up3(x2, x)       # Upsample: x2 + output of previous block\n",
    "        x = self.up2(x1, x)       # Upsample: x1 + output of previous block\n",
    "        x = self.up1(x0, x)       # Upsample: x0 + output of previous block\n",
    "\n",
    "        x = self.final_up(x)      # Final upsampling to the original size\n",
    "        x = self.final_conv(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22e554",
   "metadata": {},
   "source": [
    "**Dice loss + Binary cross-entropy loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f608ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss   = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        bce = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        return bce + dice_loss\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs, targets: (B,1,H,W) probabilities and binaries\n",
    "        # compute morphological gradient: max_pool – min_pool\n",
    "        max_t = F.max_pool2d(targets, kernel_size=3, stride=1, padding=1)\n",
    "        min_t = -F.max_pool2d(-targets, kernel_size=3, stride=1, padding=1)\n",
    "        bd_t  = max_t - min_t\n",
    "\n",
    "        max_p = F.max_pool2d(inputs, kernel_size=3, stride=1, padding=1)\n",
    "        min_p = -F.max_pool2d(-inputs, kernel_size=3, stride=1, padding=1)\n",
    "        bd_p  = max_p - min_p\n",
    "\n",
    "        # L1 on the boundary maps\n",
    "        return F.l1_loss(bd_p, bd_t)\n",
    "\n",
    "class DiceBCEBoundaryLoss(nn.Module):\n",
    "    def __init__(self, boundary_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.seg_loss     = DiceBCELoss()\n",
    "        self.boundary_loss= BoundaryLoss()\n",
    "        self.boundary_w   = boundary_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        seg = self.seg_loss(inputs, targets)\n",
    "        bnd = self.boundary_loss(inputs, targets)\n",
    "        return seg + self.boundary_w * bnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb63fea",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a6214ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87d316b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/50, Train Loss: 0.5817, Train F1: 0.9583, Val Loss: 0.4886, Val F1: 0.9571\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.4321, Train F1: 0.9682, Val Loss: 0.4138, Val F1: 0.9630\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.3668, Train F1: 0.9714, Val Loss: 0.3447, Val F1: 0.9704\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.3311, Train F1: 0.9700, Val Loss: 0.3174, Val F1: 0.9691\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.2864, Train F1: 0.9744, Val Loss: 0.2831, Val F1: 0.9718\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.2585, Train F1: 0.9751, Val Loss: 0.2608, Val F1: 0.9710\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.2341, Train F1: 0.9759, Val Loss: 0.2273, Val F1: 0.9734\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.2116, Train F1: 0.9761, Val Loss: 0.2107, Val F1: 0.9742\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.1957, Train F1: 0.9765, Val Loss: 0.1984, Val F1: 0.9752\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.1782, Train F1: 0.9764, Val Loss: 0.1811, Val F1: 0.9739\n",
      "Saved overlay image at: overlay_images/fold1_epoch10.png\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.1682, Train F1: 0.9774, Val Loss: 0.1801, Val F1: 0.9743\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.1572, Train F1: 0.9762, Val Loss: 0.1675, Val F1: 0.9736\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.1481, Train F1: 0.9778, Val Loss: 0.1447, Val F1: 0.9758\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.1359, Train F1: 0.9770, Val Loss: 0.1478, Val F1: 0.9728\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.1288, Train F1: 0.9785, Val Loss: 0.1345, Val F1: 0.9761\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.1257, Train F1: 0.9781, Val Loss: 0.1388, Val F1: 0.9729\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.1435, Train F1: 0.9701, Val Loss: 0.1517, Val F1: 0.9700\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.1328, Train F1: 0.9736, Val Loss: 0.1411, Val F1: 0.9703\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.1199, Train F1: 0.9741, Val Loss: 0.1532, Val F1: 0.9646\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.1205, Train F1: 0.9763, Val Loss: 0.1224, Val F1: 0.9732\n",
      "Early stopping triggered in fold 1 at epoch 20\n",
      "Starting fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/50, Train Loss: 0.7224, Train F1: 0.8930, Val Loss: 0.6952, Val F1: 0.8895\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.5740, Train F1: 0.9653, Val Loss: 0.5789, Val F1: 0.9627\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.5145, Train F1: 0.9680, Val Loss: 0.5073, Val F1: 0.9652\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.4642, Train F1: 0.9654, Val Loss: 0.4723, Val F1: 0.9616\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.4177, Train F1: 0.9714, Val Loss: 0.4101, Val F1: 0.9681\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.3816, Train F1: 0.9749, Val Loss: 0.3783, Val F1: 0.9722\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.3466, Train F1: 0.9751, Val Loss: 0.3507, Val F1: 0.9727\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.3172, Train F1: 0.9765, Val Loss: 0.3237, Val F1: 0.9727\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.2919, Train F1: 0.9708, Val Loss: 0.3048, Val F1: 0.9667\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.2739, Train F1: 0.9765, Val Loss: 0.2754, Val F1: 0.9726\n",
      "Saved overlay image at: overlay_images/fold2_epoch10.png\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.2466, Train F1: 0.9778, Val Loss: 0.2543, Val F1: 0.9737\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.2366, Train F1: 0.9579, Val Loss: 0.3004, Val F1: 0.9557\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.2199, Train F1: 0.9763, Val Loss: 0.2097, Val F1: 0.9742\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.1986, Train F1: 0.9745, Val Loss: 0.2183, Val F1: 0.9713\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.1864, Train F1: 0.9770, Val Loss: 0.1927, Val F1: 0.9741\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.1722, Train F1: 0.9771, Val Loss: 0.1849, Val F1: 0.9740\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.1636, Train F1: 0.9792, Val Loss: 0.1672, Val F1: 0.9750\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.1518, Train F1: 0.9803, Val Loss: 0.1707, Val F1: 0.9763\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.1428, Train F1: 0.9792, Val Loss: 0.1523, Val F1: 0.9755\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.1355, Train F1: 0.9805, Val Loss: 0.1443, Val F1: 0.9765\n",
      "Saved overlay image at: overlay_images/fold2_epoch20.png\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.1291, Train F1: 0.9809, Val Loss: 0.1379, Val F1: 0.9765\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.1242, Train F1: 0.9813, Val Loss: 0.1311, Val F1: 0.9768\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.1186, Train F1: 0.9796, Val Loss: 0.1310, Val F1: 0.9761\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.1136, Train F1: 0.9806, Val Loss: 0.1258, Val F1: 0.9764\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.1092, Train F1: 0.9811, Val Loss: 0.1252, Val F1: 0.9768\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.1057, Train F1: 0.9821, Val Loss: 0.1184, Val F1: 0.9773\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.0999, Train F1: 0.9822, Val Loss: 0.1153, Val F1: 0.9771\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.0975, Train F1: 0.9802, Val Loss: 0.1194, Val F1: 0.9747\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.0988, Train F1: 0.9803, Val Loss: 0.1091, Val F1: 0.9762\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.0951, Train F1: 0.9812, Val Loss: 0.1088, Val F1: 0.9768\n",
      "Saved overlay image at: overlay_images/fold2_epoch30.png\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.0910, Train F1: 0.9821, Val Loss: 0.1046, Val F1: 0.9763\n",
      "Early stopping triggered in fold 2 at epoch 31\n",
      "Starting fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1/50, Train Loss: 0.5439, Train F1: 0.9574, Val Loss: 0.4625, Val F1: 0.9541\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.3919, Train F1: 0.9694, Val Loss: 0.3712, Val F1: 0.9673\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.3399, Train F1: 0.9726, Val Loss: 0.3358, Val F1: 0.9699\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.2965, Train F1: 0.9696, Val Loss: 0.2961, Val F1: 0.9688\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.2641, Train F1: 0.9753, Val Loss: 0.2595, Val F1: 0.9732\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.2387, Train F1: 0.9746, Val Loss: 0.2446, Val F1: 0.9726\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.2168, Train F1: 0.9778, Val Loss: 0.2200, Val F1: 0.9749\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.2011, Train F1: 0.9765, Val Loss: 0.2066, Val F1: 0.9735\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.1849, Train F1: 0.9765, Val Loss: 0.1951, Val F1: 0.9743\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.1724, Train F1: 0.9745, Val Loss: 0.1827, Val F1: 0.9721\n",
      "Saved overlay image at: overlay_images/fold3_epoch10.png\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.1595, Train F1: 0.9776, Val Loss: 0.1659, Val F1: 0.9744\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.1471, Train F1: 0.9772, Val Loss: 0.1586, Val F1: 0.9749\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.1391, Train F1: 0.9763, Val Loss: 0.1515, Val F1: 0.9736\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.1328, Train F1: 0.9803, Val Loss: 0.1392, Val F1: 0.9766\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.1243, Train F1: 0.9801, Val Loss: 0.1303, Val F1: 0.9769\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.1198, Train F1: 0.9798, Val Loss: 0.1306, Val F1: 0.9758\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.1133, Train F1: 0.9788, Val Loss: 0.1276, Val F1: 0.9757\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.1085, Train F1: 0.9804, Val Loss: 0.1195, Val F1: 0.9763\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.1053, Train F1: 0.9806, Val Loss: 0.1161, Val F1: 0.9761\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.1008, Train F1: 0.9822, Val Loss: 0.1104, Val F1: 0.9774\n",
      "Saved overlay image at: overlay_images/fold3_epoch20.png\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.0947, Train F1: 0.9818, Val Loss: 0.1133, Val F1: 0.9758\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.0923, Train F1: 0.9788, Val Loss: 0.1079, Val F1: 0.9750\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.0904, Train F1: 0.9813, Val Loss: 0.1048, Val F1: 0.9767\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.0883, Train F1: 0.9814, Val Loss: 0.1022, Val F1: 0.9763\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.0865, Train F1: 0.9817, Val Loss: 0.1008, Val F1: 0.9760\n",
      "Early stopping triggered in fold 3 at epoch 25\n",
      "Starting fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1/50, Train Loss: 0.5747, Train F1: 0.9571, Val Loss: 0.4262, Val F1: 0.9611\n",
      "Fold 4, Epoch 2/50, Train Loss: 0.4206, Train F1: 0.9665, Val Loss: 0.3819, Val F1: 0.9682\n",
      "Fold 4, Epoch 3/50, Train Loss: 0.3725, Train F1: 0.9703, Val Loss: 0.3390, Val F1: 0.9700\n",
      "Fold 4, Epoch 4/50, Train Loss: 0.3303, Train F1: 0.9737, Val Loss: 0.2983, Val F1: 0.9746\n",
      "Fold 4, Epoch 5/50, Train Loss: 0.3005, Train F1: 0.9688, Val Loss: 0.2887, Val F1: 0.9705\n",
      "Fold 4, Epoch 6/50, Train Loss: 0.2769, Train F1: 0.9706, Val Loss: 0.2544, Val F1: 0.9717\n",
      "Fold 4, Epoch 7/50, Train Loss: 0.2522, Train F1: 0.9768, Val Loss: 0.2379, Val F1: 0.9763\n",
      "Fold 4, Epoch 8/50, Train Loss: 0.2253, Train F1: 0.9756, Val Loss: 0.2070, Val F1: 0.9753\n",
      "Fold 4, Epoch 9/50, Train Loss: 0.2108, Train F1: 0.9759, Val Loss: 0.1883, Val F1: 0.9757\n",
      "Fold 4, Epoch 10/50, Train Loss: 0.1957, Train F1: 0.9768, Val Loss: 0.1872, Val F1: 0.9760\n",
      "Saved overlay image at: overlay_images/fold4_epoch10.png\n",
      "Fold 4, Epoch 11/50, Train Loss: 0.1798, Train F1: 0.9746, Val Loss: 0.1808, Val F1: 0.9748\n",
      "Fold 4, Epoch 12/50, Train Loss: 0.1716, Train F1: 0.9775, Val Loss: 0.1688, Val F1: 0.9764\n",
      "Fold 4, Epoch 13/50, Train Loss: 0.1610, Train F1: 0.9752, Val Loss: 0.1609, Val F1: 0.9739\n",
      "Fold 4, Epoch 14/50, Train Loss: 0.1644, Train F1: 0.9740, Val Loss: 0.1472, Val F1: 0.9740\n",
      "Fold 4, Epoch 15/50, Train Loss: 0.1531, Train F1: 0.9718, Val Loss: 0.1467, Val F1: 0.9717\n",
      "Fold 4, Epoch 16/50, Train Loss: 0.1556, Train F1: 0.9494, Val Loss: 0.2094, Val F1: 0.9512\n",
      "Fold 4, Epoch 17/50, Train Loss: 0.1471, Train F1: 0.9646, Val Loss: 0.1523, Val F1: 0.9663\n",
      "Early stopping triggered in fold 4 at epoch 17\n",
      "Starting fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 1/50, Train Loss: 0.5539, Train F1: 0.9539, Val Loss: 0.4672, Val F1: 0.9545\n",
      "Fold 5, Epoch 2/50, Train Loss: 0.4052, Train F1: 0.9689, Val Loss: 0.3883, Val F1: 0.9671\n",
      "Fold 5, Epoch 3/50, Train Loss: 0.3537, Train F1: 0.9711, Val Loss: 0.3555, Val F1: 0.9689\n",
      "Fold 5, Epoch 4/50, Train Loss: 0.3131, Train F1: 0.9736, Val Loss: 0.3128, Val F1: 0.9718\n",
      "Fold 5, Epoch 5/50, Train Loss: 0.2802, Train F1: 0.9752, Val Loss: 0.2844, Val F1: 0.9728\n",
      "Fold 5, Epoch 6/50, Train Loss: 0.2537, Train F1: 0.9757, Val Loss: 0.2599, Val F1: 0.9735\n",
      "Fold 5, Epoch 7/50, Train Loss: 0.2312, Train F1: 0.9772, Val Loss: 0.2412, Val F1: 0.9747\n",
      "Fold 5, Epoch 8/50, Train Loss: 0.2096, Train F1: 0.9747, Val Loss: 0.2219, Val F1: 0.9720\n",
      "Fold 5, Epoch 9/50, Train Loss: 0.1953, Train F1: 0.9694, Val Loss: 0.2262, Val F1: 0.9668\n",
      "Fold 5, Epoch 10/50, Train Loss: 0.1916, Train F1: 0.9628, Val Loss: 0.2222, Val F1: 0.9615\n",
      "Saved overlay image at: overlay_images/fold5_epoch10.png\n",
      "Fold 5, Epoch 11/50, Train Loss: 0.1881, Train F1: 0.9723, Val Loss: 0.1987, Val F1: 0.9708\n",
      "Fold 5, Epoch 12/50, Train Loss: 0.1656, Train F1: 0.9759, Val Loss: 0.1762, Val F1: 0.9714\n",
      "Early stopping triggered in fold 5 at epoch 12\n",
      "Best model saved with validation F1: 0.9774\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Assume full_dataset, device, UNetResNet101, DiceBCELoss, and joint_transform are defined already.\n",
    "# full_dataset = CustomDataset('denoised_training_set', 'masked_annotations', joint_transform=joint_transform)\n",
    "\n",
    "# Number of folds and training epochs\n",
    "num_folds = 5\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "\n",
    "# Set up KFold splitter\n",
    "indices = np.arange(len(full_dataset))\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Folder to save overlay images (optional)\n",
    "overlay_folder = 'overlay_images'\n",
    "os.makedirs(overlay_folder, exist_ok=True)\n",
    "\n",
    "# Helper function for overlay (as before)\n",
    "def overlay_mask_on_image(image, mask, color=(255, 0, 0), alpha=0.4):\n",
    "    color_mask = np.zeros_like(image)\n",
    "    color_mask[mask == 255] = color\n",
    "    overlay = cv2.addWeighted(image, 1 - alpha, color_mask, alpha, 0)\n",
    "    return overlay\n",
    "\n",
    "# Variables to track the best model overall (based on validation F1 score)\n",
    "best_val_f1_overall = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Begin cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
    "    print(f\"Starting fold {fold+1}/{num_folds}\")\n",
    "    \n",
    "    # Create Subset datasets for current fold\n",
    "    train_subset = Subset(full_dataset, train_idx)\n",
    "    val_subset = Subset(full_dataset, val_idx)\n",
    "    \n",
    "    # Create dataloaders for current fold\n",
    "    train_loader = DataLoader(train_subset, batch_size=10, shuffle=True)\n",
    "    validation_loader = DataLoader(val_subset, batch_size=10, shuffle=False)\n",
    "    \n",
    "    # Reinitialize the model, loss, optimizer, and scheduler for each fold\n",
    "    model = UNetResNet101().to(device)\n",
    "    loss_function = DiceBCEBoundaryLoss(boundary_weight=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    \n",
    "    best_fold_val_f1 = -1.0\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss_epoch = running_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate training set for F1 score\n",
    "        model.eval()\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in train_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                binary_masks = (masks > 0.5).float()\n",
    "                \n",
    "                all_train_preds.append(preds.cpu().numpy().flatten())\n",
    "                all_train_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "        all_train_preds = np.concatenate(all_train_preds)\n",
    "        all_train_targets = np.concatenate(all_train_targets)\n",
    "        train_f1 = f1_score(all_train_targets, all_train_preds)\n",
    "        \n",
    "        # Evaluate validation set\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in validation_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_function(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                preds = (outputs > 0.5).float()\n",
    "                binary_masks = (masks > 0.5).float()\n",
    "                all_val_preds.append(preds.cpu().numpy().flatten())\n",
    "                all_val_targets.append(binary_masks.cpu().numpy().flatten())\n",
    "        all_val_preds = np.concatenate(all_val_preds)\n",
    "        all_val_targets = np.concatenate(all_val_targets)\n",
    "        val_f1 = f1_score(all_val_targets, all_val_preds)\n",
    "        val_loss_epoch = val_loss / len(validation_loader)\n",
    "        \n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss_epoch:.4f}, Train F1: {train_f1:.4f}, \"\n",
    "              f\"Val Loss: {val_loss_epoch:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Adjust learning rate based on validation loss\n",
    "        scheduler.step(val_loss_epoch)\n",
    "        \n",
    "        # If current validation F1 is best so far, update best model state across folds/epochs\n",
    "        if val_f1 > best_val_f1_overall:\n",
    "            best_val_f1_overall = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "        # Early stopping for current fold: check if current validation F1 improved over best_fold_val_f1\n",
    "        if val_f1 > best_fold_val_f1:\n",
    "            best_fold_val_f1 = val_f1\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered in fold {fold+1} at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Optionally, every 10 epochs, save an overlay visualization from validation\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                for images, _ in validation_loader:\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    preds = (outputs > 0.5).float()\n",
    "                    img_tensor = images[0].cpu()  # first image from the batch\n",
    "                    pred_tensor = preds[0].cpu()  # corresponding prediction\n",
    "                    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "                    img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "                    mask_np = pred_tensor.squeeze().numpy()\n",
    "                    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "                    overlay_img = overlay_mask_on_image(img_uint8, mask_uint8, color=(255, 0, 0), alpha=0.4)\n",
    "                    # Convert to BGR for cv2.imwrite\n",
    "                    overlay_bgr = cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)\n",
    "                    overlay_save_path = os.path.join(overlay_folder, f'fold{fold+1}_epoch{epoch+1}.png')\n",
    "                    cv2.imwrite(overlay_save_path, overlay_bgr)\n",
    "                    print(f\"Saved overlay image at: {overlay_save_path}\")\n",
    "                    break  # Process only one batch for overlay visualization\n",
    "\n",
    "# After all folds, save the best model\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'best_unet_resnet101_model.pth')\n",
    "    print(f\"Best model saved with validation F1: {best_val_f1_overall:.4f}\")\n",
    "else:\n",
    "    print(\"No model was saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c257ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = UNetResNet101(n_classes=1).to(device)\n",
    "# dummy_input = torch.rand(1, 3, 224, 224).to(device)  # Adjust input size as necessary\n",
    "# output = model(dummy_input)\n",
    "# print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214323f5",
   "metadata": {},
   "source": [
    "**Running Model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3acc027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for 500_HC.png at output_segmentations/seg_500_HC.png\n",
      "Saved segmentation for 501_HC.png at output_segmentations/seg_501_HC.png\n",
      "Saved segmentation for 502_HC.png at output_segmentations/seg_502_HC.png\n",
      "Saved segmentation for 503_HC.png at output_segmentations/seg_503_HC.png\n",
      "Saved segmentation for 504_HC.png at output_segmentations/seg_504_HC.png\n",
      "Saved segmentation for 505_HC.png at output_segmentations/seg_505_HC.png\n",
      "Saved segmentation for 506_HC.png at output_segmentations/seg_506_HC.png\n",
      "Saved segmentation for 507_2HC.png at output_segmentations/seg_507_2HC.png\n",
      "Saved segmentation for 507_HC.png at output_segmentations/seg_507_HC.png\n",
      "Saved segmentation for 508_HC.png at output_segmentations/seg_508_HC.png\n",
      "Saved segmentation for 509_HC.png at output_segmentations/seg_509_HC.png\n",
      "Saved segmentation for 510_HC.png at output_segmentations/seg_510_HC.png\n",
      "Saved segmentation for 511_HC.png at output_segmentations/seg_511_HC.png\n",
      "Saved segmentation for 512_HC.png at output_segmentations/seg_512_HC.png\n",
      "Saved segmentation for 513_HC.png at output_segmentations/seg_513_HC.png\n",
      "Saved segmentation for 514_HC.png at output_segmentations/seg_514_HC.png\n",
      "Saved segmentation for 515_HC.png at output_segmentations/seg_515_HC.png\n",
      "Saved segmentation for 516_HC.png at output_segmentations/seg_516_HC.png\n",
      "Saved segmentation for 517_HC.png at output_segmentations/seg_517_HC.png\n",
      "Saved segmentation for 518_HC.png at output_segmentations/seg_518_HC.png\n",
      "Saved segmentation for 519_HC.png at output_segmentations/seg_519_HC.png\n",
      "Saved segmentation for 520_HC.png at output_segmentations/seg_520_HC.png\n",
      "Saved segmentation for 521_HC.png at output_segmentations/seg_521_HC.png\n",
      "Saved segmentation for 522_HC.png at output_segmentations/seg_522_HC.png\n",
      "Saved segmentation for 523_HC.png at output_segmentations/seg_523_HC.png\n",
      "Saved segmentation for 524_HC.png at output_segmentations/seg_524_HC.png\n",
      "Saved segmentation for 525_HC.png at output_segmentations/seg_525_HC.png\n",
      "Saved segmentation for 526_2HC.png at output_segmentations/seg_526_2HC.png\n",
      "Saved segmentation for 526_HC.png at output_segmentations/seg_526_HC.png\n",
      "Saved segmentation for 527_HC.png at output_segmentations/seg_527_HC.png\n",
      "Saved segmentation for 528_HC.png at output_segmentations/seg_528_HC.png\n",
      "Saved segmentation for 529_HC.png at output_segmentations/seg_529_HC.png\n",
      "Saved segmentation for 530_HC.png at output_segmentations/seg_530_HC.png\n",
      "Saved segmentation for 531_2HC.png at output_segmentations/seg_531_2HC.png\n",
      "Saved segmentation for 531_HC.png at output_segmentations/seg_531_HC.png\n",
      "Saved segmentation for 532_2HC.png at output_segmentations/seg_532_2HC.png\n",
      "Saved segmentation for 532_HC.png at output_segmentations/seg_532_HC.png\n",
      "Saved segmentation for 533_HC.png at output_segmentations/seg_533_HC.png\n",
      "Saved segmentation for 534_HC.png at output_segmentations/seg_534_HC.png\n",
      "Saved segmentation for 535_HC.png at output_segmentations/seg_535_HC.png\n",
      "Saved segmentation for 536_HC.png at output_segmentations/seg_536_HC.png\n",
      "Saved segmentation for 537_HC.png at output_segmentations/seg_537_HC.png\n",
      "Saved segmentation for 538_HC.png at output_segmentations/seg_538_HC.png\n",
      "Saved segmentation for 539_HC.png at output_segmentations/seg_539_HC.png\n",
      "Saved segmentation for 540_HC.png at output_segmentations/seg_540_HC.png\n",
      "Saved segmentation for 541_2HC.png at output_segmentations/seg_541_2HC.png\n",
      "Saved segmentation for 541_HC.png at output_segmentations/seg_541_HC.png\n",
      "Saved segmentation for 542_HC.png at output_segmentations/seg_542_HC.png\n",
      "Saved segmentation for 543_HC.png at output_segmentations/seg_543_HC.png\n",
      "Saved segmentation for 544_2HC.png at output_segmentations/seg_544_2HC.png\n",
      "Saved segmentation for 544_HC.png at output_segmentations/seg_544_HC.png\n",
      "Saved segmentation for 545_2HC.png at output_segmentations/seg_545_2HC.png\n",
      "Saved segmentation for 545_HC.png at output_segmentations/seg_545_HC.png\n",
      "Saved segmentation for 546_HC.png at output_segmentations/seg_546_HC.png\n",
      "Saved segmentation for 547_2HC.png at output_segmentations/seg_547_2HC.png\n",
      "Saved segmentation for 547_HC.png at output_segmentations/seg_547_HC.png\n",
      "Saved segmentation for 548_HC.png at output_segmentations/seg_548_HC.png\n",
      "Saved segmentation for 549_HC.png at output_segmentations/seg_549_HC.png\n",
      "Saved segmentation for 550_HC.png at output_segmentations/seg_550_HC.png\n",
      "Saved segmentation for 551_HC.png at output_segmentations/seg_551_HC.png\n",
      "Saved segmentation for 552_HC.png at output_segmentations/seg_552_HC.png\n",
      "Saved segmentation for 553_HC.png at output_segmentations/seg_553_HC.png\n",
      "Saved segmentation for 554_HC.png at output_segmentations/seg_554_HC.png\n",
      "Saved segmentation for 555_HC.png at output_segmentations/seg_555_HC.png\n",
      "Saved segmentation for 556_2HC.png at output_segmentations/seg_556_2HC.png\n",
      "Saved segmentation for 556_HC.png at output_segmentations/seg_556_HC.png\n",
      "Saved segmentation for 557_HC.png at output_segmentations/seg_557_HC.png\n",
      "Saved segmentation for 558_HC.png at output_segmentations/seg_558_HC.png\n",
      "Saved segmentation for 559_HC.png at output_segmentations/seg_559_HC.png\n",
      "Saved segmentation for 560_HC.png at output_segmentations/seg_560_HC.png\n",
      "Saved segmentation for 561_2HC.png at output_segmentations/seg_561_2HC.png\n",
      "Saved segmentation for 561_3HC.png at output_segmentations/seg_561_3HC.png\n",
      "Saved segmentation for 561_HC.png at output_segmentations/seg_561_HC.png\n",
      "Saved segmentation for 562_HC.png at output_segmentations/seg_562_HC.png\n",
      "Saved segmentation for 563_HC.png at output_segmentations/seg_563_HC.png\n",
      "Saved segmentation for 564_HC.png at output_segmentations/seg_564_HC.png\n",
      "Saved segmentation for 565_HC.png at output_segmentations/seg_565_HC.png\n",
      "Saved segmentation for 566_2HC.png at output_segmentations/seg_566_2HC.png\n",
      "Saved segmentation for 566_HC.png at output_segmentations/seg_566_HC.png\n",
      "Saved segmentation for 567_2HC.png at output_segmentations/seg_567_2HC.png\n",
      "Saved segmentation for 567_HC.png at output_segmentations/seg_567_HC.png\n",
      "Saved segmentation for 568_HC.png at output_segmentations/seg_568_HC.png\n",
      "Saved segmentation for 569_HC.png at output_segmentations/seg_569_HC.png\n",
      "Saved segmentation for 570_2HC.png at output_segmentations/seg_570_2HC.png\n",
      "Saved segmentation for 570_3HC.png at output_segmentations/seg_570_3HC.png\n",
      "Saved segmentation for 570_HC.png at output_segmentations/seg_570_HC.png\n",
      "Saved segmentation for 571_HC.png at output_segmentations/seg_571_HC.png\n",
      "Saved segmentation for 572_HC.png at output_segmentations/seg_572_HC.png\n",
      "Saved segmentation for 573_HC.png at output_segmentations/seg_573_HC.png\n",
      "Saved segmentation for 574_2HC.png at output_segmentations/seg_574_2HC.png\n",
      "Saved segmentation for 574_HC.png at output_segmentations/seg_574_HC.png\n",
      "Saved segmentation for 575_HC.png at output_segmentations/seg_575_HC.png\n",
      "Saved segmentation for 576_HC.png at output_segmentations/seg_576_HC.png\n",
      "Saved segmentation for 577_HC.png at output_segmentations/seg_577_HC.png\n",
      "Saved segmentation for 578_HC.png at output_segmentations/seg_578_HC.png\n",
      "Saved segmentation for 579_HC.png at output_segmentations/seg_579_HC.png\n",
      "Saved segmentation for 580_HC.png at output_segmentations/seg_580_HC.png\n",
      "Saved segmentation for 581_HC.png at output_segmentations/seg_581_HC.png\n",
      "Saved segmentation for 582_HC.png at output_segmentations/seg_582_HC.png\n",
      "Saved segmentation for 583_2HC.png at output_segmentations/seg_583_2HC.png\n",
      "Saved segmentation for 583_HC.png at output_segmentations/seg_583_HC.png\n",
      "Saved segmentation for 584_HC.png at output_segmentations/seg_584_HC.png\n",
      "Saved segmentation for 585_HC.png at output_segmentations/seg_585_HC.png\n",
      "Saved segmentation for 586_HC.png at output_segmentations/seg_586_HC.png\n",
      "Saved segmentation for 587_2HC.png at output_segmentations/seg_587_2HC.png\n",
      "Saved segmentation for 587_HC.png at output_segmentations/seg_587_HC.png\n",
      "Saved segmentation for 588_2HC.png at output_segmentations/seg_588_2HC.png\n",
      "Saved segmentation for 588_HC.png at output_segmentations/seg_588_HC.png\n",
      "Saved segmentation for 589_HC.png at output_segmentations/seg_589_HC.png\n",
      "Saved segmentation for 590_HC.png at output_segmentations/seg_590_HC.png\n",
      "Saved segmentation for 591_HC.png at output_segmentations/seg_591_HC.png\n",
      "Saved segmentation for 592_2HC.png at output_segmentations/seg_592_2HC.png\n",
      "Saved segmentation for 592_HC.png at output_segmentations/seg_592_HC.png\n",
      "Saved segmentation for 593_HC.png at output_segmentations/seg_593_HC.png\n",
      "Saved segmentation for 594_HC.png at output_segmentations/seg_594_HC.png\n",
      "Saved segmentation for 595_HC.png at output_segmentations/seg_595_HC.png\n",
      "Saved segmentation for 596_HC.png at output_segmentations/seg_596_HC.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for 597_HC.png at output_segmentations/seg_597_HC.png\n",
      "Saved segmentation for 598_HC.png at output_segmentations/seg_598_HC.png\n",
      "Saved segmentation for 599_HC.png at output_segmentations/seg_599_HC.png\n",
      "Saved segmentation for 600_HC.png at output_segmentations/seg_600_HC.png\n",
      "Saved segmentation for 601_HC.png at output_segmentations/seg_601_HC.png\n",
      "Saved segmentation for 602_HC.png at output_segmentations/seg_602_HC.png\n",
      "Saved segmentation for 603_HC.png at output_segmentations/seg_603_HC.png\n",
      "Saved segmentation for 604_HC.png at output_segmentations/seg_604_HC.png\n",
      "Saved segmentation for 605_HC.png at output_segmentations/seg_605_HC.png\n",
      "Saved segmentation for 606_HC.png at output_segmentations/seg_606_HC.png\n",
      "Saved segmentation for 607_HC.png at output_segmentations/seg_607_HC.png\n",
      "Saved segmentation for 608_HC.png at output_segmentations/seg_608_HC.png\n",
      "Saved segmentation for 609_2HC.png at output_segmentations/seg_609_2HC.png\n",
      "Saved segmentation for 609_HC.png at output_segmentations/seg_609_HC.png\n",
      "Saved segmentation for 610_HC.png at output_segmentations/seg_610_HC.png\n",
      "Saved segmentation for 611_HC.png at output_segmentations/seg_611_HC.png\n",
      "Saved segmentation for 612_HC.png at output_segmentations/seg_612_HC.png\n",
      "Saved segmentation for 613_2HC.png at output_segmentations/seg_613_2HC.png\n",
      "Saved segmentation for 613_3HC.png at output_segmentations/seg_613_3HC.png\n",
      "Saved segmentation for 613_HC.png at output_segmentations/seg_613_HC.png\n",
      "Saved segmentation for 614_HC.png at output_segmentations/seg_614_HC.png\n",
      "Saved segmentation for 615_HC.png at output_segmentations/seg_615_HC.png\n",
      "Saved segmentation for 616_HC.png at output_segmentations/seg_616_HC.png\n",
      "Saved segmentation for 617_2HC.png at output_segmentations/seg_617_2HC.png\n",
      "Saved segmentation for 617_HC.png at output_segmentations/seg_617_HC.png\n",
      "Saved segmentation for 618_2HC.png at output_segmentations/seg_618_2HC.png\n",
      "Saved segmentation for 618_HC.png at output_segmentations/seg_618_HC.png\n",
      "Saved segmentation for 619_HC.png at output_segmentations/seg_619_HC.png\n",
      "Saved segmentation for 620_HC.png at output_segmentations/seg_620_HC.png\n",
      "Saved segmentation for 621_HC.png at output_segmentations/seg_621_HC.png\n",
      "Saved segmentation for 622_HC.png at output_segmentations/seg_622_HC.png\n",
      "Saved segmentation for 623_HC.png at output_segmentations/seg_623_HC.png\n",
      "Saved segmentation for 624_HC.png at output_segmentations/seg_624_HC.png\n",
      "Saved segmentation for 625_HC.png at output_segmentations/seg_625_HC.png\n",
      "Saved segmentation for 626_HC.png at output_segmentations/seg_626_HC.png\n",
      "Saved segmentation for 627_HC.png at output_segmentations/seg_627_HC.png\n",
      "Saved segmentation for 628_2HC.png at output_segmentations/seg_628_2HC.png\n",
      "Saved segmentation for 628_HC.png at output_segmentations/seg_628_HC.png\n",
      "Saved segmentation for 629_HC.png at output_segmentations/seg_629_HC.png\n",
      "Saved segmentation for 630_2HC.png at output_segmentations/seg_630_2HC.png\n",
      "Saved segmentation for 630_HC.png at output_segmentations/seg_630_HC.png\n",
      "Saved segmentation for 631_2HC.png at output_segmentations/seg_631_2HC.png\n",
      "Saved segmentation for 631_HC.png at output_segmentations/seg_631_HC.png\n",
      "Saved segmentation for 632_HC.png at output_segmentations/seg_632_HC.png\n",
      "Saved segmentation for 633_HC.png at output_segmentations/seg_633_HC.png\n",
      "Saved segmentation for 634_HC.png at output_segmentations/seg_634_HC.png\n",
      "Saved segmentation for 635_HC.png at output_segmentations/seg_635_HC.png\n",
      "Saved segmentation for 636_HC.png at output_segmentations/seg_636_HC.png\n",
      "Saved segmentation for 637_HC.png at output_segmentations/seg_637_HC.png\n",
      "Saved segmentation for 638_HC.png at output_segmentations/seg_638_HC.png\n",
      "Saved segmentation for 639_2HC.png at output_segmentations/seg_639_2HC.png\n",
      "Saved segmentation for 639_HC.png at output_segmentations/seg_639_HC.png\n",
      "Saved segmentation for 640_HC.png at output_segmentations/seg_640_HC.png\n",
      "Saved segmentation for 641_HC.png at output_segmentations/seg_641_HC.png\n",
      "Saved segmentation for 642_HC.png at output_segmentations/seg_642_HC.png\n",
      "Saved segmentation for 643_HC.png at output_segmentations/seg_643_HC.png\n",
      "Saved segmentation for 644_HC.png at output_segmentations/seg_644_HC.png\n",
      "Saved segmentation for 645_HC.png at output_segmentations/seg_645_HC.png\n",
      "Saved segmentation for 646_HC.png at output_segmentations/seg_646_HC.png\n",
      "Saved segmentation for 647_HC.png at output_segmentations/seg_647_HC.png\n",
      "Saved segmentation for 648_2HC.png at output_segmentations/seg_648_2HC.png\n",
      "Saved segmentation for 648_HC.png at output_segmentations/seg_648_HC.png\n",
      "Saved segmentation for 649_HC.png at output_segmentations/seg_649_HC.png\n",
      "Saved segmentation for 650_HC.png at output_segmentations/seg_650_HC.png\n",
      "Saved segmentation for 651_HC.png at output_segmentations/seg_651_HC.png\n",
      "Saved segmentation for 652_2HC.png at output_segmentations/seg_652_2HC.png\n",
      "Saved segmentation for 652_HC.png at output_segmentations/seg_652_HC.png\n",
      "Saved segmentation for 653_HC.png at output_segmentations/seg_653_HC.png\n",
      "Saved segmentation for 654_HC.png at output_segmentations/seg_654_HC.png\n",
      "Saved segmentation for 655_HC.png at output_segmentations/seg_655_HC.png\n",
      "Saved segmentation for 656_HC.png at output_segmentations/seg_656_HC.png\n",
      "Saved segmentation for 657_2HC.png at output_segmentations/seg_657_2HC.png\n",
      "Saved segmentation for 657_HC.png at output_segmentations/seg_657_HC.png\n",
      "Saved segmentation for 658_HC.png at output_segmentations/seg_658_HC.png\n",
      "Saved segmentation for 659_HC.png at output_segmentations/seg_659_HC.png\n",
      "Saved segmentation for 660_HC.png at output_segmentations/seg_660_HC.png\n",
      "Saved segmentation for 661_HC.png at output_segmentations/seg_661_HC.png\n",
      "Saved segmentation for 662_HC.png at output_segmentations/seg_662_HC.png\n",
      "Saved segmentation for 663_HC.png at output_segmentations/seg_663_HC.png\n",
      "Saved segmentation for 664_HC.png at output_segmentations/seg_664_HC.png\n",
      "Saved segmentation for 665_HC.png at output_segmentations/seg_665_HC.png\n",
      "Saved segmentation for 666_HC.png at output_segmentations/seg_666_HC.png\n",
      "Saved segmentation for 667_HC.png at output_segmentations/seg_667_HC.png\n",
      "Saved segmentation for 668_HC.png at output_segmentations/seg_668_HC.png\n",
      "Saved segmentation for 669_HC.png at output_segmentations/seg_669_HC.png\n",
      "Saved segmentation for 670_HC.png at output_segmentations/seg_670_HC.png\n",
      "Saved segmentation for 671_2HC.png at output_segmentations/seg_671_2HC.png\n",
      "Saved segmentation for 671_3HC.png at output_segmentations/seg_671_3HC.png\n",
      "Saved segmentation for 671_HC.png at output_segmentations/seg_671_HC.png\n",
      "Saved segmentation for 672_HC.png at output_segmentations/seg_672_HC.png\n",
      "Saved segmentation for 673_2HC.png at output_segmentations/seg_673_2HC.png\n",
      "Saved segmentation for 673_HC.png at output_segmentations/seg_673_HC.png\n",
      "Saved segmentation for 674_HC.png at output_segmentations/seg_674_HC.png\n",
      "Saved segmentation for 675_2HC.png at output_segmentations/seg_675_2HC.png\n",
      "Saved segmentation for 675_HC.png at output_segmentations/seg_675_HC.png\n",
      "Saved segmentation for 676_HC.png at output_segmentations/seg_676_HC.png\n",
      "Saved segmentation for 677_2HC.png at output_segmentations/seg_677_2HC.png\n",
      "Saved segmentation for 677_HC.png at output_segmentations/seg_677_HC.png\n",
      "Saved segmentation for 678_HC.png at output_segmentations/seg_678_HC.png\n",
      "Saved segmentation for 679_HC.png at output_segmentations/seg_679_HC.png\n",
      "Saved segmentation for 680_HC.png at output_segmentations/seg_680_HC.png\n",
      "Saved segmentation for 681_HC.png at output_segmentations/seg_681_HC.png\n",
      "Saved segmentation for 682_HC.png at output_segmentations/seg_682_HC.png\n",
      "Saved segmentation for 683_HC.png at output_segmentations/seg_683_HC.png\n",
      "Saved segmentation for 684_HC.png at output_segmentations/seg_684_HC.png\n",
      "Saved segmentation for 685_HC.png at output_segmentations/seg_685_HC.png\n",
      "Saved segmentation for 686_2HC.png at output_segmentations/seg_686_2HC.png\n",
      "Saved segmentation for 686_HC.png at output_segmentations/seg_686_HC.png\n",
      "Saved segmentation for 687_HC.png at output_segmentations/seg_687_HC.png\n",
      "Saved segmentation for 688_HC.png at output_segmentations/seg_688_HC.png\n",
      "Saved segmentation for 689_HC.png at output_segmentations/seg_689_HC.png\n",
      "Saved segmentation for 690_2HC.png at output_segmentations/seg_690_2HC.png\n",
      "Saved segmentation for 690_HC.png at output_segmentations/seg_690_HC.png\n",
      "Saved segmentation for 691_HC.png at output_segmentations/seg_691_HC.png\n",
      "Saved segmentation for 692_2HC.png at output_segmentations/seg_692_2HC.png\n",
      "Saved segmentation for 692_HC.png at output_segmentations/seg_692_HC.png\n",
      "Saved segmentation for 693_HC.png at output_segmentations/seg_693_HC.png\n",
      "Saved segmentation for 694_HC.png at output_segmentations/seg_694_HC.png\n",
      "Saved segmentation for 695_HC.png at output_segmentations/seg_695_HC.png\n",
      "Saved segmentation for 696_HC.png at output_segmentations/seg_696_HC.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved segmentation for 697_HC.png at output_segmentations/seg_697_HC.png\n",
      "Saved segmentation for 698_HC.png at output_segmentations/seg_698_HC.png\n",
      "Saved segmentation for 699_HC.png at output_segmentations/seg_699_HC.png\n",
      "Saved segmentation for 700_HC.png at output_segmentations/seg_700_HC.png\n",
      "Saved segmentation for 701_HC.png at output_segmentations/seg_701_HC.png\n",
      "Saved segmentation for 702_HC.png at output_segmentations/seg_702_HC.png\n",
      "Saved segmentation for 703_HC.png at output_segmentations/seg_703_HC.png\n",
      "Saved segmentation for 704_2HC.png at output_segmentations/seg_704_2HC.png\n",
      "Saved segmentation for 704_HC.png at output_segmentations/seg_704_HC.png\n",
      "Saved segmentation for 705_HC.png at output_segmentations/seg_705_HC.png\n",
      "Saved segmentation for 706_HC.png at output_segmentations/seg_706_HC.png\n",
      "Saved segmentation for 707_HC.png at output_segmentations/seg_707_HC.png\n",
      "Saved segmentation for 708_2HC.png at output_segmentations/seg_708_2HC.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a test dataset class\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir) if x.endswith('.png')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_path\n",
    "\n",
    "# Define deterministic transforms for test data\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),  # Match mask interpolation\n",
    "#     transforms.Lambda(lambda x: TF.adjust_gamma(x, 0.5)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Set the directory where your test images are stored\n",
    "test_dir = 'denoised_test_set' \n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = TestDataset(test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Instantiate your model (assumed defined in the notebook)\n",
    "model = UNetResNet101(n_classes=1).to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('best_unet_resnet101_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Create an output directory for the segmentation results\n",
    "output_dir = 'output_segmentations'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Inference loop: run the model on each test image and save the segmentation mask\n",
    "for image, image_path in test_loader:\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Get the probability map from the model (sigmoid already applied)\n",
    "        # Threshold the probability map to obtain a binary segmentation mask\n",
    "        seg_mask = (output > 0.5).float()\n",
    "    \n",
    "    # Convert tensor to NumPy array and scale to 0-255 for visualization/saving\n",
    "    seg_mask_np = seg_mask.cpu().numpy().squeeze() * 255\n",
    "\n",
    "    # Generate output filename based on input image name\n",
    "    base_name = os.path.basename(image_path[0])\n",
    "    output_filename = os.path.join(output_dir, f\"seg_{base_name}\")\n",
    "    \n",
    "    # Save the segmentation mask image using OpenCV\n",
    "    cv2.imwrite(output_filename, seg_mask_np.astype('uint8'))\n",
    "    \n",
    "    # Optionally, display the segmentation mask using matplotlib\n",
    "#     plt.imshow(seg_mask_np, cmap='gray')\n",
    "#     plt.title(f\"Segmentation: {base_name}\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    print(f\"Saved segmentation for {base_name} at {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b490cd5",
   "metadata": {},
   "source": [
    "**DSC (Dice Similarity Coefficient) with filled annotations as ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4328eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Dice Similarity Coefficient (DSC): 0.9795\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "def dice_coefficient(pred, true, smooth=1e-8):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient between two binary masks.\n",
    "    \n",
    "    Args:\n",
    "        pred (np.array): Predicted binary mask (0 or 1).\n",
    "        true (np.array): Ground truth binary mask (0 or 1).\n",
    "        smooth (float): A small value to avoid division by zero.\n",
    "        \n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred = pred.flatten().astype(np.float32)\n",
    "    true = true.flatten().astype(np.float32)\n",
    "    intersection = np.sum(pred * true)\n",
    "    dice = (2.0 * intersection + smooth) / (np.sum(pred) + np.sum(true) + smooth)\n",
    "    return dice\n",
    "\n",
    "def hausdorff_distance(pred, true):\n",
    "    \"\"\"\n",
    "    Computes the Hausdorff distance between the boundaries of two binary masks.\n",
    "    Extracts the boundaries using Canny edge detection, then computes the symmetric directed Hausdorff distance.\n",
    "    \n",
    "    Args:\n",
    "        pred (np.array): Predicted binary mask (with values 0 or 255, uint8).\n",
    "        true (np.array): Ground truth binary mask (with values 0 or 255, uint8).\n",
    "    \n",
    "    Returns:\n",
    "        float: Hausdorff distance.\n",
    "    \"\"\"\n",
    "    pred_edges = cv2.Canny(pred, 30, 100)\n",
    "    true_edges = cv2.Canny(true, 30, 100)\n",
    "    \n",
    "    pred_points = np.column_stack(np.where(pred_edges != 0))\n",
    "    true_points = np.column_stack(np.where(true_edges != 0))\n",
    "    \n",
    "    if pred_points.size == 0 or true_points.size == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    d1 = directed_hausdorff(pred_points, true_points)[0]\n",
    "    d2 = directed_hausdorff(true_points, pred_points)[0]\n",
    "    return max(d1, d2)\n",
    "\n",
    "# Directories for predicted segmentation masks and ground-truth masked annotations\n",
    "pred_dir = 'output_segmentations'\n",
    "gt_dir = 'masked_annotations_test'  # File name style: \"500_HC_Annotation.png\"\n",
    "\n",
    "# Get a sorted list of predicted files (assuming .png format)\n",
    "pred_files = sorted([f for f in os.listdir(pred_dir) if f.endswith('.png')])\n",
    "\n",
    "dice_scores = []\n",
    "hd_scores = []\n",
    "\n",
    "for pred_file in pred_files:\n",
    "    pred_path = os.path.join(pred_dir, pred_file)\n",
    "    \n",
    "    # Extract the base name by removing the \"seg_\" prefix and \".png\" extension\n",
    "    base_name = pred_file.replace(\"seg_\", \"\").replace(\".png\", \"\")  # e.g., \"500_HC\"\n",
    "    # Form ground truth filename by appending \"_Annotation.png\"\n",
    "    gt_file = base_name + \"_Annotation.png\"  # e.g., \"500_HC_Annotation.png\"\n",
    "    gt_path = os.path.join(gt_dir, gt_file)\n",
    "    \n",
    "    if not os.path.exists(gt_path):\n",
    "        print(f\"Ground truth file {gt_file} not found for {pred_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Load predicted mask and ground truth mask in grayscale\n",
    "    pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "    gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if pred_mask is None or gt_mask is None:\n",
    "        print(f\"Error loading {pred_file} or {gt_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Resize ground truth mask to match predicted mask size using nearest neighbor interpolation\n",
    "    gt_mask_resized = cv2.resize(gt_mask, (pred_mask.shape[1], pred_mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Convert masks to binary: threshold at 127 (if values are 0 and 255)\n",
    "    _, pred_mask_bin = cv2.threshold(pred_mask, 127, 1, cv2.THRESH_BINARY)\n",
    "    _, gt_mask_bin   = cv2.threshold(gt_mask_resized, 127, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Calculate Dice (DSC)\n",
    "    dice = dice_coefficient(pred_mask_bin, gt_mask_bin)\n",
    "    dice_scores.append(dice)\n",
    "    \n",
    "    # For Hausdorff distance, convert binary masks back to uint8 with values 0 or 255.\n",
    "#     pred_mask_uint8 = (pred_mask_bin * 255).astype(np.uint8)\n",
    "#     gt_mask_uint8   = (gt_mask_bin * 255).astype(np.uint8)\n",
    "#     hd = hausdorff_distance(pred_mask_uint8, gt_mask_uint8)\n",
    "#     hd_scores.append(hd)\n",
    "\n",
    "# Compute overall average metrics, ignoring NaN values in Hausdorff distance if any.\n",
    "mean_dice = np.nanmean(dice_scores)\n",
    "# mean_hd = np.nanmean(hd_scores)\n",
    "\n",
    "print(\"Mean Dice Similarity Coefficient (DSC): {:.4f}\".format(mean_dice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e52edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst HD case: seg_613_2HC.png (HD = 9.90 mm)\n"
     ]
    }
   ],
   "source": [
    "worst_idx = np.argmax(hd_scores)  # Index of image with max HD\n",
    "worst_file = pred_files[worst_idx]\n",
    "print(f\"Worst HD case: {worst_file} (HD = {hd_scores[worst_idx]:.2f} mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "385ceaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HD stats: Median = 4.12 mm, 95th %ile = 7.07 mm\n"
     ]
    }
   ],
   "source": [
    "print(f\"HD stats: Median = {np.median(hd_scores):.2f} mm, 95th %ile = {np.percentile(hd_scores, 95):.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff6f929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_outlier(pred_dir, gt_dir, worst_file):\n",
    "    base_name = worst_file.replace(\"seg_\", \"\").replace(\".png\", \"\")\n",
    "    gt_file = base_name + \"_Annotation.png\"\n",
    "\n",
    "    pred_mask = cv2.imread(os.path.join(pred_dir, worst_file), cv2.IMREAD_GRAYSCALE)\n",
    "    gt_mask = cv2.imread(os.path.join(gt_dir, gt_file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(121), plt.imshow(pred_mask, cmap='gray'), plt.title(\"Predicted\")\n",
    "    plt.subplot(122), plt.imshow(gt_mask, cmap='gray'), plt.title(\"Ground Truth\")\n",
    "    plt.suptitle(f\"Worst HD Case: {worst_file} (HD = 10.30 mm)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_outlier(pred_dir, gt_dir, \"seg_704_2HC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb148a",
   "metadata": {},
   "source": [
    "**Morphological Opening and Closing + Canny edge Detector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3a94443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved edge image for: seg_608_HC.png\n",
      "Processed and saved edge image for: seg_646_HC.png\n",
      "Processed and saved edge image for: seg_517_HC.png\n",
      "Processed and saved edge image for: seg_631_HC.png\n",
      "Processed and saved edge image for: seg_669_HC.png\n",
      "Processed and saved edge image for: seg_676_HC.png\n",
      "Processed and saved edge image for: seg_665_HC.png\n",
      "Processed and saved edge image for: seg_648_2HC.png\n",
      "Processed and saved edge image for: seg_639_HC.png\n",
      "Processed and saved edge image for: seg_592_HC.png\n",
      "Processed and saved edge image for: seg_570_3HC.png\n",
      "Processed and saved edge image for: seg_528_HC.png\n",
      "Processed and saved edge image for: seg_560_HC.png\n",
      "Processed and saved edge image for: seg_593_HC.png\n",
      "Processed and saved edge image for: seg_520_HC.png\n",
      "Processed and saved edge image for: seg_584_HC.png\n",
      "Processed and saved edge image for: seg_610_HC.png\n",
      "Processed and saved edge image for: seg_567_2HC.png\n",
      "Processed and saved edge image for: seg_707_HC.png\n",
      "Processed and saved edge image for: seg_618_2HC.png\n",
      "Processed and saved edge image for: seg_550_HC.png\n",
      "Processed and saved edge image for: seg_607_HC.png\n",
      "Processed and saved edge image for: seg_570_HC.png\n",
      "Processed and saved edge image for: seg_621_HC.png\n",
      "Processed and saved edge image for: seg_619_HC.png\n",
      "Processed and saved edge image for: seg_543_HC.png\n",
      "Processed and saved edge image for: seg_535_HC.png\n",
      "Processed and saved edge image for: seg_501_HC.png\n",
      "Processed and saved edge image for: seg_546_HC.png\n",
      "Processed and saved edge image for: seg_704_2HC.png\n",
      "Processed and saved edge image for: seg_583_2HC.png\n",
      "Processed and saved edge image for: seg_529_HC.png\n",
      "Processed and saved edge image for: seg_613_3HC.png\n",
      "Processed and saved edge image for: seg_611_HC.png\n",
      "Processed and saved edge image for: seg_507_2HC.png\n",
      "Processed and saved edge image for: seg_632_HC.png\n",
      "Processed and saved edge image for: seg_605_HC.png\n",
      "Processed and saved edge image for: seg_628_2HC.png\n",
      "Processed and saved edge image for: seg_557_HC.png\n",
      "Processed and saved edge image for: seg_523_HC.png\n",
      "Processed and saved edge image for: seg_692_HC.png\n",
      "Processed and saved edge image for: seg_647_HC.png\n",
      "Processed and saved edge image for: seg_699_HC.png\n",
      "Processed and saved edge image for: seg_524_HC.png\n",
      "Processed and saved edge image for: seg_532_2HC.png\n",
      "Processed and saved edge image for: seg_627_HC.png\n",
      "Processed and saved edge image for: seg_601_HC.png\n",
      "Processed and saved edge image for: seg_675_2HC.png\n",
      "Processed and saved edge image for: seg_672_HC.png\n",
      "Processed and saved edge image for: seg_652_HC.png\n",
      "Processed and saved edge image for: seg_570_2HC.png\n",
      "Processed and saved edge image for: seg_589_HC.png\n",
      "Processed and saved edge image for: seg_583_HC.png\n",
      "Processed and saved edge image for: seg_515_HC.png\n",
      "Processed and saved edge image for: seg_641_HC.png\n",
      "Processed and saved edge image for: seg_574_2HC.png\n",
      "Processed and saved edge image for: seg_671_HC.png\n",
      "Processed and saved edge image for: seg_582_HC.png\n",
      "Processed and saved edge image for: seg_539_HC.png\n",
      "Processed and saved edge image for: seg_609_HC.png\n",
      "Processed and saved edge image for: seg_564_HC.png\n",
      "Processed and saved edge image for: seg_588_2HC.png\n",
      "Processed and saved edge image for: seg_667_HC.png\n",
      "Processed and saved edge image for: seg_697_HC.png\n",
      "Processed and saved edge image for: seg_670_HC.png\n",
      "Processed and saved edge image for: seg_633_HC.png\n",
      "Processed and saved edge image for: seg_636_HC.png\n",
      "Processed and saved edge image for: seg_506_HC.png\n",
      "Processed and saved edge image for: seg_695_HC.png\n",
      "Processed and saved edge image for: seg_513_HC.png\n",
      "Processed and saved edge image for: seg_663_HC.png\n",
      "Processed and saved edge image for: seg_657_2HC.png\n",
      "Processed and saved edge image for: seg_614_HC.png\n",
      "Processed and saved edge image for: seg_689_HC.png\n",
      "Processed and saved edge image for: seg_622_HC.png\n",
      "Processed and saved edge image for: seg_590_HC.png\n",
      "Processed and saved edge image for: seg_652_2HC.png\n",
      "Processed and saved edge image for: seg_626_HC.png\n",
      "Processed and saved edge image for: seg_700_HC.png\n",
      "Processed and saved edge image for: seg_683_HC.png\n",
      "Processed and saved edge image for: seg_588_HC.png\n",
      "Processed and saved edge image for: seg_685_HC.png\n",
      "Processed and saved edge image for: seg_521_HC.png\n",
      "Processed and saved edge image for: seg_516_HC.png\n",
      "Processed and saved edge image for: seg_591_HC.png\n",
      "Processed and saved edge image for: seg_544_2HC.png\n",
      "Processed and saved edge image for: seg_531_HC.png\n",
      "Processed and saved edge image for: seg_502_HC.png\n",
      "Processed and saved edge image for: seg_612_HC.png\n",
      "Processed and saved edge image for: seg_575_HC.png\n",
      "Processed and saved edge image for: seg_500_HC.png\n",
      "Processed and saved edge image for: seg_703_HC.png\n",
      "Processed and saved edge image for: seg_511_HC.png\n",
      "Processed and saved edge image for: seg_545_2HC.png\n",
      "Processed and saved edge image for: seg_634_HC.png\n",
      "Processed and saved edge image for: seg_541_2HC.png\n",
      "Processed and saved edge image for: seg_514_HC.png\n",
      "Processed and saved edge image for: seg_505_HC.png\n",
      "Processed and saved edge image for: seg_657_HC.png\n",
      "Processed and saved edge image for: seg_553_HC.png\n",
      "Processed and saved edge image for: seg_613_HC.png\n",
      "Processed and saved edge image for: seg_536_HC.png\n",
      "Processed and saved edge image for: seg_702_HC.png\n",
      "Processed and saved edge image for: seg_540_HC.png\n",
      "Processed and saved edge image for: seg_538_HC.png\n",
      "Processed and saved edge image for: seg_508_HC.png\n",
      "Processed and saved edge image for: seg_507_HC.png\n",
      "Processed and saved edge image for: seg_573_HC.png\n",
      "Processed and saved edge image for: seg_518_HC.png\n",
      "Processed and saved edge image for: seg_547_HC.png\n",
      "Processed and saved edge image for: seg_684_HC.png\n",
      "Processed and saved edge image for: seg_525_HC.png\n",
      "Processed and saved edge image for: seg_522_HC.png\n",
      "Processed and saved edge image for: seg_625_HC.png\n",
      "Processed and saved edge image for: seg_561_2HC.png\n",
      "Processed and saved edge image for: seg_551_HC.png\n",
      "Processed and saved edge image for: seg_547_2HC.png\n",
      "Processed and saved edge image for: seg_677_HC.png\n",
      "Processed and saved edge image for: seg_569_HC.png\n",
      "Processed and saved edge image for: seg_701_HC.png\n",
      "Processed and saved edge image for: seg_534_HC.png\n",
      "Processed and saved edge image for: seg_671_3HC.png\n",
      "Processed and saved edge image for: seg_561_HC.png\n",
      "Processed and saved edge image for: seg_552_HC.png\n",
      "Processed and saved edge image for: seg_563_HC.png\n",
      "Processed and saved edge image for: seg_668_HC.png\n",
      "Processed and saved edge image for: seg_690_HC.png\n",
      "Processed and saved edge image for: seg_659_HC.png\n",
      "Processed and saved edge image for: seg_642_HC.png\n",
      "Processed and saved edge image for: seg_592_2HC.png\n",
      "Processed and saved edge image for: seg_597_HC.png\n",
      "Processed and saved edge image for: seg_504_HC.png\n",
      "Processed and saved edge image for: seg_696_HC.png\n",
      "Processed and saved edge image for: seg_690_2HC.png\n",
      "Processed and saved edge image for: seg_519_HC.png\n",
      "Processed and saved edge image for: seg_526_HC.png\n",
      "Processed and saved edge image for: seg_576_HC.png\n",
      "Processed and saved edge image for: seg_666_HC.png\n",
      "Processed and saved edge image for: seg_509_HC.png\n",
      "Processed and saved edge image for: seg_628_HC.png\n",
      "Processed and saved edge image for: seg_558_HC.png\n",
      "Processed and saved edge image for: seg_698_HC.png\n",
      "Processed and saved edge image for: seg_600_HC.png\n",
      "Processed and saved edge image for: seg_602_HC.png\n",
      "Processed and saved edge image for: seg_649_HC.png\n",
      "Processed and saved edge image for: seg_568_HC.png\n",
      "Processed and saved edge image for: seg_662_HC.png\n",
      "Processed and saved edge image for: seg_656_HC.png\n",
      "Processed and saved edge image for: seg_512_HC.png\n",
      "Processed and saved edge image for: seg_655_HC.png\n",
      "Processed and saved edge image for: seg_541_HC.png\n",
      "Processed and saved edge image for: seg_542_HC.png\n",
      "Processed and saved edge image for: seg_679_HC.png\n",
      "Processed and saved edge image for: seg_620_HC.png\n",
      "Processed and saved edge image for: seg_673_HC.png\n",
      "Processed and saved edge image for: seg_549_HC.png\n",
      "Processed and saved edge image for: seg_545_HC.png\n",
      "Processed and saved edge image for: seg_567_HC.png\n",
      "Processed and saved edge image for: seg_660_HC.png\n",
      "Processed and saved edge image for: seg_648_HC.png\n",
      "Processed and saved edge image for: seg_675_HC.png\n",
      "Processed and saved edge image for: seg_618_HC.png\n",
      "Processed and saved edge image for: seg_677_2HC.png\n",
      "Processed and saved edge image for: seg_671_2HC.png\n",
      "Processed and saved edge image for: seg_548_HC.png\n",
      "Processed and saved edge image for: seg_686_HC.png\n",
      "Processed and saved edge image for: seg_595_HC.png\n",
      "Processed and saved edge image for: seg_623_HC.png\n",
      "Processed and saved edge image for: seg_693_HC.png\n",
      "Processed and saved edge image for: seg_606_HC.png\n",
      "Processed and saved edge image for: seg_596_HC.png\n",
      "Processed and saved edge image for: seg_682_HC.png\n",
      "Processed and saved edge image for: seg_638_HC.png\n",
      "Processed and saved edge image for: seg_617_HC.png\n",
      "Processed and saved edge image for: seg_664_HC.png\n",
      "Processed and saved edge image for: seg_615_HC.png\n",
      "Processed and saved edge image for: seg_527_HC.png\n",
      "Processed and saved edge image for: seg_688_HC.png\n",
      "Processed and saved edge image for: seg_617_2HC.png\n",
      "Processed and saved edge image for: seg_705_HC.png\n",
      "Processed and saved edge image for: seg_565_HC.png\n",
      "Processed and saved edge image for: seg_706_HC.png\n",
      "Processed and saved edge image for: seg_640_HC.png\n",
      "Processed and saved edge image for: seg_637_HC.png\n",
      "Processed and saved edge image for: seg_630_2HC.png\n",
      "Processed and saved edge image for: seg_674_HC.png\n",
      "Processed and saved edge image for: seg_643_HC.png\n",
      "Processed and saved edge image for: seg_554_HC.png\n",
      "Processed and saved edge image for: seg_609_2HC.png\n",
      "Processed and saved edge image for: seg_650_HC.png\n",
      "Processed and saved edge image for: seg_691_HC.png\n",
      "Processed and saved edge image for: seg_585_HC.png\n",
      "Processed and saved edge image for: seg_651_HC.png\n",
      "Processed and saved edge image for: seg_653_HC.png\n",
      "Processed and saved edge image for: seg_556_HC.png\n",
      "Processed and saved edge image for: seg_599_HC.png\n",
      "Processed and saved edge image for: seg_624_HC.png\n",
      "Processed and saved edge image for: seg_681_HC.png\n",
      "Processed and saved edge image for: seg_678_HC.png\n",
      "Processed and saved edge image for: seg_594_HC.png\n",
      "Processed and saved edge image for: seg_686_2HC.png\n",
      "Processed and saved edge image for: seg_645_HC.png\n",
      "Processed and saved edge image for: seg_687_HC.png\n",
      "Processed and saved edge image for: seg_692_2HC.png\n",
      "Processed and saved edge image for: seg_578_HC.png\n",
      "Processed and saved edge image for: seg_580_HC.png\n",
      "Processed and saved edge image for: seg_708_2HC.png\n",
      "Processed and saved edge image for: seg_532_HC.png\n",
      "Processed and saved edge image for: seg_680_HC.png\n",
      "Processed and saved edge image for: seg_559_HC.png\n",
      "Processed and saved edge image for: seg_562_HC.png\n",
      "Processed and saved edge image for: seg_587_2HC.png\n",
      "Processed and saved edge image for: seg_639_2HC.png\n",
      "Processed and saved edge image for: seg_537_HC.png\n",
      "Processed and saved edge image for: seg_613_2HC.png\n",
      "Processed and saved edge image for: seg_704_HC.png\n",
      "Processed and saved edge image for: seg_635_HC.png\n",
      "Processed and saved edge image for: seg_630_HC.png\n",
      "Processed and saved edge image for: seg_533_HC.png\n",
      "Processed and saved edge image for: seg_694_HC.png\n",
      "Processed and saved edge image for: seg_577_HC.png\n",
      "Processed and saved edge image for: seg_604_HC.png\n",
      "Processed and saved edge image for: seg_566_2HC.png\n",
      "Processed and saved edge image for: seg_581_HC.png\n",
      "Processed and saved edge image for: seg_644_HC.png\n",
      "Processed and saved edge image for: seg_586_HC.png\n",
      "Processed and saved edge image for: seg_510_HC.png\n",
      "Processed and saved edge image for: seg_654_HC.png\n",
      "Processed and saved edge image for: seg_673_2HC.png\n",
      "Processed and saved edge image for: seg_531_2HC.png\n",
      "Processed and saved edge image for: seg_571_HC.png\n",
      "Processed and saved edge image for: seg_598_HC.png\n",
      "Processed and saved edge image for: seg_629_HC.png\n",
      "Processed and saved edge image for: seg_603_HC.png\n",
      "Processed and saved edge image for: seg_579_HC.png\n",
      "Processed and saved edge image for: seg_555_HC.png\n",
      "Processed and saved edge image for: seg_561_3HC.png\n",
      "Processed and saved edge image for: seg_616_HC.png\n",
      "Processed and saved edge image for: seg_587_HC.png\n",
      "Processed and saved edge image for: seg_658_HC.png\n",
      "Processed and saved edge image for: seg_503_HC.png\n",
      "Processed and saved edge image for: seg_566_HC.png\n",
      "Processed and saved edge image for: seg_526_2HC.png\n",
      "Processed and saved edge image for: seg_544_HC.png\n",
      "Processed and saved edge image for: seg_574_HC.png\n",
      "Processed and saved edge image for: seg_661_HC.png\n",
      "Processed and saved edge image for: seg_556_2HC.png\n",
      "Processed and saved edge image for: seg_631_2HC.png\n",
      "Processed and saved edge image for: seg_572_HC.png\n",
      "Processed and saved edge image for: seg_530_HC.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = 'output_segmentations'\n",
    "output_folder = 'output_edges'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define a structuring element for the morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Loop over all segmented images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        # Read the segmentation image in grayscale\n",
    "        seg_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Apply morphological opening (erosion followed by dilation) to remove small artifacts\n",
    "        opened = cv2.morphologyEx(seg_img, cv2.MORPH_OPEN, kernel)\n",
    "        # Then apply morphological closing (dilation followed by erosion) to fill small holes\n",
    "        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Apply Canny edge detector to extract the contour\n",
    "        # Adjust thresholds as necessary (here, 50 and 150 are example values)\n",
    "        edges = cv2.Canny(closed, 50, 150)\n",
    "        \n",
    "        # Save the edge-detected image\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, edges)\n",
    "        \n",
    "        # Optionally, display the original segmentation, post-morphology, and edge image side by side\n",
    "#         plt.figure(figsize=(12, 4))\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(seg_img, cmap='gray')\n",
    "#         plt.title('Original Segmentation')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(closed, cmap='gray')\n",
    "#         plt.title('After Morphological Ops')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(edges, cmap='gray')\n",
    "#         plt.title('Canny Edges')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.show()\n",
    "        print(f\"Processed and saved edge image for: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa71db59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ellipse parameters for 250 images to 'ellipse_parameters2.csv'.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import csv\n",
    "# import math\n",
    "# from PIL import Image\n",
    "\n",
    "# # -------------------------------\n",
    "# # Parameters\n",
    "# # -------------------------------\n",
    "# resized_dim    = 256                          # dimension used in preprocessing/inference\n",
    "# seg_folder     = 'output_segmentations'       # folder with cleaned segmentation masks\n",
    "# orig_folder    = 'denoised_test_set'         # folder with original test images\n",
    "# pixel_csv      = 'test_set_pixel_size.csv'    # CSV mapping filename to pixel size (mm)\n",
    "# output_csv     = 'ellipse_parameters2.csv'     # output CSV for ellipse params\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 1: Load pixel size info\n",
    "# # -------------------------------\n",
    "# pixel_size_dict = {}\n",
    "# with open(pixel_csv, mode='r') as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     for row in reader:\n",
    "#         pixel_size_dict[row['filename']] = float(row['pixel size(mm)'])\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 2: Prepare CSV output\n",
    "# # -------------------------------\n",
    "# header = [\"filename\",\n",
    "#           \"center_x_mm\",\n",
    "#           \"center_y_mm\",\n",
    "#           \"semi_axes_a_mm\",\n",
    "#           \"semi_axes_b_mm\",\n",
    "#           \"angle_rad\"]\n",
    "# rows = []\n",
    "\n",
    "# # Structuring element for morphology\n",
    "# kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 3: Loop over segmentation masks\n",
    "# # -------------------------------\n",
    "# for fname in sorted(os.listdir(seg_folder)):\n",
    "#     if not fname.lower().endswith('.png'):\n",
    "#         continue\n",
    "\n",
    "#     seg_path = os.path.join(seg_folder, fname)\n",
    "#     seg_img = cv2.imread(seg_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     if seg_img is None:\n",
    "#         print(f\"Failed to load {fname}\")\n",
    "#         continue\n",
    "\n",
    "#     # Morphological open & close\n",
    "#     opened = cv2.morphologyEx(seg_img, cv2.MORPH_OPEN, kernel)\n",
    "#     closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "#     smooth = cv2.GaussianBlur(closed, (5,5), sigmaX=1)\n",
    "#     _, closed = cv2.threshold(smooth, 127, 255, cv2.THRESH_BINARY)\n",
    "#     # then findContours(closed) → fitEllipse(...)\n",
    "\n",
    "\n",
    "#     # Find contours on the cleaned mask\n",
    "#     contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "#     if not contours:\n",
    "#         print(f\"No contours in {fname}\")\n",
    "#         continue\n",
    "\n",
    "#     cnt = max(contours, key=cv2.contourArea)\n",
    "#     if len(cnt) < 5:\n",
    "#         print(f\"Too few contour points in {fname}\")\n",
    "#         continue\n",
    "\n",
    "    \n",
    "#     # Fit ellipse\n",
    "#     ellipse = cv2.fitEllipse(cnt)\n",
    "#     (cx, cy), (full_a, full_b), angle_deg = ellipse\n",
    "\n",
    "#     # Convert axes to semi-axes in resized space\n",
    "#     semi_a_resized = full_a / 2.0\n",
    "#     semi_b_resized = full_b / 2.0\n",
    "\n",
    "#     # -------------------------------\n",
    "#     # Step 4: Map back to original image size\n",
    "#     # -------------------------------\n",
    "#     base_fn = fname.replace(\"seg_\", \"\", 1)\n",
    "#     orig_path = os.path.join(orig_folder, base_fn)\n",
    "#     if not os.path.exists(orig_path):\n",
    "#         print(f\"Original image missing: {base_fn}\")\n",
    "#         continue\n",
    "\n",
    "#     with Image.open(orig_path) as im:\n",
    "#         orig_w, orig_h = im.size\n",
    "\n",
    "#     scale_x = orig_w / float(resized_dim)\n",
    "#     scale_y = orig_h / float(resized_dim)\n",
    "\n",
    "#     cx_orig = cx * scale_x\n",
    "#     cy_orig = cy * scale_y\n",
    "#     a_orig  = semi_a_resized * scale_x\n",
    "#     b_orig  = semi_b_resized * scale_y\n",
    "\n",
    "#     # -------------------------------\n",
    "#     # Step 5: Convert to millimeters\n",
    "#     # -------------------------------\n",
    "#     if base_fn not in pixel_size_dict:\n",
    "#         print(f\"Missing pixel size for {base_fn}\")\n",
    "#         continue\n",
    "#     px2mm = pixel_size_dict[base_fn]\n",
    "\n",
    "#     cx_mm = cx_orig * px2mm\n",
    "#     cy_mm = cy_orig * px2mm\n",
    "#     a_mm  = a_orig  * px2mm\n",
    "#     b_mm  = b_orig  * px2mm\n",
    "#     angle_rad = math.radians(angle_deg)\n",
    "\n",
    "#     rows.append([base_fn, cx_mm, cy_mm, a_mm, b_mm, angle_rad])\n",
    "\n",
    "# # -------------------------------\n",
    "# # Step 6: Write CSV\n",
    "# # -------------------------------\n",
    "# with open(output_csv, mode='w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerows(rows)\n",
    "\n",
    "# print(f\"Saved ellipse parameters for {len(rows)} images to '{output_csv}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587634d",
   "metadata": {},
   "source": [
    "**Morphological Opening and Closing + Canny edge Detector on filled annotations for MHD (Mean Hausdorff Distance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a4a267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved edge image for: 531_2HC_Annotation.png\n",
      "Processed and saved edge image for: 586_HC_Annotation.png\n",
      "Processed and saved edge image for: 669_HC_Annotation.png\n",
      "Processed and saved edge image for: 587_HC_Annotation.png\n",
      "Processed and saved edge image for: 695_HC_Annotation.png\n",
      "Processed and saved edge image for: 516_HC_Annotation.png\n",
      "Processed and saved edge image for: 569_HC_Annotation.png\n",
      "Processed and saved edge image for: 520_HC_Annotation.png\n",
      "Processed and saved edge image for: 621_HC_Annotation.png\n",
      "Processed and saved edge image for: 617_2HC_Annotation.png\n",
      "Processed and saved edge image for: 591_HC_Annotation.png\n",
      "Processed and saved edge image for: 653_HC_Annotation.png\n",
      "Processed and saved edge image for: 566_2HC_Annotation.png\n",
      "Processed and saved edge image for: 506_HC_Annotation.png\n",
      "Processed and saved edge image for: 577_HC_Annotation.png\n",
      "Processed and saved edge image for: 613_3HC_Annotation.png\n",
      "Processed and saved edge image for: 570_HC_Annotation.png\n",
      "Processed and saved edge image for: 640_HC_Annotation.png\n",
      "Processed and saved edge image for: 618_HC_Annotation.png\n",
      "Processed and saved edge image for: 592_2HC_Annotation.png\n",
      "Processed and saved edge image for: 609_2HC_Annotation.png\n",
      "Processed and saved edge image for: 561_3HC_Annotation.png\n",
      "Processed and saved edge image for: 676_HC_Annotation.png\n",
      "Processed and saved edge image for: 583_2HC_Annotation.png\n",
      "Processed and saved edge image for: 518_HC_Annotation.png\n",
      "Processed and saved edge image for: 530_HC_Annotation.png\n",
      "Processed and saved edge image for: 689_HC_Annotation.png\n",
      "Processed and saved edge image for: 633_HC_Annotation.png\n",
      "Processed and saved edge image for: 628_2HC_Annotation.png\n",
      "Processed and saved edge image for: 521_HC_Annotation.png\n",
      "Processed and saved edge image for: 515_HC_Annotation.png\n",
      "Processed and saved edge image for: 544_2HC_Annotation.png\n",
      "Processed and saved edge image for: 500_HC_Annotation.png\n",
      "Processed and saved edge image for: 580_HC_Annotation.png\n",
      "Processed and saved edge image for: 574_2HC_Annotation.png\n",
      "Processed and saved edge image for: 584_HC_Annotation.png\n",
      "Processed and saved edge image for: 692_2HC_Annotation.png\n",
      "Processed and saved edge image for: 563_HC_Annotation.png\n",
      "Processed and saved edge image for: 704_2HC_Annotation.png\n",
      "Processed and saved edge image for: 652_HC_Annotation.png\n",
      "Processed and saved edge image for: 551_HC_Annotation.png\n",
      "Processed and saved edge image for: 610_HC_Annotation.png\n",
      "Processed and saved edge image for: 503_HC_Annotation.png\n",
      "Processed and saved edge image for: 642_HC_Annotation.png\n",
      "Processed and saved edge image for: 572_HC_Annotation.png\n",
      "Processed and saved edge image for: 512_HC_Annotation.png\n",
      "Processed and saved edge image for: 573_HC_Annotation.png\n",
      "Processed and saved edge image for: 687_HC_Annotation.png\n",
      "Processed and saved edge image for: 605_HC_Annotation.png\n",
      "Processed and saved edge image for: 632_HC_Annotation.png\n",
      "Processed and saved edge image for: 549_HC_Annotation.png\n",
      "Processed and saved edge image for: 680_HC_Annotation.png\n",
      "Processed and saved edge image for: 672_HC_Annotation.png\n",
      "Processed and saved edge image for: 649_HC_Annotation.png\n",
      "Processed and saved edge image for: 647_HC_Annotation.png\n",
      "Processed and saved edge image for: 659_HC_Annotation.png\n",
      "Processed and saved edge image for: 671_HC_Annotation.png\n",
      "Processed and saved edge image for: 703_HC_Annotation.png\n",
      "Processed and saved edge image for: 668_HC_Annotation.png\n",
      "Processed and saved edge image for: 507_HC_Annotation.png\n",
      "Processed and saved edge image for: 554_HC_Annotation.png\n",
      "Processed and saved edge image for: 660_HC_Annotation.png\n",
      "Processed and saved edge image for: 557_HC_Annotation.png\n",
      "Processed and saved edge image for: 627_HC_Annotation.png\n",
      "Processed and saved edge image for: 663_HC_Annotation.png\n",
      "Processed and saved edge image for: 501_HC_Annotation.png\n",
      "Processed and saved edge image for: 593_HC_Annotation.png\n",
      "Processed and saved edge image for: 575_HC_Annotation.png\n",
      "Processed and saved edge image for: 522_HC_Annotation.png\n",
      "Processed and saved edge image for: 681_HC_Annotation.png\n",
      "Processed and saved edge image for: 567_2HC_Annotation.png\n",
      "Processed and saved edge image for: 527_HC_Annotation.png\n",
      "Processed and saved edge image for: 566_HC_Annotation.png\n",
      "Processed and saved edge image for: 691_HC_Annotation.png\n",
      "Processed and saved edge image for: 684_HC_Annotation.png\n",
      "Processed and saved edge image for: 535_HC_Annotation.png\n",
      "Processed and saved edge image for: 505_HC_Annotation.png\n",
      "Processed and saved edge image for: 686_HC_Annotation.png\n",
      "Processed and saved edge image for: 504_HC_Annotation.png\n",
      "Processed and saved edge image for: 675_HC_Annotation.png\n",
      "Processed and saved edge image for: 502_HC_Annotation.png\n",
      "Processed and saved edge image for: 526_HC_Annotation.png\n",
      "Processed and saved edge image for: 656_HC_Annotation.png\n",
      "Processed and saved edge image for: 578_HC_Annotation.png\n",
      "Processed and saved edge image for: 648_2HC_Annotation.png\n",
      "Processed and saved edge image for: 603_HC_Annotation.png\n",
      "Processed and saved edge image for: 553_HC_Annotation.png\n",
      "Processed and saved edge image for: 606_HC_Annotation.png\n",
      "Processed and saved edge image for: 622_HC_Annotation.png\n",
      "Processed and saved edge image for: 590_HC_Annotation.png\n",
      "Processed and saved edge image for: 690_HC_Annotation.png\n",
      "Processed and saved edge image for: 525_HC_Annotation.png\n",
      "Processed and saved edge image for: 702_HC_Annotation.png\n",
      "Processed and saved edge image for: 615_HC_Annotation.png\n",
      "Processed and saved edge image for: 617_HC_Annotation.png\n",
      "Processed and saved edge image for: 526_2HC_Annotation.png\n",
      "Processed and saved edge image for: 637_HC_Annotation.png\n",
      "Processed and saved edge image for: 523_HC_Annotation.png\n",
      "Processed and saved edge image for: 567_HC_Annotation.png\n",
      "Processed and saved edge image for: 509_HC_Annotation.png\n",
      "Processed and saved edge image for: 636_HC_Annotation.png\n",
      "Processed and saved edge image for: 561_2HC_Annotation.png\n",
      "Processed and saved edge image for: 629_HC_Annotation.png\n",
      "Processed and saved edge image for: 562_HC_Annotation.png\n",
      "Processed and saved edge image for: 677_HC_Annotation.png\n",
      "Processed and saved edge image for: 613_HC_Annotation.png\n",
      "Processed and saved edge image for: 630_2HC_Annotation.png\n",
      "Processed and saved edge image for: 673_HC_Annotation.png\n",
      "Processed and saved edge image for: 588_2HC_Annotation.png\n",
      "Processed and saved edge image for: 594_HC_Annotation.png\n",
      "Processed and saved edge image for: 544_HC_Annotation.png\n",
      "Processed and saved edge image for: 595_HC_Annotation.png\n",
      "Processed and saved edge image for: 661_HC_Annotation.png\n",
      "Processed and saved edge image for: 536_HC_Annotation.png\n",
      "Processed and saved edge image for: 564_HC_Annotation.png\n",
      "Processed and saved edge image for: 677_2HC_Annotation.png\n",
      "Processed and saved edge image for: 574_HC_Annotation.png\n",
      "Processed and saved edge image for: 662_HC_Annotation.png\n",
      "Processed and saved edge image for: 582_HC_Annotation.png\n",
      "Processed and saved edge image for: 529_HC_Annotation.png\n",
      "Processed and saved edge image for: 541_HC_Annotation.png\n",
      "Processed and saved edge image for: 697_HC_Annotation.png\n",
      "Processed and saved edge image for: 599_HC_Annotation.png\n",
      "Processed and saved edge image for: 643_HC_Annotation.png\n",
      "Processed and saved edge image for: 540_HC_Annotation.png\n",
      "Processed and saved edge image for: 538_HC_Annotation.png\n",
      "Processed and saved edge image for: 565_HC_Annotation.png\n",
      "Processed and saved edge image for: 624_HC_Annotation.png\n",
      "Processed and saved edge image for: 583_HC_Annotation.png\n",
      "Processed and saved edge image for: 700_HC_Annotation.png\n",
      "Processed and saved edge image for: 507_2HC_Annotation.png\n",
      "Processed and saved edge image for: 688_HC_Annotation.png\n",
      "Processed and saved edge image for: 576_HC_Annotation.png\n",
      "Processed and saved edge image for: 596_HC_Annotation.png\n",
      "Processed and saved edge image for: 625_HC_Annotation.png\n",
      "Processed and saved edge image for: 639_2HC_Annotation.png\n",
      "Processed and saved edge image for: 508_HC_Annotation.png\n",
      "Processed and saved edge image for: 589_HC_Annotation.png\n",
      "Processed and saved edge image for: 611_HC_Annotation.png\n",
      "Processed and saved edge image for: 609_HC_Annotation.png\n",
      "Processed and saved edge image for: 655_HC_Annotation.png\n",
      "Processed and saved edge image for: 631_2HC_Annotation.png\n",
      "Processed and saved edge image for: 511_HC_Annotation.png\n",
      "Processed and saved edge image for: 552_HC_Annotation.png\n",
      "Processed and saved edge image for: 698_HC_Annotation.png\n",
      "Processed and saved edge image for: 630_HC_Annotation.png\n",
      "Processed and saved edge image for: 616_HC_Annotation.png\n",
      "Processed and saved edge image for: 612_HC_Annotation.png\n",
      "Processed and saved edge image for: 665_HC_Annotation.png\n",
      "Processed and saved edge image for: 555_HC_Annotation.png\n",
      "Processed and saved edge image for: 510_HC_Annotation.png\n",
      "Processed and saved edge image for: 683_HC_Annotation.png\n",
      "Processed and saved edge image for: 706_HC_Annotation.png\n",
      "Processed and saved edge image for: 561_HC_Annotation.png\n",
      "Processed and saved edge image for: 550_HC_Annotation.png\n",
      "Processed and saved edge image for: 664_HC_Annotation.png\n",
      "Processed and saved edge image for: 657_HC_Annotation.png\n",
      "Processed and saved edge image for: 579_HC_Annotation.png\n",
      "Processed and saved edge image for: 704_HC_Annotation.png\n",
      "Processed and saved edge image for: 602_HC_Annotation.png\n",
      "Processed and saved edge image for: 638_HC_Annotation.png\n",
      "Processed and saved edge image for: 604_HC_Annotation.png\n",
      "Processed and saved edge image for: 524_HC_Annotation.png\n",
      "Processed and saved edge image for: 657_2HC_Annotation.png\n",
      "Processed and saved edge image for: 631_HC_Annotation.png\n",
      "Processed and saved edge image for: 547_2HC_Annotation.png\n",
      "Processed and saved edge image for: 607_HC_Annotation.png\n",
      "Processed and saved edge image for: 667_HC_Annotation.png\n",
      "Processed and saved edge image for: 674_HC_Annotation.png\n",
      "Processed and saved edge image for: 546_HC_Annotation.png\n",
      "Processed and saved edge image for: 623_HC_Annotation.png\n",
      "Processed and saved edge image for: 701_HC_Annotation.png\n",
      "Processed and saved edge image for: 692_HC_Annotation.png\n",
      "Processed and saved edge image for: 652_2HC_Annotation.png\n",
      "Processed and saved edge image for: 690_2HC_Annotation.png\n",
      "Processed and saved edge image for: 598_HC_Annotation.png\n",
      "Processed and saved edge image for: 707_HC_Annotation.png\n",
      "Processed and saved edge image for: 519_HC_Annotation.png\n",
      "Processed and saved edge image for: 588_HC_Annotation.png\n",
      "Processed and saved edge image for: 651_HC_Annotation.png\n",
      "Processed and saved edge image for: 658_HC_Annotation.png\n",
      "Processed and saved edge image for: 646_HC_Annotation.png\n",
      "Processed and saved edge image for: 693_HC_Annotation.png\n",
      "Processed and saved edge image for: 641_HC_Annotation.png\n",
      "Processed and saved edge image for: 517_HC_Annotation.png\n",
      "Processed and saved edge image for: 635_HC_Annotation.png\n",
      "Processed and saved edge image for: 654_HC_Annotation.png\n",
      "Processed and saved edge image for: 513_HC_Annotation.png\n",
      "Processed and saved edge image for: 613_2HC_Annotation.png\n",
      "Processed and saved edge image for: 618_2HC_Annotation.png\n",
      "Processed and saved edge image for: 548_HC_Annotation.png\n",
      "Processed and saved edge image for: 532_2HC_Annotation.png\n",
      "Processed and saved edge image for: 650_HC_Annotation.png\n",
      "Processed and saved edge image for: 685_HC_Annotation.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved edge image for: 626_HC_Annotation.png\n",
      "Processed and saved edge image for: 543_HC_Annotation.png\n",
      "Processed and saved edge image for: 639_HC_Annotation.png\n",
      "Processed and saved edge image for: 614_HC_Annotation.png\n",
      "Processed and saved edge image for: 556_2HC_Annotation.png\n",
      "Processed and saved edge image for: 532_HC_Annotation.png\n",
      "Processed and saved edge image for: 699_HC_Annotation.png\n",
      "Processed and saved edge image for: 679_HC_Annotation.png\n",
      "Processed and saved edge image for: 570_3HC_Annotation.png\n",
      "Processed and saved edge image for: 587_2HC_Annotation.png\n",
      "Processed and saved edge image for: 705_HC_Annotation.png\n",
      "Processed and saved edge image for: 528_HC_Annotation.png\n",
      "Processed and saved edge image for: 556_HC_Annotation.png\n",
      "Processed and saved edge image for: 542_HC_Annotation.png\n",
      "Processed and saved edge image for: 675_2HC_Annotation.png\n",
      "Processed and saved edge image for: 644_HC_Annotation.png\n",
      "Processed and saved edge image for: 541_2HC_Annotation.png\n",
      "Processed and saved edge image for: 708_2HC_Annotation.png\n",
      "Processed and saved edge image for: 531_HC_Annotation.png\n",
      "Processed and saved edge image for: 547_HC_Annotation.png\n",
      "Processed and saved edge image for: 537_HC_Annotation.png\n",
      "Processed and saved edge image for: 673_2HC_Annotation.png\n",
      "Processed and saved edge image for: 581_HC_Annotation.png\n",
      "Processed and saved edge image for: 571_HC_Annotation.png\n",
      "Processed and saved edge image for: 671_3HC_Annotation.png\n",
      "Processed and saved edge image for: 694_HC_Annotation.png\n",
      "Processed and saved edge image for: 608_HC_Annotation.png\n",
      "Processed and saved edge image for: 670_HC_Annotation.png\n",
      "Processed and saved edge image for: 666_HC_Annotation.png\n",
      "Processed and saved edge image for: 560_HC_Annotation.png\n",
      "Processed and saved edge image for: 696_HC_Annotation.png\n",
      "Processed and saved edge image for: 545_2HC_Annotation.png\n",
      "Processed and saved edge image for: 514_HC_Annotation.png\n",
      "Processed and saved edge image for: 634_HC_Annotation.png\n",
      "Processed and saved edge image for: 619_HC_Annotation.png\n",
      "Processed and saved edge image for: 600_HC_Annotation.png\n",
      "Processed and saved edge image for: 559_HC_Annotation.png\n",
      "Processed and saved edge image for: 628_HC_Annotation.png\n",
      "Processed and saved edge image for: 545_HC_Annotation.png\n",
      "Processed and saved edge image for: 601_HC_Annotation.png\n",
      "Processed and saved edge image for: 682_HC_Annotation.png\n",
      "Processed and saved edge image for: 620_HC_Annotation.png\n",
      "Processed and saved edge image for: 585_HC_Annotation.png\n",
      "Processed and saved edge image for: 558_HC_Annotation.png\n",
      "Processed and saved edge image for: 539_HC_Annotation.png\n",
      "Processed and saved edge image for: 597_HC_Annotation.png\n",
      "Processed and saved edge image for: 570_2HC_Annotation.png\n",
      "Processed and saved edge image for: 533_HC_Annotation.png\n",
      "Processed and saved edge image for: 678_HC_Annotation.png\n",
      "Processed and saved edge image for: 671_2HC_Annotation.png\n",
      "Processed and saved edge image for: 648_HC_Annotation.png\n",
      "Processed and saved edge image for: 534_HC_Annotation.png\n",
      "Processed and saved edge image for: 645_HC_Annotation.png\n",
      "Processed and saved edge image for: 592_HC_Annotation.png\n",
      "Processed and saved edge image for: 686_2HC_Annotation.png\n",
      "Processed and saved edge image for: 568_HC_Annotation.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define input folder for the filled annotations and the output folder for the processed annotations\n",
    "input_folder = 'masked_annotations_test'  # Folder with filled ground truth annotations, e.g., \"500_HC_Annotation.png\"\n",
    "output_folder = 'postprocessing_annotations'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define a structuring element for the morphological operations (same as used on the predictions)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "resized_dim = 256\n",
    "\n",
    "# Loop over all annotation images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        # Read the filled annotation image in grayscale\n",
    "        annotation_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        annotation_img = cv2.resize(\n",
    "            annotation_img,\n",
    "            (resized_dim, resized_dim),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "        \n",
    "        # Apply morphological opening (erosion followed by dilation) to remove small artifacts\n",
    "        opened = cv2.morphologyEx(annotation_img, cv2.MORPH_OPEN, kernel)\n",
    "        # Then apply morphological closing (dilation followed by erosion) to fill small holes\n",
    "        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Apply Canny edge detector to extract the boundary\n",
    "        # Adjust thresholds if necessary; here using 50 and 150 as example values\n",
    "        edges = cv2.Canny(closed, 50, 150)\n",
    "        \n",
    "        # Save the processed (edge-detected) annotation image\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, edges)\n",
    "        \n",
    "        # Optionally display the images side by side\n",
    "        # plt.figure(figsize=(12, 4))\n",
    "        # plt.subplot(1, 3, 1)\n",
    "        # plt.imshow(annotation_img, cmap='gray')\n",
    "        # plt.title('Original Annotation')\n",
    "        # plt.axis('off')\n",
    "        #\n",
    "        # plt.subplot(1, 3, 2)\n",
    "        # plt.imshow(closed, cmap='gray')\n",
    "        # plt.title('After Morphological Ops')\n",
    "        # plt.axis('off')\n",
    "        #\n",
    "        # plt.subplot(1, 3, 3)\n",
    "        # plt.imshow(edges, cmap='gray')\n",
    "        # plt.title('Canny Edges')\n",
    "        # plt.axis('off')\n",
    "        #\n",
    "        # plt.show()\n",
    "        \n",
    "        print(f\"Processed and saved edge image for: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc5da6",
   "metadata": {},
   "source": [
    "**MHD (Mean Hausdorff Distance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13055df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Hausdorff Distance (MHD): 0.6096 mm\n",
      "Standard Hausdorff Distance (HD): 1.8518 mm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Load the CSV file with pixel size info.\n",
    "# ------------------------------------------------------------\n",
    "# The CSV (test_set_pixel_size.csv) is assumed to have columns \"filename\" and \"pixel size(mm)\"\n",
    "# where the filename corresponds to the original test image (e.g., \"500_HC.png\").\n",
    "pixel_size_dict = {}\n",
    "csv_file = 'test_set_pixel_size.csv'\n",
    "with open(csv_file, mode='r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        filename_csv = row['filename']  # e.g., \"500_HC.png\"\n",
    "        pixel_size_mm = float(row['pixel size(mm)'])\n",
    "        pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Set the resized dimension used in your inference pipeline.\n",
    "# ------------------------------------------------------------\n",
    "resized_dim = 256  # All images are resized to 256x256 during preprocessing/inference\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Define function to compute Hausdorff distance with coordinate transformation.\n",
    "# ------------------------------------------------------------\n",
    "def hausdorff_distance_transformed(pred_edges, true_edges, scale_x, scale_y):\n",
    "    \"\"\"\n",
    "    Computes the Hausdorff distance between two edge images after transforming\n",
    "    the coordinates from the resized (256x256) space to the original image space.\n",
    "    \n",
    "    Args:\n",
    "        pred_edges (np.array): Predicted edge image in 256x256 space (uint8),\n",
    "                               with edges as nonzero (e.g., 255).\n",
    "        true_edges (np.array): Ground truth edge image in 256x256 space (uint8).\n",
    "        scale_x (float): Factor to convert x coordinates.\n",
    "        scale_y (float): Factor to convert y coordinates.\n",
    "        \n",
    "    Returns:\n",
    "        float: Hausdorff distance in pixel units (in the original image space).\n",
    "    \"\"\"\n",
    "    # np.where returns (rows, cols) which are (y, x)\n",
    "    pred_points = np.column_stack(np.where(pred_edges != 0))\n",
    "    true_points = np.column_stack(np.where(true_edges != 0))\n",
    "    \n",
    "    if pred_points.size == 0 or true_points.size == 0:\n",
    "        return np.nan  # Return NaN if no edges are found in one of the images.\n",
    "    \n",
    "    # Transform predicted points from resized space to original space.\n",
    "    pred_points_transformed = np.zeros_like(pred_points, dtype=np.float32)\n",
    "    pred_points_transformed[:, 0] = pred_points[:, 0] * scale_y  # y coordinate\n",
    "    pred_points_transformed[:, 1] = pred_points[:, 1] * scale_x  # x coordinate\n",
    "    \n",
    "    # Similarly, transform ground truth points.\n",
    "    true_points_transformed = np.zeros_like(true_points, dtype=np.float32)\n",
    "    true_points_transformed[:, 0] = true_points[:, 0] * scale_y\n",
    "    true_points_transformed[:, 1] = true_points[:, 1] * scale_x\n",
    "    \n",
    "    d1 = directed_hausdorff(pred_points_transformed, true_points_transformed)[0]\n",
    "    d2 = directed_hausdorff(true_points_transformed, pred_points_transformed)[0]\n",
    "    return max(d1, d2)\n",
    "\n",
    "\n",
    "def mean_hausdorff_distance_transformed(pred_edges, true_edges, scale_x, scale_y):\n",
    "    \"\"\"\n",
    "    Computes the Mean Hausdorff Distance (MHD) between two edge images after coordinate transformation.\n",
    "    \"\"\"\n",
    "    pred_points = np.column_stack(np.where(pred_edges != 0))\n",
    "    true_points = np.column_stack(np.where(true_edges != 0))\n",
    "    \n",
    "    if pred_points.size == 0 or true_points.size == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Transform points to original image space\n",
    "    pred_points_transformed = pred_points * np.array([scale_y, scale_x])\n",
    "    true_points_transformed = true_points * np.array([scale_y, scale_x])\n",
    "    \n",
    "    # Compute MHD\n",
    "    dist_matrix = cdist(pred_points_transformed, true_points_transformed, 'euclidean')\n",
    "    min_dists_pred_to_true = np.min(dist_matrix, axis=1)\n",
    "    min_dists_true_to_pred = np.min(dist_matrix, axis=0)\n",
    "    mhd = np.mean(np.concatenate([min_dists_pred_to_true, min_dists_true_to_pred]))\n",
    "    \n",
    "    return mhd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3: Set directories for predicted edge images, ground truth boundary annotations,\n",
    "#         and the original test images.\n",
    "# ------------------------------------------------------------\n",
    "pred_edge_dir = 'output_edges'                  # Predicted edge images (resized: 256x256)\n",
    "gt_edge_dir = 'postprocessing_annotations'        # Ground truth boundaries (resized: 256x256)\n",
    "orig_image_dir = 'denoised_test_set'              # Original test images (actual dimensions), e.g., \"500_HC.png\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 4: Loop over predicted edge files, compute dynamic scaling, then compute MHD.\n",
    "# ------------------------------------------------------------\n",
    "pred_files = sorted([f for f in os.listdir(pred_edge_dir) if f.endswith('.png')])\n",
    "hd_scores_mm = []  # To store Hausdorff distances in millimeters\n",
    "hd_scores_mm_std = []\n",
    "\n",
    "for pred_file in pred_files:\n",
    "    pred_path = os.path.join(pred_edge_dir, pred_file)\n",
    "    \n",
    "    # Derive the base filename. For example, if the predicted file is \"seg_500_HC.png\",\n",
    "    # remove the \"seg_\" prefix to get \"500_HC.png\".\n",
    "    base_filename = pred_file.replace(\"seg_\", \"\", 1)  # e.g., \"500_HC.png\"\n",
    "    \n",
    "    # Build the key for CSV lookup (should match the original image filename)\n",
    "    csv_key = base_filename  # e.g., \"500_HC.png\"\n",
    "    if csv_key not in pixel_size_dict:\n",
    "        print(f\"No pixel size found for {csv_key}; skipping {pred_file}.\")\n",
    "        continue\n",
    "    pixel_to_mm = pixel_size_dict[csv_key]\n",
    "    \n",
    "    # Get the corresponding original test image from orig_image_dir.\n",
    "    orig_img_path = os.path.join(orig_image_dir, base_filename)\n",
    "    if not os.path.exists(orig_img_path):\n",
    "        print(f\"Original image {base_filename} not found; skipping {pred_file}.\")\n",
    "        continue\n",
    "    with Image.open(orig_img_path) as img:\n",
    "        orig_width, orig_height = img.size  # Dynamically retrieve original dimensions\n",
    "    \n",
    "    # Compute scale factors for this image:\n",
    "    scale_x = orig_width / float(resized_dim)\n",
    "    scale_y = orig_height / float(resized_dim)\n",
    "    \n",
    "    # Construct the corresponding ground truth (boundary) filename.\n",
    "    gt_file = base_filename.replace(\".png\", \"_Annotation.png\")  # e.g., \"500_HC_Annotation.png\"\n",
    "    gt_path = os.path.join(gt_edge_dir, gt_file)\n",
    "    if not os.path.exists(gt_path):\n",
    "        print(f\"Ground truth file {gt_file} not found for {pred_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Load predicted and ground truth edge images in grayscale (both should be 256x256)\n",
    "    pred_edges = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "    true_edges = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if pred_edges is None or true_edges is None:\n",
    "        print(f\"Error loading {pred_file} or {gt_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Compute Hausdorff distance in pixel units (transformed to original image coordinate space)\n",
    "    hd_pixels = mean_hausdorff_distance_transformed(pred_edges, true_edges, scale_x, scale_y)\n",
    "    hd_pixels_std = hausdorff_distance_transformed(pred_edges, true_edges, scale_x, scale_y)\n",
    "    if np.isnan(hd_pixels):\n",
    "        print(f\"No edges found in one of the images for {pred_file}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert the Hausdorff distance from original pixel units to millimeters.\n",
    "    hd_mm = hd_pixels * pixel_to_mm\n",
    "    hd_scores_mm.append(hd_mm)\n",
    "    \n",
    "    hd_mm_std = hd_pixels_std * pixel_to_mm\n",
    "    hd_scores_mm_std.append(hd_mm_std)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 5: Compute and print overall Mean Hausdorff Distance in mm.\n",
    "# ------------------------------------------------------------\n",
    "mean_hd_mm = np.nanmean(hd_scores_mm)\n",
    "mean_hd_mm_std = np.nanmean(hd_scores_mm_std)\n",
    "print(\"Mean Hausdorff Distance (MHD): {:.4f} mm\".format(mean_hd_mm))\n",
    "print(\"Standard Hausdorff Distance (HD): {:.4f} mm\".format(mean_hd_mm_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a792bc6",
   "metadata": {},
   "source": [
    "**Ellipse fiting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa48dc8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#     import csv\n",
    "#     import cv2\n",
    "#     import numpy as np\n",
    "#     import os\n",
    "#     import math\n",
    "\n",
    "#     # --- Step 1: Load the pixel size information from the CSV file ---\n",
    "#     # Assume your CSV file 'test_set_pixel_size.csv' has at least these columns:\n",
    "#     # filename,pixel_size_mm\n",
    "#     pixel_size_dict = {}\n",
    "#     with open('test_set_pixel_size.csv', mode='r') as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         for row in reader:\n",
    "#             # Adjust the column names if they are different in your CSV file.\n",
    "#             filename_csv = row['filename']\n",
    "#             pixel_size_mm = float(row['pixel size(mm)'])\n",
    "#             pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "#     # --- Step 2: Process the edge images and fit ellipses ---\n",
    "#     edges_folder = 'output_edges'\n",
    "#     csv_output = 'ellipse_results.csv'\n",
    "#     header = [\"filename\", \"center_x_mm\", \"center_y_mm\", \"semi_axes_a_mm\", \"semi_axes_b_mm\", \"angle_rad\"]\n",
    "#     rows = []\n",
    "\n",
    "#     for filename in sorted(os.listdir(edges_folder)):\n",
    "#         if filename.endswith('.png'):\n",
    "#             filepath = os.path.join(edges_folder, filename)\n",
    "#             # Read the edge image in grayscale\n",
    "#             edge_img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#             # Find contours in the edge image\n",
    "#             contours, _ = cv2.findContours(edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             if len(contours) == 0:\n",
    "#                 print(f\"No contours found in {filename}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Choose the largest contour (assumed to be the head contour)\n",
    "#             largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "#             # cv2.fitEllipse requires at least 5 points\n",
    "#             if len(largest_contour) < 5:\n",
    "#                 print(f\"Not enough points for ellipse fitting in {filename}. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Fit ellipse to the largest contour\n",
    "#             ellipse = cv2.fitEllipse(largest_contour)\n",
    "#             # ellipse returns ((center_x, center_y), (full_axis_length_a, full_axis_length_b), angle_in_degrees)\n",
    "#             center, axes, angle = ellipse\n",
    "#             # Compute semi-axes (the axes given are the full lengths)\n",
    "#             semi_a = axes[0] / 2.0  # semi-major axis in pixels\n",
    "#             semi_b = axes[1] / 2.0  # semi-minor axis in pixels\n",
    "\n",
    "#             # --- Step 3: Look up the pixel conversion factor for this image ---\n",
    "#             # The filenames in the CSV are expected to be like \"001_HC.png\"\n",
    "#             # and our edge images are named \"seg_001_HC.png\". Remove the \"seg_\" prefix.\n",
    "#             base_filename = filename.replace(\"seg_\", \"\", 1)\n",
    "#             if base_filename in pixel_size_dict:\n",
    "#                 pixel_to_mm = pixel_size_dict[base_filename]\n",
    "#             else:\n",
    "#                 print(f\"Pixel size for {base_filename} not found in CSV. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Convert measurements from pixels to millimeters\n",
    "#             center_x_mm = center[0] * pixel_to_mm\n",
    "#             center_y_mm = center[1] * pixel_to_mm\n",
    "#             semi_a_mm = semi_a * pixel_to_mm\n",
    "#             semi_b_mm = semi_b * pixel_to_mm\n",
    "\n",
    "#             # Convert angle from degrees to radians\n",
    "#             angle_rad = math.radians(angle)\n",
    "\n",
    "#             # Append the result: filename, center_x_mm, center_y_mm, semi_axes_a_mm, semi_axes_b_mm, angle_rad\n",
    "#             rows.append([base_filename, center_x_mm, center_y_mm, semi_a_mm, semi_b_mm, angle_rad])\n",
    "\n",
    "#     # --- Step 4: Write results to a CSV file ---\n",
    "#     with open(csv_output, mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(header)\n",
    "#         writer.writerows(rows)\n",
    "\n",
    "#     print(f\"CSV file '{csv_output}' saved with {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "266b6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import math\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Define original image dimensions and resized dimensions\n",
    "# # ------------------------------------------------------------\n",
    "# original_width = 800    # original image width in pixels\n",
    "# original_height = 540   # original image height in pixels\n",
    "# resized_dim = 256       # both width and height after resizing\n",
    "\n",
    "# # Compute scale factors for converting from the resized space to the original space:\n",
    "# scale_x = original_width / float(resized_dim)   # e.g., 800/256\n",
    "# scale_y = original_height / float(resized_dim)  # e.g., 540/256\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Step 1: Load the pixel size information from the CSV file\n",
    "# # ------------------------------------------------------------\n",
    "# # The CSV (test_set_pixel_size.csv) is assumed to have columns \"filename\" and \"pixel size(mm)\" \n",
    "# # where the filename is based on the original image (e.g., \"001_HC.png\").\n",
    "# pixel_size_dict = {}\n",
    "# csv_file = 'test_set_pixel_size.csv'\n",
    "# with open(csv_file, mode='r') as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     for row in reader:\n",
    "#         filename_csv = row['filename']  # e.g., \"001_HC.png\"\n",
    "#         pixel_size_mm = float(row['pixel size(mm)'])\n",
    "#         pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Step 2: Process the edge images (from output_edges folder) and fit ellipses.\n",
    "# #         Note: These edge images are produced from images resized to 256x256.\n",
    "# # ------------------------------------------------------------\n",
    "# edges_folder = 'output_edges'\n",
    "# csv_output = 'ellipse_results.csv'\n",
    "# header = [\"filename\", \"center_x_mm\", \"center_y_mm\", \"semi_axes_a_mm\", \"semi_axes_b_mm\", \"angle_rad\"]\n",
    "# rows = []\n",
    "\n",
    "# for filename in sorted(os.listdir(edges_folder)):\n",
    "#     if filename.endswith('.png'):\n",
    "#         filepath = os.path.join(edges_folder, filename)\n",
    "#         # Read the edge image in grayscale (should be 256x256)\n",
    "#         edge_img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "#         if edge_img is None:\n",
    "#             print(f\"Error loading {filename}; skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Find contours in the edge image\n",
    "#         contours, _ = cv2.findContours(edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#         if len(contours) == 0:\n",
    "#             print(f\"No contours found in {filename}. Skipping.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Choose the largest contour (assumed to be the head contour)\n",
    "#         largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "#         # cv2.fitEllipse requires at least 5 points\n",
    "#         if len(largest_contour) < 5:\n",
    "#             print(f\"Not enough points for ellipse fitting in {filename}. Skipping.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Fit ellipse to the largest contour\n",
    "#         ellipse = cv2.fitEllipse(largest_contour)\n",
    "#         # ellipse returns ((center_x, center_y), (full_axis_length_a, full_axis_length_b), angle_in_degrees)\n",
    "#         center, axes, angle = ellipse\n",
    "#         # Compute semi-axes in the resized coordinate space\n",
    "#         semi_a_resized = axes[0] / 2.0\n",
    "#         semi_b_resized = axes[1] / 2.0\n",
    "\n",
    "#         # Scale the center and semi-axes from resized (256x256) to original image coordinate space\n",
    "#         center_x_original = center[0] * scale_x\n",
    "#         center_y_original = center[1] * scale_y\n",
    "#         semi_a_original = semi_a_resized * scale_x\n",
    "#         semi_b_original = semi_b_resized * scale_y\n",
    "\n",
    "#         # --- Step 3: Look up the pixel-to-mm conversion factor for this image ---\n",
    "#         # The CSV expects file names like \"001_HC.png\"; our predictions are named \"seg_001_HC.png\"\n",
    "#         base_filename = filename.replace(\"seg_\", \"\", 1)  # e.g., \"001_HC.png\"\n",
    "#         if base_filename in pixel_size_dict:\n",
    "#             pixel_to_mm = pixel_size_dict[base_filename]\n",
    "#         else:\n",
    "#             print(f\"Pixel size for {base_filename} not found in CSV. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Convert measurements from original pixel space to millimeters\n",
    "#         center_x_mm = center_x_original * pixel_to_mm\n",
    "#         center_y_mm = center_y_original * pixel_to_mm\n",
    "#         semi_a_mm = semi_a_original * pixel_to_mm\n",
    "#         semi_b_mm = semi_b_original * pixel_to_mm\n",
    "\n",
    "#         # Convert angle from degrees to radians\n",
    "#         angle_rad = math.radians(angle)\n",
    "\n",
    "#         # Append the result\n",
    "#         rows.append([base_filename, center_x_mm, center_y_mm, semi_a_mm, semi_b_mm, angle_rad])\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Step 4: Write results to a CSV file\n",
    "# # ------------------------------------------------------------\n",
    "# with open(csv_output, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerows(rows)\n",
    "\n",
    "# print(f\"CSV file '{csv_output}' saved with {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58c083a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'ellipse_results.csv' saved with 250 rows.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define resized dimension (from your on-the-fly augmentation)\n",
    "# ------------------------------------------------------------\n",
    "resized_dim = 256  # images are resized to 256x256 during preprocessing/inference\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Load the pixel size information from the CSV file\n",
    "# ------------------------------------------------------------\n",
    "# The CSV (test_set_pixel_size.csv) is assumed to have columns \"filename\" and \"pixel size(mm)\"\n",
    "# where the filename is based on the original test image (e.g., \"500_HC.png\").\n",
    "pixel_size_dict = {}\n",
    "csv_file = 'test_set_pixel_size.csv'\n",
    "with open(csv_file, mode='r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        filename_csv = row['filename']  # e.g., \"500_HC.png\"\n",
    "        pixel_size_mm = float(row['pixel size(mm)'])\n",
    "        pixel_size_dict[filename_csv] = pixel_size_mm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Folder for original test images (to retrieve dynamic dimensions)\n",
    "# ------------------------------------------------------------\n",
    "orig_image_dir = 'denoised_test_set'  # Test images are stored here (e.g., \"500_HC.png\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Process the edge images (from output_edges folder) and fit ellipses.\n",
    "#         Note: These edge images are produced from images resized to 256x256.\n",
    "# ------------------------------------------------------------\n",
    "edges_folder = 'output_edges'\n",
    "csv_output = 'ellipse_results.csv'\n",
    "header = [\"filename\", \"center_x_mm\", \"center_y_mm\", \"semi_axes_a_mm\", \"semi_axes_b_mm\", \"angle_rad\"]\n",
    "rows = []\n",
    "\n",
    "for filename in sorted(os.listdir(edges_folder)):\n",
    "    if filename.endswith('.png'):\n",
    "        filepath = os.path.join(edges_folder, filename)\n",
    "        # Read the edge image in grayscale (expected size: 256x256)\n",
    "        edge_img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        if edge_img is None:\n",
    "            print(f\"Error loading {filename}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Find contours in the edge image\n",
    "        contours, _ = cv2.findContours(edge_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 0:\n",
    "            print(f\"No contours found in {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Choose the largest contour (assumed to be the head contour)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # cv2.fitEllipse requires at least 5 points\n",
    "        if len(largest_contour) < 5:\n",
    "            print(f\"Not enough points for ellipse fitting in {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Fit ellipse to the largest contour\n",
    "        ellipse = cv2.fitEllipse(largest_contour)\n",
    "        # ellipse returns ((center_x, center_y), (full_axis_length_a, full_axis_length_b), angle_in_degrees)\n",
    "        center, axes, angle = ellipse\n",
    "        # Compute semi-axes in the resized coordinate space\n",
    "        semi_a_resized = axes[0] / 2.0\n",
    "        semi_b_resized = axes[1] / 2.0\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Step 3: Dynamically determine the original image dimensions.\n",
    "        # ------------------------------------------------------------\n",
    "        # The predictions are based on images that were resized from the original.\n",
    "        # For a given predicted file, its base name (e.g., \"500_HC.png\") is used to find the corresponding original image.\n",
    "        # For test data, the predicted file is likely named \"seg_500_HC.png\" (if using the same prefix convention).\n",
    "        base_filename = filename.replace(\"seg_\", \"\", 1)  # e.g., \"500_HC.png\"\n",
    "        orig_img_path = os.path.join(orig_image_dir, base_filename)\n",
    "        if not os.path.exists(orig_img_path):\n",
    "            print(f\"Original test image {base_filename} not found; skipping {filename}.\")\n",
    "            continue\n",
    "        with Image.open(orig_img_path) as img:\n",
    "            orig_width, orig_height = img.size  # (width, height)\n",
    "\n",
    "        # Compute scale factors for this image from the resized dimension to its original dimensions.\n",
    "        scale_x = orig_width / float(resized_dim)\n",
    "        scale_y = orig_height / float(resized_dim)\n",
    "        \n",
    "        # ------------------------------------------------------------\n",
    "        # Step 4: Scale the ellipse parameters from the resized (256x256) space back to the original space.\n",
    "        # ------------------------------------------------------------\n",
    "        center_x_original = center[0] * scale_x\n",
    "        center_y_original = center[1] * scale_y\n",
    "        semi_a_original = semi_a_resized * scale_x\n",
    "        semi_b_original = semi_b_resized * scale_y\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Step 5: Look up the pixel-to-mm conversion factor for this image.\n",
    "        # The CSV expects file names like \"500_HC.png\"; our base_filename should match.\n",
    "        # ------------------------------------------------------------\n",
    "        if base_filename in pixel_size_dict:\n",
    "            pixel_to_mm = pixel_size_dict[base_filename]\n",
    "        else:\n",
    "            print(f\"Pixel size for {base_filename} not found in CSV. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Convert the measurements from original pixel space to millimeters.\n",
    "        center_x_mm = center_x_original * pixel_to_mm\n",
    "        center_y_mm = center_y_original * pixel_to_mm\n",
    "        semi_a_mm = semi_a_original * pixel_to_mm\n",
    "        semi_b_mm = semi_b_original * pixel_to_mm\n",
    "\n",
    "        # Convert angle from degrees to radians\n",
    "        angle_rad = math.radians(angle)\n",
    "\n",
    "        # Append the results: filename, center_x_mm, center_y_mm, semi_axes_a_mm, semi_axes_b_mm, angle_rad\n",
    "        rows.append([base_filename, center_x_mm, center_y_mm, semi_a_mm, semi_b_mm, angle_rad])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 6: Write results to a CSV file\n",
    "# ------------------------------------------------------------\n",
    "with open(csv_output, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"CSV file '{csv_output}' saved with {len(rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
